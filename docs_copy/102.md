




### 任务简述

* 为了更贴合工业项目的开发流程，我们将从系统主服务的构建开始，因此我们的第一个任务是：构建文本标签化主服务，它是一个基于Flask的主服务，有了它，我们将直接得到标签系统的对外API，这个API以一段文本为输入，以该文本被打上的label为输出。此时，我们将完成各个模块的主要流程代码，如：文本预处理，实现图谱匹配，实现匹配歧义判断等，这些步骤的作用和实现都将在之后的介绍中逐个说明。

### 任务目的

* 构建起文本标签主服务，对外提供标签化API。


### 任务步骤
	
* Step1: 在服务器上搭建虚拟环境
* Step2: 在主函数中实现文本预处理
* Step3: 实现图谱匹配过程
* Step4: 匹配歧义判断  
* Step5: 概率调整
* Step6: 概率归一化与父标签检索
* Step7: Flask主服务封装与测试


---





#### Step1: 在服务器上搭建虚拟环境

* 当前步骤简述：
	* 在企业里，每一个项目都会分解任务给不同的工程师，每个人都可能使用不同的技术栈，因此需要搭建不同虚拟环境。我们当前的项目按照文本和多模态阶段分成两个虚拟环境；在这个任务步骤中，我们将搭建文本虚拟环境：text3.5（3.5是python的版本）。


* 当前步骤的目标：
	* 使用conda搭建起文本虚拟环境：text3.5

* 环境版本要求：
	* 系统版本：centos7.9
	* 虚拟环境python版本：python3.5
	* python中的工具包版本：
		* neo4j-driver==1.7.4
		* psutil==5.9.0
		* scikit-learn==0.20.4
		* pandas>=0.20.3
		* numpy>=1.13.1
		* jieba>=0.39
		* requests>=2.18.4
		* tensorflow==1.14.0
		* matplotlib==3.0.3
		* keras==2.2.4
	
* 其他相关要求：
	* 因为之后会用到图数据库neo4j创建标签图谱，因此需要安装neo4j3.3.5。
	* 我们的主服务会使用supervisor工具进行监控，因此需要安装supervisor（最新版本即可）。



#### 让我们动手做起来吧！


* 1, 下载安装Anaconda3，其中就包括虚拟环境工具conda以及python环境。

```shell
# 解压Anaconda3需要bzip2，所以在这里需要先安装
yum install -y bzip2

# 进入指定的安装包下载路径，这里是root路径
cd /root 

# 使用curl进行远程安装包下载
curl -O https://repo.anaconda.com/archive/Anaconda3-5.2.0-Linux-x86_64.sh
# 以下是最新的anaconda3地址，使用哪一个都可以
# curl -O https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh


# 使用sh命令进行安装，中间出现的各种说明，按照默认"yes"即可
sh Anaconda3-5.2.0-Linux-x86_64.sh

# 配置~/.bashrc
vim ~/.bashrc
# 添加一行: 
export PATH=/root/anaconda3/bin/:$PATH

# 关闭文件，进行source生效即可
source ~/.bashrc
``` 



* 2，使用conda创建虚拟环境

```shell
# 创建名字为text3.5的虚拟环境，使用python版本为3.5
conda create -n text3.5 python=3.5

# 根据提示激活环境
conda activate text3.5
# 或者
source activate text3.5
```




* 3，创建项目路径

```shell
cd /
mkdir data
cd /data/
# 在/data路径下创建名为 labeled_project 的项目路径
mkdir labeled_project
```


* 4，创建requirements.txt文件并进行工具包安装

```shell
# 在/data/labeled_project路径下打开requirements.txt文件
vim requirements.txt

# 在requirements.txt中写入必要的安装包版本
 
# neo4j-driver==1.7.4
# psutil==5.9.0
# scikit-learn==0.20.4
# pandas>=0.20.3
# numpy>=1.13.1
# jieba>=0.39
# requests>=2.18.4
# tensorflow==1.14.0
# matplotlib==3.0.3
# keras==2.2.4

# 在虚拟环境text3.5中安装以下工具包
pip install -r requirements.txt
```




* 5，安装图数据库neo4j，因为我们的标签图谱将使用图数据库存储

```shell
# 生成yum镜像
rpm --import http://debian.neo4j.org/neotechnology.gpg.key
cat <<EOF>  /etc/yum.repos.d/neo4j.repo
[neo4j]
name=Neo4j RPM Repository
baseurl=http://yum.neo4j.org/stable
enabled=1
gpgcheck=1
EOF

# 安装neo4j
yum install neo4j-3.3.5

# 修改图数据库相关配置，配置位置在/etc/neo4j/neo4j.conf
# neo4j.conf修改后的文件将在附件中提供
# 启动neo4j
neo4j start

# 查看启动状态，提示：Neo4j is running at pid 13616
neo4j status
```


* 6，安装必要的服务监控组件

```shell
yum install supervisor -y
```



* 当前步骤总结：
	* 到这里，我们就已经搭建起text3.5的项目环境，在接下来的开发中，每次登陆服务器我们需要激活该环境，在其中进行一些列的开发。


---

#### Step2: 在主函数中实现文本预处理


* 当前步骤简述：
	* 我们当前是在构建“文本”标签化系统，下一次迭代才会涉及图片和视频的处理。在工业生产中，文本形式的输入少不了一定预处理，我们将在这个步骤中实现。



* 文本预处理包括：
	* 对输入进行长度限制
	* 对输入进行分词处理
	* 对分词结果进行去停用词处理


* 工具要求：
	* 分词使用主流的jieba工具（在实际大厂中，可能会有自己的基础分词服务，使用方式和工具类似）



* 代码实现位置：
	* 我们将在/data/labeled_project/创建api.py文件，即：
	* /data/labeled_project/api.py


#### 让我们动手做起来吧！


* 代码实现：

```python
import os
import jieba
import fileinput

# 定义了用户自定义词典路径和停用词典路径
# 两个txt文件将在附件中提供
userdict_path = "/data/labeled_project/text_labeled/userdict.txt"
stopdict_path = "/data/labeled_project/text_labeled/stopdict.txt"


# 加载用户自定义词典
jieba.load_userdict(userdict_path)

# 定义输入文本最大长度限制为200
MAX_LIMIT = 200


def handle_cn_text(text: str):
    """用于完成预处理的主要流程, 以原始文本为输入，以分词和去停用词后的词汇列表为输出."""

    # 对输入进行合法性检验
    if not text: return []

    # 使用jieba的cut方法对使用最大限制进行切片的输入文本进行分词
    word_list = jieba.cut(text[:MAX_LIMIT])

    def _load_stop_dict():
        """用于从文件中加载停用词词表"""

        # 使用fileinput加载停用词表至内存,使用字符串的strip()方法去除两端空白符
        stop_word_set = set(map(lambda x: x.strip(), fileinput.FileInput(stopdict_path)))
        return stop_word_set

    # 调用_load_stop_dict()函数
    stop_word_set = _load_stop_dict()

    # 使用高阶函数filter进行循环过滤操作生成最终结果
    word_list = list(filter(lambda x: x not in stop_word_set, word_list))
    return word_list
```


> * 运行示例：

```python
text = "我的眼睛很大很大,可以装得下天空，装得下高山，装得下大海，装得下整个世界；我的眼睛又很小很小，有心事时，就连两行眼泪，也装不下."
word_list = handle_cn_text(text)
print(word_list)
```

```text
['眼睛', '很大', '很大', '装得', '天空', '装得', '高山', '装得', '大海', '装得', '整个', '世界', '眼睛', '很小', '很小', '心事', '两行', '眼泪', '装不下']
```



* 当前步骤总结：
	* 通过handle_cn_text函数，我们就完成了基于jieba的文本预处理中的三个要求。



---


#### Step3: 实现图谱匹配过程

* 当前步骤简述：
	* 我们在这里需要实现图谱匹配过程，在实现之前，你需要了解标签-词汇图谱是什么以及它的作用。我们再回顾一下正在做的文本标签化系统，是以一段文本为输入，以该文本应该有的标签为输出，那么标签该如何获得呢，按照中文语言的特点，经常在句子会有一些关键词让我们辨识它的所属类别（也就是标签），比如："张国荣演的《霸王别姬》真不错！"，从“霸王别姬”这样的关键词中，我们就可以做出一些猜测，这句话可能在谈论电影；没错，有的同学也可能猜测是音乐（有首歌也叫霸王别姬），所以可见，文本中的关键词是能够帮助我们有效的缩小标签的范围的。因此，我们的图谱就是由标签-关键词组成，文本在预处理后，将每个词汇通过图谱进行可能的标签查询，来召回一定量的可能标签。


* 标签词汇图谱分析：

![](http://121.199.45.168:8000/img/%E5%9B%BE%E8%B0%B1.png)


> * 图谱由节点和关系(边)组成.
> * 蓝色节点代表标签,橘色节点代表词汇.
> * 在节点与节点之间存在着不同类型的边.
> * 蓝色节点(标签节点)之间的边表示包含关系,没有权重值.
> * 蓝色节点与橘色节点(词汇节点)之间的边表示隶属关系,有权重值，代表该词汇属于该标签的概率.
> * 所有的节点与边组成了一个树结构,也就是我们的图谱.
> * 图谱匹配的过程,即将分词列表中的词汇与词汇节点进行匹配，相同则返回该标签节点名称和边上的权重.


* 图谱匹配在整个系统中的作用：
	* 通过匹配词汇召回所有可能的标签.


* 相关假设：
	* 图谱的具体构建我们会在任务二中进行，所以当前我们的图数据库中什么也没有，但我们仍然可以假设它们已经存在来完成代码，虽然我们什么标签也不会查到。等我们之后完成了图谱构建，我们还会来验证你的代码。



* 代码实现位置：
	* /data/labeled_project/api.py


---

#### 让我们动手做起来吧！


* 代码实现：

```python
# 首先导入操作图数据库neo4j的必备官方工具neo4j-driver,
# 从settings.py配置文件中导入数据库配置NEO4J_CONFIG
# settings.py文件将在附件中提供和说明
from neo4j.v1 import GraphDatabase
from settings import NEO4J_CONFIG

# 导入用于扁平化列表的chain方法
from itertools import chain

def get_index_map_label(word_list):
    """
    用于获取每个词汇在图谱中对应的类别标签
    该函数以词汇列表为输入, 以词汇出现在词汇列表
    中的索引和对应的[标签, 权重]列表为输出.
    """
    # 对word_list进行合法性检验
    if not word_list: return []

    # 使用GraphDatabase开启一个driver.
    _driver = GraphDatabase.driver(**NEO4J_CONFIG)

    # 开启neo4j的一个session 
    with _driver.session() as session:
        def _f(index, word):
            """以词汇列表中一组词索引和词作为输入,
            返回该索引和词对应的标签列表."""
            # 进行输入的合法性判断
            if not word: return []

            # 建立cypher语句, 它匹配一条图中的路径, 该路径以一个词汇为开端通过一条边连接一个Label节点,
            # 返回标签的title属性,和边的权重, 这正是我们图谱构建时定义的连接模式.
            cypher = "MATCH(a:Vocabulary{name:%r})-[r:Related]-(b:Label) \
                      RETURN b.title, r.weight" % (word)
            record = session.run(cypher)
            result = list(map(lambda x: [x[0], x[1]], record))
            if not result: return []
            return [str(index), result]

        # 将word_list的索引和词汇作为输入传给_f()函数,并将返回结果做chain操作
        index_map_label = list(
            chain(*map(lambda x: _f(x[0], x[1]), enumerate(word_list))))
    return index_map_label
```

> * 运行示例：

```python
word_list = ['眼睛', '很大', '很大', '装得', '天空', '装得', '高山', '装得', '大海', '装得', '整个', '世界', '眼睛', '很小', '很小', '心事', '两行', '眼泪', '装不下']
index_map_label = get_index_map_label(word_list)
print(index_map_label)
```

```python
# 因为我们图谱还没有构建, 因此暂时会返回一个空列表
# 实际上应该返回类似结构: ["0", [["美妆", 0.654], ["电影":0.765]]]
[]
```

* 当前步骤总结：
	* 通过get_index_map_label函数我们就完成来图谱匹配的过程，它在实际生产中，帮助我们召回大量可能性高的标签。

---



#### Step4: 匹配歧义判断

* 当前步骤简述：
	* “匹配歧义判断”虽然听起来比较抽象，但是实际很好理解，所谓匹配歧义正如我们之前“霸王别姬”的例子，到底是电影还是音乐，单纯从关键词的角度去理解，那这就是歧义。出现了歧义我们就需要去做判断哪个是正确的（或者都是正确的/错误的），用什么来判断呢，那就需要文本语义模型。因此，匹配歧义判断实质是检测歧义并调用模型服务的过程。 



* 匹配歧义的例子：

![](http://121.199.45.168:8000/img/%E5%88%86%E6%AD%A7.png)

> * "闪现"一词匹配到两个标签, LOL和王者农药, 说明这个词汇在句子中具有歧义，需要进行更深层次的判断.

* 匹配歧义判断的作用:
	* 在词汇出现歧义时,通过模型重新计算所属标签的概率，从语义层面获得更真实的标签概率.


* 相关假设：
	* 在这里我们还没有模型服务（将在之后的任务中构建），所以需要通过创建“空壳函数”来弥补，保证代码run通。
	* 在api.py的当前路径创建路径model_train, 在model_train路径下，创建multithread_predict.py文件，在该文件中添加以下函数。（我们将在之后的任务中重写该函数）



```python
def request_model_serve(word_list, label_list):
    return [["电影", 0.865]]
```


* 代码实现位置：
	* /data/labeled_project/api.py




#### 让我们动手做起来吧！

* 代码实现：

```python
# 导入多模型预测函数 
from model_train.multithread_predict import request_model_serve

def weight_update(word_list, index_map_label):
    """该函数将分词列表和具有初始概率的标签-概率列表作为输入,将模型预测后的标签-概率列表作为输出"""
    # 首先将列表转化为字典的形式
    # index_map_label >>> ["1", [["美食", 0.735], ["音乐", 0.654]],  "2",  [["美妆", 0.734]] >>> 
    # {"1": [["美食", 0.735],["音乐",  0.654]], "2": [["美妆", 0.734]]}
    index_map_label = dict(zip(index_map_label[::2], index_map_label[1::2]))
    for k, v in index_map_label.items():
        # v的长度大于1说明存在歧义现象 
        if len(v) > 1:
            # 获取对应的标签作为参数,即通知服务应该调用哪些模型进行预测.
            label_list = list(map(lambda x: x[0], v))
            # 通过request_model_serve函数获得标签最新的预测概率,并使用字典方式更新.
            # v >>> [["美食": 0.954]]
            v = request_model_serve(word_list, label_list)
            index_map_label.update({k:v})
    # 将字典转化为列表形式
    index_map_label_ = list(chain(*map(lambda x: [x[0], x[1]], index_map_label.items())))
    return index_map_label_
```

> * 运行示例：

```python
word_list = ['眼睛', '很大', '很大', '装得', '天空', '装得', '高山', '装得', '大海', '装得', '整个', '世界', '眼睛', '很小', '很小', '心事', '两行', '眼泪', '装不下']         

index_map_label = ["0", [["美妆", 0.654], ["电影", 0.765]]]
index_map_label_ = weight_update(word_list, index_map_label)
print(index_map_label_)
```


```
["0", [["电影", 0.865]]]
```


* 当前步骤总结：
	* 通过weight_update函数我们完成了匹配歧义判断，其中包括歧义检测和调用模型服务，虽然我们的模型服务还是空壳函数，但我们将在之后的任务中实现它。



---

#### Step5: 概率调整

* 当前步骤简述：
	* 我们的标签召回思路是使用关键词，关键词是否出现会影响到标签是否被召回。而且同时当某个标签下出现的词汇较多时，我们会认为这种标签的重要性较高。所谓概率调整就是实现这种标签重要性的计算。


* 为什么进行概率调整的例子：

> * 假如"我爱苹果" 中的"苹果"会匹配的标签： [["水果", 0.654], ["电影", 0.654], ["公司", 0.654]] 
>> * 分析：
>>> * 出现了一次苹果, 可能是在说水果，电影，或者公司, 他们的概率基本上是相同的. 这句话打上什么标签不能确定.


> * 假如"我爱苹果，橘子，香蕉"会匹配标签： [["水果", 0.654], ["电影", 0.654], ["公司", 0.654], ["水果", 0.654], ["水果", 0.654]]
>> * 分析：
>>> * 全句共出现了三次有关水果的词，如果水果的概率是苹果，橘子，香蕉为水果的概率和，这样就大于了电影或者公司的概率. 基本上可以打上一个确定的标签了.

* 概率调整的作用:
	* 保证随着某一类别词汇出现的次数增多,这个类别的概率会随之增加.


* 概率调整的计算方式：
	* 加性运算（加法）



* 代码实现位置：
	* /data/labeled_project/api.py


#### 让我们动手做起来吧!

* 代码实现：

```python
# 导入可以进行扁平化操作的reduce
# 导入进行合并操作的pandas
from functools import reduce
import pandas as pd

def control_increase(index_map_label_):
    """以模型预测后的标签-权重列表为输入, 以标签归并后的结果为输出"""
    if not index_map_label_: return []

    # index_map_label_ >>> 
    #  ["2", [["电影", 0.765]], "3", [["情感故事",  0.876], ["明星", 0.765]]]
    # 将index_map_label_奇数项即[label, score]取出放在字典中
    # k的数据结构形式: 
    # [{'label': '电影', 'score': 0.765}, {'label': '情感故事', 'score': 0.876},
    #  {'label': '明星', 'score': 0.765}]
    k = list(map(lambda x: {"label": x[0], "score": x[1]}, reduce(
        lambda z, y: z + y, index_map_label_[1::2])))

    # 使用pandas中的groupby方法进行合并分值
    df = pd.DataFrame(k)
    df_ = df.groupby(by=['label'])['score'].sum()
    return df_
```

> * 运行示例：

```python
index_map_label_ = ["2", [["电影", 0.765]], "3", [["情感故事",  0.876], ["明星", 0.765]]]
df_ = control_increase(index_map_label_)
print(df_)
```

```text
label
电影    1.641
明星      0.765
Name: score, dtype: float64
```


* 当前步骤总结：
	* 通过control_increase我们完成了概率调整操作，可以有效的对比标签的重要性。


---



#### Step6: 概率归一化与父标签检索


* 当前步骤简述：
	* 这是我们任务的最后一步，其实也是系统的最后一步；因为之前的概率调整是加性运算，很可能使得概率>1，所以这里我们需要归一化才能进行输出，同时，我们输出的标签也不仅仅是标签体系下的叶子节点标签，也希望能够输出其父标签。现在可以假设“王者荣耀”是我们的叶子节点标签，那么“游戏”就是它的父标签，这些关系都存在图谱当中，当我们一直子标签时，通过查询的方式即可获得其父标签。



* 概率归一化的作用:
	* 使标签概率的结果在（0到1）的概率值域内.

* 父标签检索的作用:
	* 当前标签系统应用在推荐系统中时，会需要更多级别的标签来丰富召回策略。


* 代码实现位置：
	* /data/labeled_project/api.py


#### 让我们动手做起来吧！

* 代码实现：

```python
import numpy as np

def father_label_and_normalized(df_):
    """
    以概率调整后的DataFrame对象为输入, 以整个系统的最终结果为输出
    输入样式为:DataFrame<[[“LOL”, 1.465]]>
    输出样式为:[{“label”: “LOL”, “score”: “0.811”, “related”:[“游戏”]}]
    """
    def _sigmoid(x):
        y = 1.0 / (1.0 + np.exp(-x))
        return round(y, 3)
    def _sg(pair):
        """获得单个标签的父级标签和归一化概率"""
        # 使用GraphDatabase开启一个driver.
        _driver = GraphDatabase.driver(**NEO4J_CONFIG)
        with _driver.session() as session:
            # 通过关系查询获得从该标签节点直到根节点的路径上的其他Label节点的title属性
            cypher = "MATCH(a:Label{title:%r})<-[r:Contain*1..3]-(b:Label) \
                      WHERE b.title<>'泛娱乐' RETURN b.title" % pair[0]
            record = session.run(cypher)
            result = list(map(lambda x: x[0], record))
        return {"label": pair[0], "score": _sigmoid(pair[1]), "related": result}
    # 遍历所有的标签
    return list(map(_sg, df_.to_dict().items()))
```

> * 运行示例：

```python
res = father_label_and_normalized(df_)
print(res)
```

```
[]
# 实际上应该返回如下格式：
# [{'label': '电影', 'score': 0.838, 'related': []}, {'label': '明星', 'score': 0.682, 'related': []}] 
```


* 当前步骤总结：
	* 通过father_label_and_normalized函数我们就完成了概率归一化和父标签检索，至此我们也基本完成了api.py中的全部内容。

---


#### Step7: Flask主服务封装与测试

* 当前步骤简述：
	* 我们在之前的步骤中，搭建了主服务的环境以及所需的业务流程代码，在这个步骤中，我们将它们串联起来，并封装在Flask服务之中。



* Flask服务简介：
	* Flask框架是当下最受欢迎的python轻量级框架, 也是pytorch官网指定的部署框架. Flask的基本模式为在程序里将一个视图函数分配给一个URL，每当用户访问这个URL时，系统就会执行给该URL分配好的视图函数，获取函数的返回值.
	* 为了保证服务的健壮性，Flask常常与gunicorn一同使用。



* 安装必要的python工具包：

```shell
pip install Flask==1.1.1
pip install gunicorn==20.0.4
```


* 代码实现位置：
	* /data/labeled_project/views.py

#### 让我们动手做起来吧！


* 代码实现：

```python
# Flask框架固定工具
from flask import Flask
from flask import request

app = Flask(__name__)


import json
import api

# 定义服务请求路径和方式, 这里使用POST请求
@app.route("/api/get_label/", methods=["POST"])
def recognition():
    # 接收POST请求，并取数据中的"text"对应的值
    text = request.form.get("text")
    # 调用输入预处理
    word_list = api.handle_cn_text(text)
    # 调用图谱匹配
    index_map_label = api.get_index_map_label(word_list)
    # 调用匹配歧义判断
    index_map_label_ = api.weight_update(word_list, index_map_label)
    if index_map_label_:
        # 调用概率调整
        df_ = api.control_increase(index_map_label_)
        # 调用概率归一化与父标签检索
        result = api.father_label_and_normalized(df_)
    else:
        result = []
    return str(result)
```
	

* 使用gunicorn启动服务

```shell
gunicorn -w 1 -b 0.0.0.0:8888  views:app
```


* 使用supervisor后台启动服务
```shell
# supervisord.conf在附件中提供
supervisord -c supervisord.conf
```

* 编写测试脚本：
	* 该脚本将写在/data/labeled_project/test.py中

```python
import requests


def test():
    url = "http://0.0.0.0:8888/api/get_label/"
    data = {"text": "衬衫、裤子、外套、帽子：均为Wan Hung, 鞋：Samo, 项链：Crazy Rhythmm, 耳钉：Yvmin!"}
    res = requests.post(url, data=data)
    print(res.text)

if __name__ == "__main__":
    test()
```

* 当前步骤总结：
	* 通过Flask我们将主要的项目步骤封装在了服务之中，并通过test()进行了API测试。至此，我们的任务就已经完成了！ 
