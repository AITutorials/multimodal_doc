




### 任务简述


* 在该任务中，我们开始构建文本分类模型，模型的数量等同于标签体系中最后一级标签的数量，每个模型都是一个二分类模型，当“关键词”有歧义时，来判断文本是否属于某一个标签。每个模型的构建包括数据处理，特征处理，模型构建等，它是基于Fasttext结构的快速预测模型。有了它，我们将能够解决文本标签化中最棘手的歧义问题，最后还会将这些模型封装成微服务。



### 任务目的

* 构建并训练文本分类模型并使用它进行歧义判断，确定文本指向的标签，最后将其部署成模型子服务。


### 任务步骤
	
* Step1: 获取训练语料
* Step2: 进行文本数据分析
* Step3: 特征处理
* Step4: 构建fasttext模型并训练
* Step5: 单模型服务部署

---





#### Step1: 获取训练语料

* 当前步骤简述：
	* 我们需要训练等同于“叶子标签”数量的二分类模型，因为就需要同样份数的训练语料，假设我们现在只有4个标签，那么在这一步，我们就需要获取4份语料，每份语料中还需要区分正负样本。

* 语料来源：
	* 正是我们在[任务二步骤三]中的原始语料。

* 正负样本的定义：
	* 将文章中的每一条句子作为该类别的正样本； 将其他类别文章中的每一条句子作为负样本。

* 输入：
	* 每一种标签对应的N篇文章路径

* 输出：
	* csv形式的文件，第一列是代表标签，第二列代表文本


* 代码实现位置：
	* 不存在的路径需要自己的创建
	* /data/labeled_project/text_labeled/model_train/get_sample.py


#### 让我们动手做起来吧！


* 代码实现：

```python
import os

# 限制句子的最小字符数和句子的最大字符数
MIN_LENGTH = 5
MAX_LENGTH = 500

def get_p_text_list(single_article_path):
    """获取单篇文章的文本列表"""
    with open(single_article_path, "r") as f:
        text = f.read()
        # 去掉换行符, 并以句号划分
        cl = text.replace("\n", ".").split("。")
        # 过滤掉长度范围之外的句子
        cl = list(filter(lambda x: MIN_LENGTH<len(x)<MAX_LENGTH, cl))
    return cl


def get_p_sample(a_path, p_path):
    """该函数用于获得正样本的csv, 以文章路径和正样本csv写入路径为参数"""
    if not os.path.exists(a_path): return
    if not os.path.exists(p_path): os.mkdir(p_path)
    # 以追加的方式打开预写入正样本的csv
    fp = open(os.path.join(p_path, "p_sample.csv"), "a")
    # 遍历文章目录下的每一篇文章
    for u in os.listdir(a_path):
        cl = get_p_text_list(os.path.join(a_path, u))
        for clc in cl:
            fp.write("1" + "\t" + clc + "\n")
    fp.close()


def get_sample(p_path, n_path_csv_list: list):
    """该函数用于获取样本集包括正负样本, 以正样本csv文件路径和负样本csv文件路径列表为参数"""
    fp = open(os.path.join(p_path, "sample.csv"), "w")
    with open(os.path.join(p_path, "p_sample.csv"), "r") as f:
        text = f.read()
    # 先将正样本写入样本csv之中
    fp.write(text)
    # 遍历负样本的csv列表
    for n_p_c in n_path_csv_list:
        with open(n_p_c, "r") as f:
            # 将其中的标签1改写为0
            text = f.read().replace("1", "0")
        # 然后写入样本的csv之中
        fp.write(text)
    fp.close()
```



> * 运行示例：

```python
# 我们以beauty为例：
single_article_path = "../create_graph/beauty/article-191721"
get_p_text_list(single_article_path)
a_path = "../create_graph/beauty/"
p_path = "./beauty"
get_p_sample(a_path, p_path)
# 选取哪些标签作为beauty的负样本
n_path_csv_list = ["./movie/p_sample.csv", "./star/p_sample.csv", "./fashion/p_sample.csv"]
get_sample(p_path, n_path_csv_list)
```


```text
1   #PINKGANG#：不粉所有人纪梵希2018全新禁忌之吻漆光唇蜜发布会暨派对 2018年8月19日，法国品牌纪梵希特别打造了一场精致酷炫的#PINKGANG#时尚派对，为庆祝纪梵希全新禁忌之吻漆光唇蜜（Gloss Interdit Vinyl）的闪耀上市
1   上海黄浦江畔的德必外滩8号，在历史悠久的法式古典建筑中，纪梵希与国内各大时尚媒体、时尚美妆领域的达人以及时髦人士一起分享并体验品牌的全新产品，感受品牌时尚叛逆的奢华魅力
1   超人气青春偶像陈立农作为品牌挚友出席活动，演员胡冰卿、陈瑶、李兰迪代表新时代潮流标杆受邀一同亮相派对，共同分享纪梵希全新唇蜜的炫目发布
1   值此之际，围绕着“#PINKGANG#不粉所有人”的主题，纪梵希为来宾营造出叛逆时髦、不受约束的派对氛围，充分展示了品牌一贯以来突破经典、个性前卫的态度
1   现场的布置以禁忌之吻漆光唇蜜的色彩为灵感，将霓虹粉色作为场馆的主色调，突显纪梵希全新产品神秘、禁忌、时尚的风格
...
...
...
0   它采用皮革饰边塑造柔软休闲款式的轮廓，激光切割顶部
0   绉胶鞋底非常耐穿
0   系带的款式也很时髦哦~皮毛是天然连毛小绵羊皮，来自瑞士
0   皮革采用小绵羊皮，材质非常好，所以特别保暖，价格自然也会贵一些
0   大家可去Shopbop上购买，价格比UGG贵很多，但是质量也好~品牌：INUIKI 官网链接戳这←单品购买链接戳这←最后一句哎哟喂，这双00刚结束，又要开始剁手买雪地靴了！
```

* 当前步骤总结：
	* 这样我们通过一系列函数构建了某一种标签的正负样本，对于其他标签也是相同的方法，大家可以通过修改文件路径进行尝试。


---


#### Step2: 进行文本数据分析

* 当前步骤简述：
	* 对语料的数据分析是AI工程师进行模型训练前非常重要的一步，它能帮助我们更好的了解语料情况，对数据质量把控起到关键作用；对于文本训练数据来讲，常见的数据分析有标签分布，文本长度分布，常见词频分布等，我们在这一步骤中就是来实现和分析这些过程。

* 获取正负标签数量分布的作用：
	*  用于帮助调整正负样本比例, 而调整正负样本比例, 对我们进行接下来的数据分析和判断模型准确率基线起到关键作用。


* 获取句子长度分布的作用：
	* 用于帮助判断句子合理的截断对齐长度, 而合理的截断长度将有效的避免稀疏特征或冗余特征的产生, 提升训练效率。

* 获取常见词频分布的作用：
	* 指导之后模型超参数max_feature(最大的特征总数)的选择和初步评估数据质量。


* 代码实现位置：
	* /data/labeled_project/text_labeled/model_train/data_analysis.py




#### 让我们动手做起来吧！


* 标签数量分布代码实现：

```python
import pandas as pd
import jieba

def get_data_labels(csv_path): 
    """获得训练数据和对应的标签, 以正负样本的csv文件路径为参数"""
    # 使用pandas读取csv文件至内存
    df = pd.read_csv(csv_path, header=None, sep="\t")

    # 对句子进行分词处理并过滤掉长度为1的词
    train_data = list(map(lambda x: list(filter(lambda x: len(x)>1, 
                                    jieba.lcut(x))), df[1].values)) 

    # 取第0列的值作为训练标签
    train_labels = df[0].values 
    return train_data, train_labels


import os
from collections import Counter

def pic_show(pic, pic_path, pic_name):
    """用于图片显示，以图片对象和预保存的路径为参数"""
    if not os.path.exists(pic_path): os.mkdir(pic_path)
    pic.savefig(os.path.join(pic_path, pic_name))



def get_labels_distribution(train_labels, pic_path, pic_name="ld.png"):
    """获取正负样本数量的基本分布情况"""
    # class_dict >>> {1: 3995, 0: 4418}
    class_dict = dict(Counter(train_labels))
    print(class_dict)
    df = pd.DataFrame(list(class_dict.values()), list(class_dict.keys()))
    pic = df.plot(kind='bar', title="类别分布图").get_figure()
    pic_show(pic, pic_path, pic_name)

```

> * 运行示例：

```python
# 训练语料路径
csv_path = "./movie/sample.csv"
train_data, train_labels = get_data_labels(csv_path)
pic_path = "./movie/"
get_labels_distribution(train_labels, pic_path)
```


```text
{1: 4640, 0: 7165}
```

![](http://121.199.45.168:8000/img/%E7%B1%BB%E5%88%AB%E5%88%86%E5%B8%83%E5%9B%BE.png)




* 结果分析:
	* 当前的正负样本数量是分别是: 4640和7165,相差2525条数据.
	* 为了使正负样本均衡, 让它们的比例为1:1, 我们将在之后进行的该类别的数据分析和模型训练中, 随机去除约2500条负样本的数量.	




---


* 句子长度分布代码实现：


```python
def get_sentence_length_distribution(train_data, pic_path, pic_name="sld.png"):
    """该函数用于获得句子长度分布情况"""
    sentence_len_list = list(map(len, train_data))
    # len_dict >>> {38: 62, 58: 18, 40: 64, 35: 83,....}  
    len_dict = dict(Counter(sentence_len_list))
    len_list = list(zip(len_dict.keys(), len_dict.values()))
    # len_list >>> [(1, 3), (2, 20), (3, 51), (4, 96), (5, 121), (6, 173), ...]
    len_list.sort(key=(lambda x: x[0]))
    df = pd.DataFrame(list(map(lambda x: x[1], len_list)), list(
        map(lambda x: x[0], len_list)))
    ax = df.plot(kind='bar', figsize=(18, 18), title="句子长度分布图")
    ax.set_xlabel("句子长度")
    ax.set_ylabel("该长度出现的次数")
    pic = ax.get_figure()
    pic_show(pic, pic_path, pic_name)
```

> * 运行示例：

```python
pic_path = "./movie/"
pic_name = "sld.png"
# train_data通过get_data_labels得到，需要进行正负样本均衡切片
get_sentence_length_distribution(train_data, pic_path, pic_name="sld.png")
```

![](http://121.199.45.168:8000/img/sld.png)





* 结果分析:
        * 通过句子长度分布图, 我们知道了句子的长度范围在0-151之间.
	* 但在0-60的长度之间, 已经包含了超过90%的句子, 因此这里可以认为60的长度是一个合理的截断对齐长度, 即不会使大量句子被截断而失去主要信息, 又能够有效避免补齐的特征数量太多, 导致模型参数过大.






---

* 常见词频分布代码实现：


```python
from itertools import chain

def get_word_frequency_distribution(train_data, pic_path, pic_name="wfd.png"):
    """该函数用于获得词频分布"""
    vocab_size = len(set(chain(*train_data)))
    print("所有样本共包含不同词汇数量为：", vocab_size)
    # 获取常见词分布字典，以便进行绘图
    # common_word_dict >>> {'电影': 1548, '自己': 968, '一个': 850, '导演': 757, '现场': 744, ...}
    common_word_dict = dict(Counter(chain(*train_data)).most_common(50))
    df = pd.DataFrame(list(common_word_dict.values()),
                       list(common_word_dict.keys()))
    pic = df.plot(kind='bar', figsize=(18, 18), title="常见词分布图").get_figure()
    pic_show(pic, pic_path, pic_name)
```

> * 运行示例：

```python
pic_path = "./movie/"
pic_name = "wfd.png"
# train_data通过get_data_labels得到，需要进行正负样本均衡切片
get_word_frequency_distribution(train_data, pic_path, pic_name="wfd.png")
```

```
所有样本共包含不同词汇数量为：24020
```


![](http://121.199.45.168:8000/img/wfd.png)




* 结果分析:
	* 通过常见词频分析, 全文词汇总数为24020, 在模型训练时定义的max_features应大于该数值.
	* 同时对比高频词汇中出现的与影视相关的词汇占比大概在50%左右, 符合正负样本的分布比例, 因此语料质量尚可.



* 当前步骤总结：
	* 我们以movie为例完成了一系列文本数据分析工作，包括标签分布，长度分布，常见词分布，同学们需要自己动手，将其他给定的标签进行数据分析。




---

#### Step3: 特征处理

* 当前步骤简述：
	* 特征处理是语料进行模型前的必要准备过程，它一般包括：词汇数值映射（tokenizer），截断补齐，n-gram特征提取，最后还需要进行一次最长补齐。


* 词汇数值映射（tokenizer）：
	* 将分词列表中的每个词映射成数字.

* 截断补齐：
	* 将映射后的句子向量进行截断，以降低模型输入的特征维度，来防止过拟合.

* n-gram特征提取：
	* 当我们处理文本问题时，上下文之间的关系往往是重要的语义信息来源，因此我们需要一定的特征处理过程，其中最重要的就是n-gram特征处理，它能够帮助我们更好的捕捉上下文信息。

* n-gram特征的例子：
	* 在这里, 可以将n-gram特征可以理解为相邻词汇的共现特征, 当n为2时, 就是连续两个词的共现。我们这里将使用2-gram, 因此以2-gram为例进行解释: 分词列表: ["是谁", "敲动", "我心"]，对应的序列列表: [1, 34, 21]，我们可以认为序列列表中的每个数字就是原始句子的特征, 即词汇是原始句子的特征. 除此之外, 我们还可以把"是谁"和"敲动"两个词共同出现且相邻也作为一种特征加入到序列列表中，此时序列列表就变成了包含2-gram特征的特征列表: [1, 34, 21, 1000]，这里的1000就代表"是谁"和"敲动"共同出现且相邻, 这种特征也就是n-gram特征.其中n为2.

* 最长补齐：
	* 为了不损失n-gram特征，使向量能够以矩阵形式作为模型输入.


* 代码实现位置：
	* 我们将以构建movie特征处理过程为例进行代码实现
	* /data/labeled_project/text_labeled/model_train/movie_model_train.py


#### 让我们动手做起来吧!

* 词汇数值映射代码实现：

```python
# 导入用于对象保存与加载的joblib
from sklearn.externals import joblib
# 导入keras中的词汇映射器Tokenizer
from keras.preprocessing.text import Tokenizer
# 导入从样本csv到内存的get_data_labels函数
from data_analysis import get_data_labels


def word_map(csv_path, tokenizer_path, cut_num):
    """进行词汇映射，以训练数据的csv路径和映射器存储路径以及截断数为参数"""
    # 使用get_data_labels函数获取简单处理后的训练数据和标签
    train_data, train_labels = get_data_labels(csv_path)
    # 进行正负样本均衡切割, 使其数量比例为1:1
    train_data = train_data[:-cut_num]
    train_labels = train_labels[:-cut_num]
    # 实例化一个词汇映射器对象
    t = Tokenizer(num_words=None, char_level=False)
    # 使用映射器拟合现有文本数据
    t.fit_on_texts(train_data)
    # 使用joblib工具保存映射器
    joblib.dump(t, tokenizer_path)
    # 使用映射器转化现有文本数据
    x_train = t.texts_to_sequences(train_data)
    # 获得标签数据
    y_train = train_labels
    return x_train, y_train

```

> * 运行示例：

```python
# 对应的样本csv路径
csv_path = "./movie/sample.csv"
# 词汇映射器保存的路径
tokenizer_path = "./movie/Tokenizer"
# 截断数
cut_num = 2525
x_train, y_train = word_map(csv_path, tokenizer_path, cut_num)
```

```text
# x_train
[[3480, 485, 9674, 979, 23, 67, 39, 1097, 432, 49, 27584, 205], 
 [17, 27585, 27586, 1355, 27587, 14019, 65, 100],
 [2282, 2609, 7, 7616, 1897, 2302, 274, 1355, 2302, 20],
 [57, 27588, 13601, 135, 586, 134, 4138], ...]

# y_train
[1 1 1 ... 0 0 0]
```





---


* 截断补齐代码实现：

```python
from keras.preprocessing import sequence

# cutlen根据数据分析中句子长度分布，覆盖90%语料的最短长度.
cutlen = 60
def padding(x_train, cutlen):
    return sequence.pad_sequences(x_train, cutlen)
```


> * 运行示例：

```python
# 通过word_map函数获得的x_train
# 通过数据分析获得的截断长度
cutlen = 60
x_train = padding(x_train, cutlen)
```

```text
[[    0     0     0 ...    49  5576  5577]
 [    0     0  1682 ...     1  1682  7179]
 [    0     0     0 ...   148 10517  7183]
 ...
 [    0     0     0 ...  7245  1567  1731]
 [    0     0     0 ...  1872   364 20985]
 [    0     0     0 ... 10353  1207 20989]]
```

---


* n-gram特征提取代码实现：

```python
import numpy as np

# 根据样本集最大词汇数选择最大特征数，应大于样本集最大词汇数
max_features = 25000

# n-gram特征的范围，一般选择为2
ngram_range = 2

def create_ngram_set(input_list):
    """
    从列表中提取n-gram特征
    >>> create_ngram_set([1, 4, 9, 4, 1, 4])
    {(4, 9), (4, 1), (1, 4), (9, 4)}
    """
    return set(zip(*[input_list[i:] for i in range(ngram_range)]))


def get_ti_and_nmf(x_train, ti_path):
    """从训练数据中获得token_indice和新的max_features"""
    # >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}
    # 创建一个盛装n-gram特征的集合.
    ngram_set = set()
    # 遍历每一个数值映射后的列表
    for input_list in x_train:
        # 遍历可能存在2-gram, 3-gram等
        for i in range(2, ngram_range + 1):
            # 获得对应的n-gram表示 
            set_of_ngram = create_ngram_set(input_list, ngram_value=i)
            # 更新n-gram集合
            ngram_set.update(set_of_ngram)

    # 去除掉(0, 0)这个2-gram特征
    ngram_set.discard(tuple([0]*ngram_range))
    # 将n-gram特征映射成整数.
    # 为了避免和之前的词汇特征冲突，n-gram产生的特征将从max_features+1开始
    start_index = max_features + 1
    # 得到对n-gram表示与对应特征值的字典
    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}
    # 将token_indice写入文件以便预测时使用
    with open(ti_path, "w") as f:
        f.write(str(token_indice))
    # token_indice的反转字典，为了求解新的最大特征数
    indice_token = {token_indice[k]: k for k in token_indice}
    # 获得加入n-gram之后的最大特征数
    new_max_features = np.max(list(indice_token.keys())) + 1
    return token_indice, new_max_features


def add_ngram(sequences, token_indice):
    """
    将n-gram特征加入到训练数据中
    如: adding bi-gram
    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]
    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}
    >>> add_ngram(sequences, token_indice, ngram_range=2)
    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]
    """
    new_sequences = []
    # 遍历序列列表中的每一个元素作为input_list, 即代表一个句子的列表
    for input_list in sequences:
        # copy一个new_list
        new_list = input_list[:].tolist()
        # 遍历n-gram的value，至少从2开始
        for ngram_value in range(2, ngram_range + 1):
            # 遍历各个可能的n-gram长度
            for i in range(len(new_list) - ngram_value + 1):
                # 获得input_list中的n-gram表示
                ngram = tuple(new_list[i:i + ngram_value])
                # 如果在token_indice中，则追加相应的数值特征
                if ngram in token_indice:
                    new_list.append(token_indice[ngram])
        new_sequences.append(new_list)
    return np.array(new_sequences)
```

> * 运行示例：

```python
# 数据进行截断对齐后的矩阵x_train
# token_indice的保存路径
ti_path = "./movie/token_indice"
token_indice, new_max_features = get_ti_and_nmf(x_train, ti_path)
x_train = add_ngram(x_train, token_indice)
```

```text
[list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1296, 1086, 9, 2510, 2325, 1004, 668, 2990, 669, 482, 669, 335, 126063, 46370, 36768, 93632, 116281, 46593, 136403, 29955, 34254, 127711, 47353, 132158])
 list([0, 0, 0, 0, 11, 4, 8280, 26, 2511, 2991, 528, 22, 411, 702, 11, 350, 8281, 604, 85, 1501, 468, 52, 11, 56, 3255, 104815, 38229, 35505, 67872, 28659, 50795, 140653, 113341, 65967, 78902, 57072, 108083, 29205, 115079, 61698, 48928, 42416, 46802, 110530, 99281, 40828])
...
]
```





---

* 最长补齐代码实现：


```python
def align(x_train):
    """用于向量按照最长长度进行补齐"""
    # 获得所有句子长度的最大值
    maxlen = max(list(map(lambda x: len(x), x_train)))
    # 调用padding函数
    x_train = padding(x_train, maxlen)
    return x_train, maxlen
```

> * 运行示例：

```python
# 由函数add_ngram输出的矩阵x_train
x_train, maxlen = align(x_train)
```

```text
# 进行了最大长度补齐的矩阵x_train
[[     0      0      0 ... 113541  36959  22941]
 [     0      0   1682 ...  42518  59855  25524]
 [     0      0      0 ...  75385  50810  68725]
 ...
 [     0      0      0 ...  97401  34490  77114]
 [     0      0      0 ...  21440  85555  32122]
 [     0      0      0 ...  56394  95696  45331]]

# 补齐的最大长度
119
```


* 当前步骤总结：
	* 通过一系列函数我们完成了关于movie模型的特征处理过程，包括词汇数值映射（tokenizer），截断补齐，n-gram特征提取和最长补齐。同学们可以以此为模版进行其他标签的处理。



---






#### Step4: 构建fasttext模型并训练

* 当前步骤简述：
        * 前面我们已经完成了fasttext模型的特征处理，现在我们开始构建fasttext模型并训练，我们需要了解它的结构以及作用，在这一步骤中我们将实现它。

* fasttext模型结构中三个重要的层（使用keras进行实现）：
	* Embedding层
	* GAP层(全局平均池化层)
	* Dense + sigmoid层


* keras中的embedding层：
	* 层结构: 结构可以看作是一个矩阵，它的大小是语料的最大特征数(new_max_features)乘以我们预定义的embedding_dims，这个矩阵就相当于是由每一个特征拓展成embedding_dims后的表示.
	* 层参数: 矩阵中的每一个数，都是模型需要求解的参数，因此Embedding层的参数总量是new_max_features x embedding_dims.
	* 输入参数: new_max_features即最大特征数, embedding_dims即词嵌入维度, input_length即句子的最大长度.
	* 输入形状: [None, input_length]
	* 输出形状: [None, input_length, embedding_dims]
	* 作用: 用向量表示每一个特征，在更高维度的映射空间捕捉词与词之间的关系.



* keras中的GAP层：
	* 层结构: 本质上是对矩阵的一种计算方法，无结构.
	* 层参数: 无
	* 输入参数: 无
	* 输入形状: [None, input_length, embedding_dims]
	* 输出形状: [None, embedding_dims]
	* 作用: 消减模型参数总量，防止过拟合.



* keras中的Dense + sigmoid层:
	* 层结构: 具有个1个节点的一层全连接网络，最后的激活函数使用sigmoid.
	* 层参数: 该节点中的w向量共50维，加上一个偏置b，共51个参数.
	* 输入参数: 分别是该层的节点数以及使用的sigmoid函数.
	* 输入形状: [None, embedding_dims]
	* 输出形状: [None, 1]
	* 作用: 将抽象的特征表示归一到指定的类别上，能够输出我们想要的0或者1的结果.



* fasttext模型选取的损失函数:
	* 二分类交叉熵损失函数

* fasttext模型选取的优化器:
	* Adam


* 代码实现位置：
        * /data/labeled_project/text_labeled/model_train/movie_model_train.py




#### 让我们动手做起来吧！


* 构建模型结构代码实现：

```python
# 首先导入keras构建模型的必备工具包
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Embedding
from keras.layers import GlobalAveragePooling1D

# 定义词嵌入维度为50
embedding_dims = 50
# 最大对齐长度, 即输入矩阵中每条向量的长度
maxlen = 119
# 最大特征数, 即输入矩阵中元素的最大值
new_max_features = 143307i


def model_build():
    """该函数用于模型结构构建"""

    # 在函数中，首先初始化一个序列模型对象
    model = Sequential()

    # 然后首层使用Embedding层进行词向量映射
    model.add(Embedding(new_max_features,
                        embedding_dims,
                        input_length=maxlen))

    # 然后用构建全局平均池化层，减少模型参数，防止过拟合
    model.add(GlobalAveragePooling1D())

    # 最后构建全连接层 + sigmoid层来进行分类.
    model.add(Dense(1, activation='sigmoid'))
    return model
```




> * 运行示例：

```
model = model_build()
```

```
<keras.engine.sequential.Sequential object at 0x7f67cc2bf208>
```



* 选取损失函数和优化器的代码实现：

```python
def model_compile(model):
    """用于选取模型的损失函数和优化方法"""
    # 使用model自带的compile方法，选择预定义好的二分类交叉熵损失函数，Adam优化方法，以及准确率评估指标.
    model.compile(loss='binary_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
    return model 
```

> * 运行示例：

```
model = model_compile(model)
```



* 模型训练和绘制准曲率和损失对照曲线代码实现：

```python
# 导入作图工具包matplotlib
import matplotlib.pyplot as plt

# batch_size是每次进行参数更新的样本数量
batch_size = 32

# epochs将全部数据遍历训练的次数
epochs = 40


def model_fit(model, x_train, y_train):
    """用于模型训练"""
    history = model.fit(x_train, 
                        y_train,
                        batch_size=batch_size,
                        epochs=epochs,
                        # validation_split表示将全部训练数据的多少划分为验证集.
                        validation_split=0.1)
    return history


def plot_loss_acc(history, acc_png_path, loss_png_path):
    """用于绘制模型的损失和acc对照曲线, 以模型训练历史为参数"""
    # 首先获得模型训练历史字典，
    # 形如{'val_loss': [0.8132099324259264, ..., 0.8765081824927494], 
    #    'val_acc': [0.029094827586206896,...,0.13038793103448276], 
    #     'loss': [0.6650978644232184,..., 0.5267722122513928], 
    #     'acc': [0.5803400383141762, ...,0.8469827586206896]}
    history_dict = history.history

    # 取出需要的的各个key对应的value，准备作为纵坐标
    acc = history_dict['acc']
    val_acc = history_dict['val_acc']
    loss = history_dict['loss']
    val_loss = history_dict['val_loss']

    # 取epochs的递增列表作为横坐标
    epochs = range(1, len(acc) + 1)

    # 绘制训练准确率的点图
    plt.plot(epochs, acc, 'bo', label='Training acc')
    # 绘制验证准确率的线图
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    # 增加标题
    plt.title('Training and validation accuracy')
    # 增加横坐标名字
    plt.xlabel('Epochs')
    # 增加纵坐标名字 
    plt.ylabel('Accuracy')
    # 将上面的图放在一块画板中 
    plt.legend()
    # 保存图片
    plt.savefig(acc_png_path)

    # 清空面板 
    plt.clf()
    # 绘制训练损失的点图
    plt.plot(epochs, loss, 'bo', label='Training loss')
    # 绘制验证损失的线图
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    # 添加标题
    plt.title('Training and validation loss')
    # 添加横坐标名字
    plt.xlabel('Epochs')
    # 添加纵坐标名字
    plt.ylabel('Loss')
    # 把两张图放在一起
    plt.legend()
    # 保存图片
    plt.savefig(loss_png_path)
```


> * 运行示例：


```python
history = model_fit(model, x_train, y_train)
acc_png_path = "./movie/acc.png"
loss_png_path = "./movie/loss.png" 
plot_loss_acc(history, acc_png_path, loss_png_path)
```


```text
# 模型训练日志
Epoch 3/40
5299/5299 [==============================] - 7s 1ms/step - loss: 0.4094 - acc: 0.7998 - val_loss: 0.9937 - val_acc: 0.1800
Epoch 4/40
5299/5299 [==============================] - 7s 1ms/step - loss: 0.3185 - acc: 0.8498 - val_loss: 0.8025 - val_acc: 0.3548
Epoch 5/40
5299/5299 [==============================] - 7s 1ms/step - loss: 0.2379 - acc: 0.9136 - val_loss: 0.7550 - val_acc: 0.4482
Epoch 6/40
5299/5299 [==============================] - 7s 1ms/step - loss: 0.1779 - acc: 0.9500 - val_loss: 0.6113 - val_acc: 0.5857
Epoch 7/40
5299/5299 [==============================] - 7s 1ms/step - loss: 0.1355 - acc: 0.9726 - val_loss: 0.5836 - val_acc: 0.6214
Epoch 8/40
5299/5299 [==============================] - 7s 1ms/step - loss: 0.1056 - acc: 0.9826 - val_loss: 0.4837 - val_acc: 0.6893
Epoch 9/40
5299/5299 [==============================] - 7s 1ms/step - loss: 0.0844 - acc: 0.9870 - val_loss: 0.5271 - val_acc: 0.6570
Epoch 10/40
4384/5299 [=======================>......] - ETA: 1s - loss: 0.0691 - acc: 0.991
```


![](http://121.199.45.168:8000/img/loss.png)

* 通过损失对照曲线判断模型是否收敛：
	* 当双损失曲线都在下降时,说明模型正在收敛, 大部分情况下,模型都会收敛.


![](http://121.199.45.168:8000/img/acc.png)

* 通过准确率对照曲线判断过拟合：
	* 当训练准确率平缓或上升而验证准确率开始平缓或下降时，在这个点处开始出现过拟合现象.



* 模型保存与加载代码实现：

```python
from keras.models import load_model
model.save(save_path)
model = load_model(save_path)
```


> * 运行示例：

```python
#模型的保存路径
save_path = "./movie/model.h5" 
model.save(save_path)
model = load_model(save_path)
```

> * 在./movie路径下, 获得一个model.h5



* 当前步骤总结：
	* 到这里，我们就完成来一个fasttext模型的训练过程，因为我们文本较短，一般语义是比较明显的，fasttext模型是足够捕捉其语义的，因此在测试集上效果一般不会太差。同学们可以尝试对更多的标签进行判别模型。



---


#### Step5：单模型服务部署


* 当前步骤简述：
	* 当我们完成来所有的模型训练后，为了能够使用这些模型，我们需要将其封装成微服务，这里使用flask+gunicorn的组合形式，还记得[任务一步骤七]吗，它们使用的方式是一样的！在这一步中，我们将以一个模型为例来实现它。



* 代码实现位置：
	* 这里是以beauty为例进行服务搭建
	* /data/labeled_project/text_labeled/model_servers/beauty/app.py



#### 让我们动手做起来吧！


* 代码实现：

```python
# Flask框架固定工具
from flask import Flask
from flask import request

app = Flask(__name__)

import sys

root_path = "/data/labeled_project/text_labeled/model_train/"
sys.path.append(root_path)

# 导入必备的工具包
import json
from sklearn.externals import joblib

# 从任意的模型训练文件中导入add_ngram增加n-gram特征以及padding截断函数
from beauty_model_train import add_ngram
from beauty_model_train import padding
# 定义模型配置路径，它指向一个json文件
model_config_path = root_path + "model_config.json"
config_list = json.load(open(model_config_path, "r"))["美妆"]

# model_config.json形如 ：
# {"影视": ["/data/labeled_project/text_labeled/model_train/movie/Tokenizer", 60, 2,
#           "/data/labeled_project/text_labeled/model_train/movie/token_indice", 119,
#           "http://localhost:8501/v1/models/movie/"],
# "美妆": ["/data/labeled_project/text_labeled/model_train/beauty/Tokenizer", 75, 2,
#           "/data/labeled_project/text_labeled/model_train/beauty/token_indice", 119,
#           "http://localhost:8502/v1/models/beauty/"]}
# json文件中是一个字典，字典中的每个key是我们标签的中文字符，每个value是一个列表
# 列表的第一项是特征处理时词汇映射器的存储地址
# 第二项是特征处理时语料的截断长度
# 第三项是n-gram取得n值
# 第四项是n-gram特征中token_indice的保存路径
# 第五项是最后的最大的对齐长度
# 第六项是该模型对应的微服务地址



# 将持久化的模型配置文件加载到内存

tokenizer_path = config_list[0]
cutlen = config_list[1]
ngram_range = config_list[2]
ti_path = config_list[3]
maxlen = config_list[4]

t = joblib.load(tokenizer_path)


# 获得n-gram映射文件
with open(ti_path, "r") as f:
    token_indice = eval(f.read())



from keras.models import load_model

model_save_path  = root_path + "beauty/model.h5"
model = load_model(model_save_path)



# 定义服务请求路径和方式, 这里使用POST请求
@app.route("/v1/models/beauty/", methods=["POST"])
def recognition():
    word_list = eval(request.form["word_list"])
    # 使用tokenizer进行数值映射
    x = t.texts_to_sequences([word_list])
    # 进行截断对齐
    x = padding(x, cutlen)
    # 添加n-gram特征
    x = add_ngram(x, token_indice, ngram_range)
    # 进行最大长度对齐
    x = padding(x, maxlen)
    y = model.predict(x)[0][0]
    return str(y)
```


* 服务启动：

```shell
gunicorn -w 1 -b 0.0.0.0:8502 app:app
```




* 服务接口测试：
	* 写在app.py同路径下api_test.py

```python
import requests

url = "http://localhost:8502/v1/models/beauty/"
data = {
    "word_list": "['我爱', '美妆']"
    }
res = requests.post(url, data=data, timeout=200)
print(res.text)
```

```
0.9228032
```


* 当前步骤总结：
	* 到这里，我们就完成了单个文本模型的服务封装，再一次温习了flask的使用，同学们可以自己动手将其他标签对应的模型也都封装成微服务。





