




### 任务简述


* 在过去年月里，由于算力的限制，一般公司无力支持图像的处理，因此主要以处理视频周边的文本，如title，获赞最多的评论内容等，作为标签化的输入，而今我们将迈出一步，以视频中的关键帧+文本信息作为输入构建多模态模型，来进行更精准的标签化。也是从这个任务开始我们正式学习多模态有关的技术，在该任务中开始使用多模态模型，最早期的多模态模型就是经典Resnet与GRU的结合，有了它，我们将能够综合利用图像信息和文本信息。



### 任务目的

* 构建起最基础的多模态处理模型Resnet+GRU，完成基于此的训练过程。


### 任务步骤
	
* Step1: 对多模态数据进行预处理和数据分析
* Step2: 构建Resnet+GRU模型
* Step3: 对Resnet+GRU模型进行训练

---





#### Step1: 对多模态数据进行预处理和数据分析

* 当前步骤简述：
	* 每一次使用模型前都需要对训练数据进行预处理和数据分析，多模态模型也不例外。这一次我们将定义使用多模态模型的数据结构，文本和图片以何种方式存储，以及该做哪些必要的分析工作。


* 数据规模要求：
	* 多模态需要文本-图像的对应数据，由于模型构造一般比较复杂，因此对数据总量的要求也比较高，生产中至少提供10万对多模态数据（单类别）。



* 数据存储形式：
	* 我们定义了这种数据存储结构，叫做jsonl

```text
# id 为序号，img 为图像的文件位置，label 为标签值，text 为文本内容
{"id":7,"img":"img/7.png","label":1,"text":"苦尽甘来时，我给你讲讲来时的路"}
{"id":10896,"img":"img/10896.png","label":0,"text":"when you try acid for the first time"}
```


* 多模态下的标签体系设计：
	* 之前的文本标签设计，依赖于文本中的内容描述来指向标签，比如谈论某个游戏人物就判断和游戏有关
	* 但这样的文本内容其实都是“直述”内容，最容易表达“直述”内容的应该是图像（游戏画面）,而文本（title）往往在表达情感，情绪（这个也和应用本身的定位有关），按照这个逻辑，多模态下的标签体系可以更加的丰富


![](http://www.tisv.cn:8900/img/label.png)


* 数据分析：
	* 对于文本：文本长度分布
	* 对于图像：图像长宽分布


* 重要说明：
> * 数据是严格的公司资产，因此我们在学习过程中一般无法拿到真实的数据，但是这并不影响我们在面试过程中阐述数据形式，和学习有关数据的技术，下面的数据分析将以“MELD”开源数据为例进行分析和讲解。
> * 数据集介绍：以视频和文本为模态的情感分析（分类任务）数据集
> * 数据集github地址：https://github.com/declare-lab/MELD/




* 具体实现内容：
	* 下载数据后你将看到很多视频文件，你需要截取它们的首帧图片
	* 在下载数据中你还会看到一份csv文件，你需要将它们转化成jsonl形式
	* 最后，你需要对其中的文本做长度分布以及获取那些首帧图片的宽高分布



* 代码实现位置：
	* 不存在的路径需要自己的创建，我们将把数据下载到下面的路径下
	* /data/labeled_project/multimodal_labeled/model_train/data/
	* 在该路径下创建data_process.py文件用于存储代码


#### 让我们动手做起来吧！


* 视频首帧截取代码实现：

```python
# pip3 install cv2
# 如果出现ImportError: libXext.so.6: cannot open shared object file: No such file or directory
# 可以执行：yum install libXext
import cv2
import os


input_path = "./dev_splits_complete/"
output_path = "./dev_sent_emo/"
if not os.path.exists(output_path):
    os.mkdir(output_path)




def get_jpg(input_path):
    files = os.listdir(input_path)
    for fn in files:
        uri = input_path + fn
        get_video_cover(uri, output_path)



def get_video_cover(uri, output_path):
    cap = cv2.VideoCapture(uri)
    rate = cap.get(5)
    frame_number = cap.get(7)  # 视频文件的帧数
    if rate==0:
        duration=0
    else:
        duration = int(frame_number / rate)  # 单位秒
    cap.set(1, 1)  # 取它的第一帧
    rval, frame = cap.read()  # 如果rval为False表示这个视频有问题，为True则正常
    filename = uri.split("/")[-1].split(".")[0] + ".jpg"
    cv2.imwrite(output_path + filename, frame)
    cap.release()
```



> * 运行示例：

```python
get_jpg(input_path)
```

> * 你将在dev_sent_emo路径下得到很多jpg图片，它们是对应视频的首帧。


* 将csv文件向jsonl转换代码实现：

```python
import pandas as pd

path = "./dev_sent_emo.csv"
dev_list = pd.read_csv(path).values.tolist()

with open("dev_sent_emo.jsonl", "w") as f:
    for dl in dev_list:
        if dl[4] != "neutral":
            if dl[4] == "negative":
                label = 0
            else:
                label = 1
            f.write(
                str(
                    {
                        "id": dl[0],
                        "text": dl[1],
                        "img": "./dev_sent_emo/dia"
                        + str(dl[5])
                        + "_utt"
                        + str(dl[6])
                        + ".jpg",
                        "label": label,
                    }
                )
                + "\n"
            )

```



* 统计文本长度分布代码实现：

```python
# 以csv文件为基础进行文本解析
# 导入必备工具包
# seaborn == 0.10.1
# matplotlib == 3.1.1
import seaborn as sns
import matplotlib.pyplot as plt


# 这里以给定的excel表格为输入
# 该数据可以在给定的原始代码中找到
# 可以将该段代码和数据拷贝到本地运行，查看可视化效果

# 读取csv文件
original_data = pd.read_csv(input_path)

# 分别在数据中添加新的句子长度列
original_data["sentence_length"] = list(
    map(lambda x: len(str(x)), original_data["Utterance"])
)

print("绘制句子长度分布图:")
sns.countplot("sentence_length", data=original_data)
# 主要关注count长度分布的纵坐标, 不需要绘制横坐标, 横坐标范围通过dist图进行查看
plt.xticks([])
plt.show()
plt.savefig("./fig1.png")


sns.distplot(original_data["sentence_length"])
# 主要关注dist长度分布横坐标, 不需要绘制纵坐标
plt.yticks([])
plt.show()
plt.savefig("./fig2.png")
```

> * 输出效果：

![](http://www.tisv.cn:8900/img/fig1.png)

![](http://www.tisv.cn:8900/img/fig2.png)



> * 分析:
	* 通过文本长度分布可以选择合适的模型截断补齐长度150.


* 获得图片宽-高分布代码实现：
	* 在我们的语料中，图片尺寸都是相同的，只需要看任意一张的宽高尺寸即可。


```python3
from PIL import Image
im = Image.open("./dev_sent_emo/dia9_utt7.jpg")
print(im.size[0], im.size[1])

# 1280 720
```



* 当前步骤总结：
	* 通过这一步，我们对原生的视频数据进行了处理，并对文本和图像进行一定的分析，确定了一些训练时需要的参数。


---


#### Step2: 构建Resnet+GRU模型 

* 当前步骤简述：
	* 在Transformer没有出现之前，早前的多模态构建更多就是根据已有的模型自定义结构，比如GRU处理文本，Resnet处理图像，再将两者的张量融合，最后通过一个网络做输出。
	* 将已有的模型融合或者说自定义业务需求的模型输入头，是算法工程师在企业里最常见也是最重要的工作。因此，在这里将带着大家来一步步构建Resnet和GRU结合的多模态模型。

> * 注：这里需要了解GRU和Resnet，解释梯度消失。



* 模型结构设计：
	* 在文本侧，首先经过Embedding层，再经过Bi-GRU得到高维张量。在图像侧，使用带有预训练参数的Resnet18，并使用一个全连接替换它的输出层，以便保证输出维度与Bi-GRU一致，最后将两侧输出张量加性融合通过一个全连接做二分类。



* 代码实现位置：
	* /data/labeled_project/multimodal_labeled/model_train/gru_resnet_train.py


#### 让我们动手做起来吧！


* 代码实现：

```python
import torch
import torch.nn as nn
import numpy as np
import torchvision
from torchvision import datasets, models, transforms


class ResnetGRUModel(nn.Module):
    def __init__(
        self,
        vocab_size,
        embed_dim,
        num_class,
        # max_length,
        dropout,
        hidden_size,
        num_layers,
    ):
        super(ResnetGRUModel, self).__init__()
        self.resnet_model = models.resnet18(pretrained=True)
        self.num_ftrs = self.resnet_model.fc.in_features
        self.resfc = nn.Linear(self.num_ftrs, hidden_size * 2)
        self.token_embedding = nn.Embedding(vocab_size, embed_dim)
        self.gru = nn.GRU(
            embed_dim,
            hidden_size,
            num_layers,
            bidirectional=True,
            batch_first=True,
            dropout=dropout,
        )
        self.fc = nn.Linear(hidden_size * 2, num_class)
        # self.init_weights()

    def init_weights(self):
        initrange = 0.5
        self.token_embedding.weight.data.uniform_(-initrange, initrange)
        self.resfc.weight.data.uniform_(-initrange, initrange)
        self.fc.weight.data.uniform_(-initrange, initrange)

    def forward(self, text, img):
        token_embedded = self.token_embedding(text)
        txt_out, _ = self.gru(token_embedded)
        # print(txt_out)
        # print(txt_out.shape)
        # print(txt_out[:, -1, :])
        # print(txt_out[:, -1, :].shape)
        self.resnet_model.fc = self.resfc
        img_out = self.resnet_model(img)
        out = txt_out[:, -1, :] + img_out
        out = self.fc(out)
        return out
```

> * 运行示例：

```python
if __name__ == "__main__":
    vocab_size = 10
    model = ResnetGRUModel(
               vocab_size=vocab_size,
               embed_dim=64,
               num_class=2,
               dropout=0.1,
               hidden_size=16,
               num_layers=2,
            )
    # 随机单条数据
    img = torch.rand(1, 3, 1280, 720)
    text = torch.tensor([[1, 2, 3, 4, 5, 6]])
    print(model(text, img))
```


```text
tensor([[ 4.7096, 11.9539]], grad_fn=<AddmmBackward>)
```

* 当前步骤总结：
	* 通过这一步，我们构建了Resnet+GRU模型，接下来我们将基于该模型进行训练。




---

#### Step3: 对Resnet+GRU模型进行训练

* 当前步骤简述：
	* 在上一步中，我们已经构建起了模型，这一步我们将完成它的训练流程。




* 具体实现内容：
	* 对文本处理：
		* 对文本进行tokenizer映射并记录映射器
		* 对文本进行截断补齐
	* 图像处理：
		* 定义一系列标准处理流程，Resize，张量化，规范化
	* 定义损失函数，优化器以及重要的超参数
	* 进行模型训练部分实现



* 代码实现位置：
	* /data/labeled_project/multimodal_labeled/model_train/gru_resnet_train.py 


#### 让我们动手做起来吧!

* 代码实现：

```python
# train_multimodal_data.jsonl
# {"id":7,"img":"./img/7.png","label":1,"text":"苦尽甘来时，我给你讲讲来时的路"}
# {"id":8,"img":"./img/8.png","label":0,"text":"苏州老大凌阿九！"}

# 读取数据并转成json
with open("./train_multimodal_data.jsonl", 'r') as fr:
    multimodal_data_list = fr.readlines()

# 处理文本读取时附带一些符号，比如换行符
multimodal_data_list = list(map(lambda x: eval(x[:-1]), multimodal_data_list))
print(multimodal_data_list)


## 文本处理流程

### 提取文本
multimodal_data_text = list(map(lambda x: x["text"], multimodal_data_list))


### 对文本进行tokenizer映射并记录映射器

import numpy as np

tokenizer = dict()
fit_text = set("".join(multimodal_data_text))
vocab_size = len(fit_text) + 1
tokenizer = {x : i+1 for i, x in enumerate(fit_text)}

print(tokenizer)


def _use_tokenizer(tokenizer, text:list, value=0):
    res = []
    for t in text:
        temp = []
        for i in t:
            temp.append(tokenizer.get(i, value))
        res.append(temp)
    return res



import json
json.dump(tokenizer, open("./gru_tokenizer.json", "w"))


### 对文本进行截断补齐
def _pad_sequences(text:list, max_length, value=0):
    res = []
    for txt in text:
        if len(txt) > max_length:
            res.append(txt[:max_length])
        else:
            res.append(txt + [value]*(max_length - len(txt)))
    return res

### 整合处理函数

def text_preprocess(text, tokenizer, max_length):
    text_token = _use_tokenizer(tokenizer, text)
    text_token = _pad_sequences(
        text_token, max_length, value=0,
    )
    return torch.tensor(text_token)


train_txt_list = text_preprocess(multimodal_data_text, tokenizer, 15)




## 图像处理流程

# 根据当前图片的尺寸进行设定
gold_size = (1280, 720)

# 使模型输入张量服从标准正态分布，第一个参数为均值列表，代表各个通道的均值，
# 第二个参数为标准差列表，代表各个通道的标准差。这里的图片都是有三个通道。
# 其中均值和标准差列表中的数值来自对ImageNet的全局采样结果。
gold_normalize = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])

# 定义一系列标准处理流程，Resize，张量化，规范化
# Resize和张量化用于统一图片尺寸和满足框架要求
# 规范化便于模型快速收敛


data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(gold_size),
        transforms.ToTensor(),
        transforms.Normalize(gold_normalize[0], gold_normalize[1])
    ]),
    'val': transforms.Compose([
        transforms.Resize(gold_size),
        transforms.ToTensor(),
        transforms.Normalize(gold_normalize[0], gold_normalize[1])
    ]),
}

train_data_dir_list = list(map(lambda x: x["img"], multimodal_data_list))

from PIL import Image

def img_loader(path):
    """图片读取"""
    with open(path, 'rb') as f:
        img = Image.open(f)
        return img.convert('RGB')

# 进行指定的转化
train_img_list = list(map(lambda x: data_transforms["train"](img_loader(x)), train_data_dir_list))


## 获取标签

train_label_list = list(map(lambda x: x["label"], multimodal_data_list))


## 整合数据成为DataLoader

from torch.utils.data import DataLoader

train_iter = list(zip(train_img_list, train_txt_list, train_label_list))

BATCH_SIZE = 8

train_dataloader = DataLoader(
    train_iter, batch_size=BATCH_SIZE, shuffle=True
)

print(train_dataloader)
```


* 定义损失函数，优化器以及重要的超参数，训练和评估你的模型

```python
import time

# Hyperparameters
EPOCHS = 20  # epoch
LR = 0.05  # learning rate

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 0.99, gamma=0.1)
total_accu = None

# 暂且认为三个数据集是相同的
test_dataloader = valid_dataloader = train_dataloader

def train(dataloader):
    model.train()
    total_acc, total_count = 0, 0
    log_interval = 500
    start_time = time.time()
    for idx, (img, txt, label) in enumerate(dataloader):
        optimizer.zero_grad()
        predited_label = model(txt, img)
        loss = criterion(predited_label, label)
        loss.backward()
        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
        optimizer.step()
        total_acc += (predited_label.argmax(1) == label).sum().item()
        total_count += label.size(0)
        if idx % log_interval == 0 and idx > 0:
            elapsed = time.time() - start_time
            print(
                "| epoch {:3d} | {:5d}/{:5d} batches "
                "| accuracy {:8.3f}".format(
                    epoch, idx, len(dataloader), total_acc / total_count
                )
            )
            total_acc, total_count = 0, 0
            start_time = time.time()


def evaluate(dataloader):
    model.eval()
    total_acc, total_count = 0, 0

    with torch.no_grad():
        for idx, (img, txt, label) in enumerate(dataloader):
            predited_label = model(txt, img)
            loss = criterion(predited_label, label)
            total_acc += (predited_label.argmax(1) == label).sum().item()
            total_count += label.size(0)
    return total_acc / total_count


for epoch in range(1, EPOCHS + 1):
    epoch_start_time = time.time()
    train(train_dataloader)
    accu_val = evaluate(valid_dataloader)
    if total_accu is not None and total_accu > accu_val:
        scheduler.step()
    else:
        total_accu = accu_val
    print("-" * 59)
    print(
        "| end of epoch {:3d} | time: {:5.2f}s | "
        "valid accuracy {:8.3f} ".format(
            epoch, time.time() - epoch_start_time, accu_val
        )
    )
    print("-" * 59)


print("Checking the results of test dataset.")
accu_test = evaluate(test_dataloader)
print("test accuracy {:8.3f}".format(accu_test))

torch.save(model.state_dict(), './model_weights.pth')
```


> * 输出效果:

```text

```

* 当前步骤总结：
	* 我们通过该步骤完成了任务五的全部内容，掌握了多模态模型的数据处理和训练逻辑，我们将在接下来的任务中尝试效果更好的模型。


