


<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <link rel="canonical" href="https://www.tisv.cn/7/">
      
      
      <link rel="shortcut icon" href="../img/AI.jpg">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-5.5.3">
    
    
      
        <title>7 - 基于多模态的视频标签化系统</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.947af8d5.min.css">
      
      
    
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
        
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-36723568-3","mkdocs.org"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview",document.location.pathname)})</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#71" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href="https://www.tisv.cn/" title="基于多模态的视频标签化系统" class="md-header-nav__button md-logo" aria-label="基于多模态的视频标签化系统">
      
  <img src="../img/AI.jpg" alt="logo">

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            基于多模态的视频标签化系统
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              7
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/AITutorials/datasets" title="前往 GitHub 仓库" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    Github | Give Me A Star
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
        
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="https://www.tisv.cn/" title="基于多模态的视频标签化系统" class="md-nav__button md-logo" aria-label="基于多模态的视频标签化系统">
      
  <img src="../img/AI.jpg" alt="logo">

    </a>
    基于多模态的视频标签化系统
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/AITutorials/datasets" title="前往 GitHub 仓库" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    Github | Give Me A Star
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../2/" title="第二章：多模态整体解决方案" class="md-nav__link">
      第二章：多模态整体解决方案
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../101/" title="项目背景" class="md-nav__link">
      项目背景
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../102/" title="任务一:构建文本标签化主服务" class="md-nav__link">
      任务一:构建文本标签化主服务
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../103/" title="任务二:构建标签词汇图谱" class="md-nav__link">
      任务二:构建标签词汇图谱
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../104/" title="任务三:文本标签化模型的训练和部署" class="md-nav__link">
      任务三:文本标签化模型的训练和部署
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../105/" title="任务四:文本标签化服务的分布式集成" class="md-nav__link">
      任务四:文本标签化服务的分布式集成
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../106/" title="任务五:使用Resnet+GRU进行多模态处理" class="md-nav__link">
      任务五:使用Resnet+GRU进行多模态处理
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../107/" title="任务六:使用VisualBERT进行多模态处理" class="md-nav__link">
      任务六:使用VisualBERT进行多模态处理
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../108.md" title="任务七:模态交互方式Co-Attention的优化改进" class="md-nav__link">
      任务七:模态交互方式Co-Attention的优化改进
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </span>
      目录
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#71" class="md-nav__link">
    7.1 模型量化技术
  </a>
  
    <nav class="md-nav" aria-label="7.1 模型量化技术">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    相关知识
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    数据集说明
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#huggingfacebert" class="md-nav__link">
    使用huggingface中的预训练BERT模型进行微调
  </a>
  
    <nav class="md-nav" aria-label="使用huggingface中的预训练BERT模型进行微调">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    第一步: 安装核心的工具包并导入
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    第二步: 下载数据集并使用脚本进行微调
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    第三步: 设定全局配置并加载微调模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    第四步: 编写用于模型使用的评估函数
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bert" class="md-nav__link">
    使用动态量化技术对训练后的bert模型进行压缩
  </a>
  
    <nav class="md-nav" aria-label="使用动态量化技术对训练后的bert模型进行压缩">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    第一步: 将模型应用动态量化技术
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    第二步: 对比压缩后模型的大小
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    第三步: 对比压缩后的模型的推理准确性和耗时
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    第四步: 序列化模型以便之后使用
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    附录
  </a>
  
    <nav class="md-nav" aria-label="附录">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#run_gluepy" class="md-nav__link">
    run_glue.py微调脚本代码
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#72" class="md-nav__link">
    7.2 模型剪枝技术
  </a>
  
    <nav class="md-nav" aria-label="7.2 模型剪枝技术">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    剪枝技术介绍与原理
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prune" class="md-nav__link">
    使用prune对已有模型进行剪枝
  </a>
  
    <nav class="md-nav" aria-label="使用prune对已有模型进行剪枝">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    第一步：导入工具包并获得模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    第二步：使用剪枝工具并了解其作用方式
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    第三步：持久化修剪后的模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_18" class="md-nav__link">
    第四步：工程中常用的修剪方法
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_19" class="md-nav__link">
    小节总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#73-onnx-runtime" class="md-nav__link">
    7.3 使用ONNX-Runtime进行模型推断加速
  </a>
  
    <nav class="md-nav" aria-label="7.3 使用ONNX-Runtime进行模型推断加速">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_20" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#onnxonnx-runtime" class="md-nav__link">
    什么是ONNX和ONNX-Runtime
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#onnx-runtime" class="md-nav__link">
    使用ONNX-Runtime进行模型推断加速的步骤
  </a>
  
    <nav class="md-nav" aria-label="使用ONNX-Runtime进行模型推断加速的步骤">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_21" class="md-nav__link">
    第一步：安装必备的工具包
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#onnx" class="md-nav__link">
    第二步：将已有模型转换成ONNX格式
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#onnx-runtime_1" class="md-nav__link">
    第三步：使用ONNX-Runtime进行模型预测
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_22" class="md-nav__link">
    第四步：对比结果差异和推断时间
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#onnx-runtime_2" class="md-nav__link">
    ONNX-Runtime能够加速的原理
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_23" class="md-nav__link">
    小节总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  
                
                
                  <h1>7</h1>
                
                <h2 id="71">7.1 模型量化技术</h2>
<h3 id="_1">学习目标</h3>
<ul>
<li>了解模型压缩技术中的动态量化与静态量化的相关知识。</li>
<li>掌握使用huggingface中的预训练BERT模型进行微调。</li>
<li>掌握使用动态量化技术对训练后的bert模型进行压缩。</li>
</ul>
<hr />
<p><center><img alt="avatar" src="../img/bert1.png" /></center></p>
<hr />
<h3 id="_2">相关知识</h3>
<ul>
<li>模型压缩:<ul>
<li>模型压缩是一种针对大型模型(参数量巨大)在使用过程中进行优化的一种常用措施。它往往能够使模型体积缩小，简化计算，增快推断速度，满足模型在特定场合(如: 移动端)的需求。目前，模型压缩可以从多方面考虑，如剪枝方法(简化模型架构)，参数量化方法(简化模型参数)，知识蒸馏等。本案例将着重讲解模型参数量化方法。</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>模型参数量化:<ul>
<li>在机器学习（深度学习）领域，模型量化一般是指将模型参数由类型FP32转换为INT8的过程，转换之后的模型大小被压缩为原来的1/4，所需内存和带宽减小4倍，同时，计算量减小约为2-4倍。模型又可分为动态量化和静态量化。</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>模型动态量化：<ul>
<li>操作最简单也是压缩效果最好的量化方式，量化过程发生在模型训练后，针对模型权重采取量化，之后会在模型预测过程中，再决定是否针对激活值采取量化，因此称作动态量化（在预测时可能发生量化）。这是我们本案例将会使用的量化方式。</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>模型静态量化：<ul>
<li>考虑到动态量化这种“一刀切”的量化方式有时会带来模型预测效果的大幅度下降，因此引入静态量化，它同样发生在模型训练后，为了判断哪些权重或激活值应该被量化，哪些应该保留或小幅度量化，在预测过程开始前，在模型中节点插入“观测者”（衡量节点使用情况的一些计算方法），他们将在一些实验数据中评估节点使用情况，来决定是否将其权重或激活值进行量化，因为在预测过程中，这些节点是否被量化已经确定，因此称作静态量化。</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>（扩展知识）量化意识训练：<ul>
<li>这是一种操作相对复杂的模型量化方法，在训练过程中使用它，原理与静态量化类似，都需要像模型中插入“观测者”，同时它还需要插入量化计算操作，使得模型训练过程中除了进行原有的浮点型计算，还要进行量化计算，但模型参数的更新还是使用浮点型，而量化计算的作用就是让模型“意识”到这个过程，通过“观测者”评估每次量化结果与训练过程中参数更新程度，为之后模型如何进行量化还能保证准确率提供衡量指标。（类似于，人在接受训练时，意识到自己接下来可能除了训练内容外，还会接受其他“操作”（量化），因此也会准备一些如果进行量化仍能达成目标的措施）</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>BERT模型:<ul>
<li>这里使用bert-base-uncased，它的编码器具有12个隐层, 输出768维张量, 12个自注意力头, 共110M参数量, 在小写的英文文本上进行训练而得到。</li>
</ul>
</li>
</ul>
<hr />
<h3 id="_3">数据集说明</h3>
<ul>
<li>
<p>GLUE数据集合的介绍：</p>
<ul>
<li>GLUE由纽约大学，华盛顿大学，Google联合推出，涵盖不同的NLP任务类型，持续至2020年1月，其中包括11个子任务数据集，成为NLP研究发展的标准。我们这里使用其实MRPC数据集。</li>
</ul>
</li>
<li>
<p>数据下载地址: 标准数据集一般使用下载脚本进行下载，会在之后的代码中演示。</p>
</li>
<li>
<p>MRPC数据集的任务类型：</p>
<ul>
<li>句子对二分类任务<ul>
<li>训练集上正样本占68%，负样本占32%</li>
</ul>
</li>
<li>评估指标这里使用：F1</li>
<li>评估指标计算方式：F1=2∗(precision∗recall)/(precision+recall)</li>
</ul>
</li>
<li>
<p>数据集预览:</p>
</li>
</ul>
<blockquote>
<ul>
<li>MRPC数据集文件样式：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="err">- MRPC/</span>
<span class="err">        - dev.tsv</span>
<span class="err">        - test.tsv</span>
<span class="err">        - train.tsv</span>
<span class="err">    - dev_ids.tsv</span>
<span class="err">    - msr_paraphrase_test.txt</span>
<span class="err">    - msr_paraphrase_train.txt</span>
</code></pre></div>


<blockquote>
<ul>
<li>文件样式说明：<ul>
<li>在使用中常用到的文件是train.tsv，dev.tsv，test.tsv，分别代表训练集，验证集和测试集。其中train.tsv与dev.tsv数据样式相同，都是带有标签的数据，其中test.tsv是不带有标签的数据。</li>
</ul>
</li>
</ul>
</blockquote>
<ul>
<li>train.tsv数据样式：</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="err">Quality #1 ID   #2 ID   #1 String   #2 String</span>
<span class="err">1   702876  702977  Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence . Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .</span>
<span class="err">0   2108705 2108831 Yucaipa owned Dominick &#39;s before selling the chain to Safeway in 1998 for $ 2.5 billion .   Yucaipa bought Dominick &#39;s in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .</span>
<span class="err">1   1330381 1330521 They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .   On June 10 , the ship &#39;s owners had published an advertisement on the Internet , offering the explosives for sale .</span>
<span class="err">0   3344667 3344648 Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 . Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .</span>
<span class="err">1   1236820 1236712 The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange .   PG &amp; E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .</span>
<span class="err">1   738533  737951  Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier .   With the scandal hanging over Stewart &#39;s company , revenue the first quarter of the year dropped 15 percent from the same period a year earlier .</span>
<span class="err">0   264589  264502  The Nasdaq had a weekly gain of 17.27 , or 1.2 percent , closing at 1,520.15 on Friday .    The tech-laced Nasdaq Composite .IXIC rallied 30.46 points , or 2.04 percent , to 1,520.15 .</span>
<span class="err">1   579975  579810  The DVD-CCA then appealed to the state Supreme Court .  The DVD CCA appealed that decision to the U.S. Supreme Court .</span>
<span class="err">...</span>
</code></pre></div>


<blockquote>
<ul>
<li>train.tsv数据样式说明：<ul>
<li>train.tsv中的数据内容共分为5列，第一列数据，0或1，代表每对句子是否具有相同的含义，0代表含义不相同，1代表含义相同。第二列和第三列分别代表每对句子的id，第四列和第五列分别具有相同/不同含义的句子对。</li>
</ul>
</li>
</ul>
</blockquote>
<hr />
<ul>
<li>test.tsv数据样式：</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="err">index   #1 ID   #2 ID   #1 String   #2 String</span>
<span class="err">0   1089874 1089925 PCCW &#39;s chief operating officer , Mike Butcher , and Alex Arena , the chief financial officer , will report directly to Mr So . Current Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So .</span>
<span class="err">1   3019446 3019327 The world &#39;s two largest automakers said their U.S. sales declined more than predicted last month as a late summer sales frenzy caused more of an industry backlash than expected . Domestic sales at both GM and No. 2 Ford Motor Co. declined more than predicted as a late summer sales frenzy prompted a larger-than-expected industry backlash .</span>
<span class="err">2   1945605 1945824 According to the federal Centers for Disease Control and Prevention ( news - web sites ) , there were 19 reported cases of measles in the United States in 2002 .   The Centers for Disease Control and Prevention said there were 19 reported cases of measles in the United States in 2002 .</span>
<span class="err">3   1430402 1430329 A tropical storm rapidly developed in the Gulf of Mexico Sunday and was expected to hit somewhere along the Texas or Louisiana coasts by Monday night . A tropical storm rapidly developed in the Gulf of Mexico on Sunday and could have hurricane-force winds when it hits land somewhere along the Louisiana coast Monday night .</span>
<span class="err">4   3354381 3354396 The company didn &#39;t detail the costs of the replacement and repairs .   But company officials expect the costs of the replacement work to run into the millions of dollars .</span>
<span class="err">5   1390995 1391183 The settling companies would also assign their possible claims against the underwriters to the investor plaintiffs , he added . Under the agreement , the settling companies will also assign their potential claims against the underwriters to the investors , he added .</span>
<span class="err">6   2201401 2201285 Air Commodore Quaife said the Hornets remained on three-minute alert throughout the operation . Air Commodore John Quaife said the security operation was unprecedented .</span>
<span class="err">7   2453843 2453998 A Washington County man may have the countys first human case of West Nile virus , the health department said Friday .  The countys first and only human case of West Nile this year was confirmed by health officials on Sept . 8 .</span>
<span class="err">...</span>
</code></pre></div>


<blockquote>
<ul>
<li>test.tsv数据样式说明：<ul>
<li>test.tsv中的数据内容共分为5列，第一列数据代表每条文本数据的索引；其余列的含义与train.tsv中相同。</li>
</ul>
</li>
</ul>
</blockquote>
<hr />
<h3 id="huggingfacebert">使用huggingface中的预训练BERT模型进行微调</h3>
<ul>
<li>第一步: 安装必要的工具包并导入</li>
<li>第二步: 下载数据集并使用脚本进行微调</li>
<li>第三步: 设定全局配置并加载微调模型</li>
<li>第四步: 编写用于模型使用的评估函数</li>
</ul>
<hr />
<h4 id="_4">第一步: 安装核心的工具包并导入</h4>
<ul>
<li>安装核心工具包:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="err"># 这是由huggingface提供的预训练模型使用工具包</span>
<span class="err">pip install transformers==2.3.0</span>
</code></pre></div>


<ul>
<li>工具包导入</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span><span class="p">,</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># 用于设定全局配置的命名空间</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>

<span class="c1"># 从torch.utils.data中导入常用的模型处理工具，会在代码使用中进行详细介绍</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="p">(</span><span class="n">DataLoader</span><span class="p">,</span> <span class="n">RandomSampler</span><span class="p">,</span> <span class="n">SequentialSampler</span><span class="p">,</span>
                              <span class="n">TensorDataset</span><span class="p">)</span>

<span class="c1"># 模型进度可视化工具，在评估过程中，帮助打印进度条</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1"># 从transformers中导入BERT模型的相关工具</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span><span class="n">BertConfig</span><span class="p">,</span> <span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">BertTokenizer</span><span class="p">,)</span>

<span class="c1"># 从transformers中导入GLUE数据集的评估指标计算方法</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">glue_compute_metrics</span> <span class="k">as</span> <span class="n">compute_metrics</span>

<span class="c1"># 从transformers中导入GLUE数据集的输出模式(回归/分类)</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">glue_output_modes</span> <span class="k">as</span> <span class="n">output_modes</span>

<span class="c1"># 从transformers中导入GLUE数据集的预处理器processors</span>
<span class="c1"># processors是将持久化文件加载到内存的过程，即输入一般为文件路径，输出是训练数据和对应标签的某种数据结构，如列表表示。</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">glue_processors</span> <span class="k">as</span> <span class="n">processors</span>

<span class="c1"># 从transformers中导入GLUE数据集的特征处理器convert_examples_to_features</span>
<span class="c1"># convert_examples_to_features是将processor的输出处理成模型需要的输入，NLP定中的一般流程为数值映射，指定长度的截断补齐等</span>
<span class="c1"># 在BERT模型上处理句子对时，还需要在句子前插入[CLS]开始标记，在两个句子中间和第二个句子末端插入[SEP]分割/结束标记</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">glue_convert_examples_to_features</span> <span class="k">as</span> <span class="n">convert_examples_to_features</span>


<span class="c1"># 设定与日志打印有关的配置</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="nb">format</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1"> - </span><span class="si">%(levelname)s</span><span class="s1"> - </span><span class="si">%(name)s</span><span class="s1"> -   </span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span>
                    <span class="n">datefmt</span> <span class="o">=</span> <span class="s1">&#39;%m/</span><span class="si">%d</span><span class="s1">/%Y %H:%M:%S&#39;</span><span class="p">,</span>
                    <span class="n">level</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">)</span>

<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;transformers.modeling_utils&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span>
   <span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">)</span>  <span class="c1"># Reduce logging</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;torch version:&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>

<span class="c1"># 设置torch允许启动的线程数, 因为之后会对比压缩模型的耗时，因此防止该变量产生影响</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_num_threads</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__config__</span><span class="o">.</span><span class="n">parallel_info</span><span class="p">())</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>torch version: 1.3.1

ATen/Parallel:
    at::get_num_threads() : 1
    at::get_num_interop_threads() : 8
OpenMP 201511 (a.k.a. OpenMP 4.5)
    omp_get_max_threads() : 1
Intel(R) Math Kernel Library Version 2019.0.4 Product Build 20190411 for Intel(R) 64 architecture applications
    mkl_get_max_threads() : 1
Intel(R) MKL-DNN v0.20.5 (Git Hash 0125f28c61c1f822fd48570b4c1066f96fcb9b2e)
std::thread::hardware_concurrency() : 16
Environment variables:
    OMP_NUM_THREADS : [not set]
    MKL_NUM_THREADS : [not set]
ATen parallel backend: OpenMP
</code></pre></div>


<hr />
<h4 id="_5">第二步: 下载数据集并使用脚本进行微调</h4>
<ul>
<li>下载GLUE中的MRPC数据集:</li>
</ul>
<div class="codehilite"><pre><span></span><code>python download_glue_data.py --data_dir<span class="o">=</span><span class="s1">&#39;./glue_data&#39;</span> --tasks<span class="o">=</span><span class="s1">&#39;MRPC&#39;</span>
</code></pre></div>


<hr />
<ul>
<li>使用<a href="http://47.92.175.143:8008/4/#1">run_glue.py[具体代码内容见附录]</a>进行模型微调:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="c1"># 注意: 这是一段使用shell运行的脚本, 运行过程中需要请求AWS的S3进行预训练模型下载</span>

<span class="c1"># 定义GLUE_DIR: 微调数据所在路径, 这里我们使用glue_data中的数据作为微调数据</span>
<span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>./glue_data
<span class="c1"># 定义OUT_DIR: 模型的保存路径, 我们将模型保存在当前目录的bert_finetuning_test文件中</span>
<span class="nb">export</span> <span class="nv">OUT_DIR</span><span class="o">=</span>./bert_finetuning_test/

python ./run_glue.py <span class="se">\</span>
    --model_type bert <span class="se">\</span>
    --model_name_or_path bert-base-uncased <span class="se">\</span>
    --task_name MRPC <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --do_lower_case <span class="se">\</span>
    --data_dir <span class="nv">$GLUE_DIR</span>/MRPC <span class="se">\</span>
    --max_seq_length <span class="m">128</span> <span class="se">\</span>
    --per_gpu_eval_batch_size<span class="o">=</span><span class="m">8</span>   <span class="se">\</span>
    --per_gpu_train_batch_size<span class="o">=</span><span class="m">8</span>   <span class="se">\</span>
    --learning_rate 2e-5 <span class="se">\</span>
    --num_train_epochs <span class="m">1</span>.0 <span class="se">\</span>
    --output_dir <span class="nv">$OUT_DIR</span>

<span class="c1"># 使用python运行微调脚本</span>
<span class="c1"># --model_type: 选择需要微调的模型类型, 这里可以选择BERT, XLNET, XLM, roBERTa, distilBERT, ALBERT</span>
<span class="c1"># --model_name_or_path: 选择具体的模型或者变体, 这里是在英文语料上微调, 因此选择bert-base-uncased</span>
<span class="c1"># --task_name: 它将代表对应的任务类型, 如MRPC代表句子对二分类任务</span>
<span class="c1"># --do_train: 使用微调脚本进行训练</span>
<span class="c1"># --do_eval: 使用微调脚本进行验证</span>
<span class="c1"># --data_dir: 训练集及其验证集所在路径, 将自动寻找该路径下的train.tsv和dev.tsv作为训练集和验证集</span>
<span class="c1"># --max_seq_length: 输入句子的最大长度, 超过则截断, 不足则补齐</span>
<span class="c1"># --learning_rate: 学习率</span>
<span class="c1"># --num_train_epochs: 训练轮数</span>
<span class="c1"># --output_dir $OUT_DIR: 训练后的模型保存路径</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>...
03/18/2020 00:55:17 - INFO - __main__ -   Loading features from cached file ./glue_data/MRPC/cached_train_bert-base-uncased_128_mrpc
03/18/2020 00:55:17 - INFO - __main__ -   ***** Running training *****
03/18/2020 00:55:17 - INFO - __main__ -     Num examples = 3668
03/18/2020 00:55:17 - INFO - __main__ -     Num Epochs = 1
03/18/2020 00:55:17 - INFO - __main__ -     Instantaneous batch size per GPU = 8
03/18/2020 00:55:17 - INFO - __main__ -     Total train batch size (w. parallel, distributed &amp; accumulation) = 8
03/18/2020 00:55:17 - INFO - __main__ -     Gradient Accumulation steps = 1
03/18/2020 00:55:17 - INFO - __main__ -     Total optimization steps = 459
Epoch:   0%|                 | 0/1 [00:00&lt;?, ?it/s]
Iteration:   2%|   | 8/459 [00:13&lt;12:42,  1.69s/it]
</code></pre></div>


<blockquote>
<ul>
<li>运行成功后会在当前目录下生成 ./bert_finetuning_test文件夹，内部文件如下:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="err">added_tokens.json  checkpoint-200  checkpoint-350  eval_results.txt         tokenizer_config.json</span>
<span class="err">checkpoint-100     checkpoint-250  checkpoint-50   pytorch_model.bin        training_args.bin</span>
<span class="err">checkpoint-150     checkpoint-300  config.json     special_tokens_map.json  vocab.txt</span>
</code></pre></div>


<hr />
<h4 id="_6">第三步: 设定全局配置并加载微调模型</h4>
<ul>
<li>设定全局配置:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="c1"># 这些配置将在调用微调模型时进行使用</span>

<span class="c1"># 实例化一个配置的命名空间</span>
<span class="n">configs</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">()</span>

<span class="c1"># 模型的输出文件路径</span>
<span class="n">configs</span><span class="o">.</span><span class="n">output_dir</span> <span class="o">=</span> <span class="s2">&quot;./bert_finetuning_test/&quot;</span>

<span class="c1"># 验证数据集所在路径(与训练集相同)</span>
<span class="n">configs</span><span class="o">.</span><span class="n">data_dir</span> <span class="o">=</span> <span class="s2">&quot;./glue_data/MRPC&quot;</span>

<span class="c1"># 预训练模型的名字</span>
<span class="n">configs</span><span class="o">.</span><span class="n">model_name_or_path</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span>

<span class="c1"># 文本的最大对齐长度</span>
<span class="n">configs</span><span class="o">.</span><span class="n">max_seq_length</span> <span class="o">=</span> <span class="mi">128</span>

<span class="c1"># GLUE中的任务名(需要小写)</span>
<span class="n">configs</span><span class="o">.</span><span class="n">task_name</span> <span class="o">=</span> <span class="s2">&quot;MRPC&quot;</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

<span class="c1"># 根据任务名从GLUE数据集处理工具包中取出对应的预处理工具</span>
<span class="n">configs</span><span class="o">.</span><span class="n">processor</span> <span class="o">=</span> <span class="n">processors</span><span class="p">[</span><span class="n">configs</span><span class="o">.</span><span class="n">task_name</span><span class="p">]()</span>

<span class="c1"># 得到对应模型输出模式(MRPC为分类)</span>
<span class="n">configs</span><span class="o">.</span><span class="n">output_mode</span> <span class="o">=</span> <span class="n">output_modes</span><span class="p">[</span><span class="n">configs</span><span class="o">.</span><span class="n">task_name</span><span class="p">]</span>

<span class="c1"># 得到该任务的对应的标签种类列表</span>
<span class="n">configs</span><span class="o">.</span><span class="n">label_list</span> <span class="o">=</span> <span class="n">configs</span><span class="o">.</span><span class="n">processor</span><span class="o">.</span><span class="n">get_labels</span><span class="p">()</span>

<span class="c1"># 定义模型类型</span>
<span class="n">configs</span><span class="o">.</span><span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;bert&quot;</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

<span class="c1"># 是否全部使用小写文本</span>
<span class="n">configs</span><span class="o">.</span><span class="n">do_lower_case</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># 使用的设备</span>
<span class="n">configs</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span>
<span class="c1"># 每次验证的批次大小</span>
<span class="n">configs</span><span class="o">.</span><span class="n">per_eval_batch_size</span> <span class="o">=</span> <span class="mi">8</span>

<span class="c1"># gpu的数量</span>
<span class="n">configs</span><span class="o">.</span><span class="n">n_gpu</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># 是否需要重写数据缓存</span>
<span class="n">configs</span><span class="o">.</span><span class="n">overwrite_cache</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div>


<hr />
<ul>
<li>加载微调模型:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="o">#</span> <span class="err">因为在模型使用中，会使用一些随机方法，为了使每次运行的结果可以复现</span>
<span class="o">#</span> <span class="err">需要设定确定的随机种子，保证每次随机化的数字在范围内浮动</span>
<span class="n">def</span> <span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
    <span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>


<span class="o">##</span> <span class="err">加载微调模型</span>

<span class="o">#</span> <span class="err">加载</span><span class="n">BERT预训练模型的数值映射器</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">configs</span><span class="p">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">do_lower_case</span><span class="o">=</span><span class="n">configs</span><span class="p">.</span><span class="n">do_lower_case</span><span class="p">)</span>

<span class="o">#</span> <span class="err">加载带有文本分类头的</span><span class="n">BERT模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">configs</span><span class="p">.</span><span class="n">output_dir</span><span class="p">)</span>

<span class="o">#</span> <span class="err">将模型传到制定设备上</span>
<span class="n">model</span><span class="p">.</span><span class="k">to</span><span class="p">(</span><span class="n">configs</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div>


<hr />
<h4 id="_7">第四步: 编写用于模型使用的评估函数</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    模型评估函数</span>
<span class="sd">    :param args: 模型的全局配置对象，里面包含模型的各种配置信息</span>
<span class="sd">    :param model: 使用的模型</span>
<span class="sd">    :param tokenizer: 文本数据的数值映射器</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># 因为之后会多次用到任务名和输出路径</span>
    <span class="c1"># 所以将其从参数中取出</span>
    <span class="n">eval_task</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">task_name</span>
    <span class="n">eval_output_dir</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">output_dir</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># 调用load_and_cache_examples加载原始或者已经缓存的数据</span>
        <span class="c1"># 得到一个验证数据集的迭代器对象</span>
        <span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">load_and_cache_examples</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">eval_task</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>

        <span class="c1"># 判断模型输出路径是否存在</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">eval_output_dir</span><span class="p">):</span>
            <span class="c1"># 不存在的话，创建该路径</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">eval_output_dir</span><span class="p">)</span>

        <span class="c1"># 使用SequentialSampler封装验证数据集的迭代器对象</span>
        <span class="c1"># SequentialSampler是采样器对象，一般在Dataloader数据加载器中使用，</span>
        <span class="c1"># 因为数据加载器是以迭代的方式产生数据，因此每个批次数据可以指定采样规则，</span>
        <span class="c1"># SequentialSampler是顺序采样器，不改变原有数据集的顺序，依次取出数据。</span>
        <span class="n">eval_sampler</span> <span class="o">=</span> <span class="n">SequentialSampler</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">)</span>
        <span class="c1"># 使用Dataloader数据加载器，参数分别是数据集的迭代器对象，采集器对象，批次大小</span>
        <span class="n">eval_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">eval_sampler</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">per_eval_batch_size</span><span class="p">)</span>

        <span class="c1"># 开始评估</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;***** Running evaluation *****&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;  Num examples = </span><span class="si">%d</span><span class="s2">&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">))</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;  Batch size = </span><span class="si">%d</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">per_eval_batch_size</span><span class="p">)</span>
        <span class="c1"># 初始化验证损失</span>
        <span class="n">eval_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="c1"># 初始化验证步数</span>
        <span class="n">nb_eval_steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># 初始化预测的概率分布</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># 初始化输出真实标签值</span>
        <span class="n">out_label_ids</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># 循环数据批次，使用tqdm封装数据加载器，可以在评估时显示进度条</span>
        <span class="c1"># desc是进度条前面的描述信息</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">eval_dataloader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Evaluating&quot;</span><span class="p">):</span>
            <span class="c1"># 评估过程中模型开启评估模式，不进行反向传播</span>
            <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
            <span class="c1"># 从batch中取出数据的所有相关信息存于元组中</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>
            <span class="c1"># 不进行梯度计算</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="c1"># 将batch携带的数据信息表示称字典形式</span>
                <span class="c1"># 这些数据信息和load_and_cache_examples函数返回的数据对象中信息相同</span>
                <span class="c1"># 词汇的映射数值, 词汇的类型数值(0或1, 代表第一句和第二句话)</span>
                <span class="c1"># 注意力掩码张量，以及对应的标签</span>
                <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;input_ids&#39;</span><span class="p">:</span>      <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                          <span class="s1">&#39;attention_mask&#39;</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                          <span class="s1">&#39;token_type_ids&#39;</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                          <span class="s1">&#39;labels&#39;</span><span class="p">:</span>         <span class="n">batch</span><span class="p">[</span><span class="mi">3</span><span class="p">]}</span>
                <span class="c1"># 将该字典作为参数输入到模型中获得输出</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
                <span class="c1"># 获得损失和预测分布</span>
                <span class="n">tmp_eval_loss</span><span class="p">,</span> <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span>
                <span class="c1"># 将损失累加求均值</span>
                <span class="n">eval_loss</span> <span class="o">+=</span> <span class="n">tmp_eval_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="c1"># 验证步数累加</span>
            <span class="n">nb_eval_steps</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># 如果是第一批次的数据</span>
            <span class="k">if</span> <span class="n">preds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># 结果分布就是模型的输出分布</span>
                <span class="n">preds</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="c1"># 输出真实标签值为输入对应的labels</span>
                <span class="n">out_label_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># 结果分布就是每一次模型输出分布组成的数组</span>
                <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="c1"># 输出真实标签值为每一次输入对应的labels组成的数组</span>
                <span class="n">out_label_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out_label_ids</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># 计算每一轮的平均损失</span>
        <span class="n">eval_loss</span> <span class="o">=</span> <span class="n">eval_loss</span> <span class="o">/</span> <span class="n">nb_eval_steps</span>
        <span class="c1"># 取结果分布中最大的值对应的索引</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># 使用compute_metrics计算对应的评估指标</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">compute_metrics</span><span class="p">(</span><span class="n">eval_task</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">out_label_ids</span><span class="p">)</span>
        <span class="c1"># 在日志中打印每一轮的评估结果</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;***** Eval results </span><span class="si">{}</span><span class="s2"> *****&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
         <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>


<span class="k">def</span> <span class="nf">load_and_cache_examples</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">task</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    加载或使用缓存数据</span>
<span class="sd">    :param args: 全局配置参数</span>
<span class="sd">    :param task: 任务名</span>
<span class="sd">    :param tokenizer: 数值映射器</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># 根据任务名(MRPC)获得对应数据预处理器</span>
    <span class="n">processor</span> <span class="o">=</span> <span class="n">processors</span><span class="p">[</span><span class="n">task</span><span class="p">]()</span>
    <span class="c1"># 获得输出模式</span>
    <span class="n">output_mode</span> <span class="o">=</span> <span class="n">output_modes</span><span class="p">[</span><span class="n">task</span><span class="p">]</span>
    <span class="c1"># 定义缓存数据文件的名字</span>
    <span class="n">cached_features_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;cached_</span><span class="si">{}</span><span class="s1">_</span><span class="si">{}</span><span class="s1">_</span><span class="si">{}</span><span class="s1">_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="s1">&#39;dev&#39;</span><span class="p">,</span>
        <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">model_name_or_path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)))</span><span class="o">.</span><span class="n">pop</span><span class="p">(),</span>
        <span class="nb">str</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_seq_length</span><span class="p">),</span>
        <span class="nb">str</span><span class="p">(</span><span class="n">task</span><span class="p">)))</span>
    <span class="c1"># 判断缓存文件是否存在，以及全局配置中是否需要重写数据</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">cached_features_file</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">overwrite_cache</span><span class="p">:</span>
        <span class="c1"># 使用torch.load(解序列化，一般用于加载模型，在这里用于加载训练数据)加载缓存文件</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">cached_features_file</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># 如果没有缓存文件，则需要使用processor从原始数据路径中加载数据</span>
        <span class="n">examples</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">get_dev_examples</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_dir</span><span class="p">)</span>
        <span class="c1"># 获取对应的标签</span>
        <span class="n">label_list</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">get_labels</span><span class="p">()</span>
        <span class="c1"># 再使用convert_examples_to_features生成模型需要的输入形式</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">convert_examples_to_features</span><span class="p">(</span><span class="n">examples</span><span class="p">,</span>
                                                <span class="n">tokenizer</span><span class="p">,</span>
                                                <span class="n">label_list</span><span class="o">=</span><span class="n">label_list</span><span class="p">,</span>
                                                <span class="n">max_length</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">max_seq_length</span><span class="p">,</span>
                                                <span class="n">output_mode</span><span class="o">=</span><span class="n">output_mode</span><span class="p">,</span>
                                                <span class="n">pad_token</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">([</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span><span class="p">])[</span><span class="mi">0</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Saving features into cached file </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">cached_features_file</span><span class="p">)</span>
        <span class="c1"># 将其保存至缓存文件路径中</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">cached_features_file</span><span class="p">)</span>


    <span class="c1"># 为了有效利用内存，之后将使用数据加载器，我们需要在这里将张量数据转换成数据迭代器对象TensorDatase</span>
    <span class="c1"># TensorDataset：用于自定义训练数据结构的迭代封装器，它可以封装任何与训练数据映射值相关的数据</span>
    <span class="c1">#（如：训练数据对应的标签，训练数据使用的掩码张量，token的类型id等），</span>
    <span class="c1"># 它们必须能转换成张量，将同训练数据映射值一起在训练过程中迭代使用。</span>

    <span class="c1"># 以下是分别把input_ids，attention_mask，token_type_ids，label封装在TensorDataset之中</span>
    <span class="n">all_input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">f</span><span class="o">.</span><span class="n">input_ids</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">features</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="n">all_attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">f</span><span class="o">.</span><span class="n">attention_mask</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">features</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="n">all_token_type_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">f</span><span class="o">.</span><span class="n">token_type_ids</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">features</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="n">all_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">f</span><span class="o">.</span><span class="n">label</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">features</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">all_input_ids</span><span class="p">,</span> <span class="n">all_attention_mask</span><span class="p">,</span> <span class="n">all_token_type_ids</span><span class="p">,</span> <span class="n">all_labels</span><span class="p">)</span>
    <span class="c1"># 返回数据迭代器对象</span>
    <span class="k">return</span> <span class="n">dataset</span>
</code></pre></div>


<ul>
<li>我们将在下面的模型量化中调用该评估函数。</li>
</ul>
<hr />
<h3 id="bert">使用动态量化技术对训练后的bert模型进行压缩</h3>
<ul>
<li>第一步: 将模型应用动态量化技术</li>
<li>第二步: 对比压缩后模型的大小</li>
<li>第三步: 对比压缩后的模型的推理准确性和耗时</li>
<li>第四步: 序列化模型以便之后使用</li>
</ul>
<hr />
<h4 id="_8">第一步: 将模型应用动态量化技术</h4>
<ul>
<li>应用动态量化技术:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="c1"># 使用torch.quantization.quantize_dynamic获得动态量化的模型</span>
<span class="c1"># 量化的网络层为所有的nn.Linear的权重，使其成为int8</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">quantize_dynamic</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="p">{</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">},</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span>
<span class="p">)</span>

<span class="c1"># 打印动态量化后的BERT模型</span>
<span class="nb">print</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">)</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>## 模型中的所有Linear层变成了DynamicQuantizedLinear层

BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (key): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (value): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, scale=1.0, zero_point=0)
          )
          (output): BertOutput(
            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, scale=1.0, zero_point=0)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (key): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (value): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, scale=1.0, zero_point=0)
          )
          (output): BertOutput(
            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, scale=1.0, zero_point=0)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
...
...

        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (key): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (value): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, scale=1.0, zero_point=0)
          )
          (output): BertOutput(
            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, scale=1.0, zero_point=0)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): DynamicQuantizedLinear(in_features=768, out_features=2, scale=1.0, zero_point=0)
)
</code></pre></div>


<hr />
<h4 id="_9">第二步: 对比压缩后模型的大小</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">print_size_of_model</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;打印模型大小&quot;&quot;&quot;</span>
    <span class="c1"># 保存模型中的参数部分到持久化文件</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">&quot;temp.p&quot;</span><span class="p">)</span>
    <span class="c1"># 打印持久化文件的大小</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Size (MB):&#39;</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">getsize</span><span class="p">(</span><span class="s2">&quot;temp.p&quot;</span><span class="p">)</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span>
    <span class="c1"># 移除该文件</span>
    <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s1">&#39;temp.p&#39;</span><span class="p">)</span>

<span class="c1"># 分别打印model和quantized_model</span>
<span class="n">print_size_of_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">print_size_of_model</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">)</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>## 模型参数文件大小缩减了257MB

Size (MB): 437.982584
Size (MB): 181.430351
</code></pre></div>


<h4 id="_10">第三步: 对比压缩后的模型的推理准确性和耗时</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">time_model_evaluation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">configs</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;获得模型评估结果和运行时间&quot;&quot;&quot;</span>
    <span class="c1"># 获得评估前时间</span>
    <span class="n">eval_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="c1"># 进行模型评估</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">configs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
    <span class="c1"># 获得评估后时间</span>
    <span class="n">eval_end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="c1"># 获得评估耗时</span>
    <span class="n">eval_duration_time</span> <span class="o">=</span> <span class="n">eval_end_time</span> <span class="o">-</span> <span class="n">eval_start_time</span>
    <span class="c1"># 打印模型评估结果</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Evaluate result:&quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
    <span class="c1"># 打印耗时</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Evaluate total time (seconds): </span><span class="si">{0:.1f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eval_duration_time</span><span class="p">))</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>Evaluating: 100%|██| 51/51 [01:36&lt;00:00,  1.89s/it]
Evaluate result: {&#39;acc&#39;: 0.8161764705882353, &#39;f1&#39;: 0.8739495798319329, &#39;acc_and_f1&#39;: 0.8450630252100841}
Evaluate total time (seconds): 96.4

Evaluating: 100%|███████████████████████████████| 51/51 [00:43&lt;00:00,  1.19it/s]
Evaluate result: {&#39;acc&#39;: 0.7965686274509803, &#39;f1&#39;: 0.8663446054750403, &#39;acc_and_f1&#39;: 0.8314566164630104}
Evaluate total time (seconds): 43.0
</code></pre></div>


<ul>
<li>结论:<ul>
<li>对模型进行动态量化后，参数文件大小明显减少。</li>
<li>动态量化后的模型在验证集上评估指标几乎不变，但是耗时却只用了原来的一半左右。</li>
</ul>
</li>
</ul>
<hr />
<h4 id="_11">第四步: 序列化模型以便之后使用</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># 量化模型的保存路径</span>
<span class="n">quantized_output_dir</span> <span class="o">=</span> <span class="n">configs</span><span class="o">.</span><span class="n">output_dir</span> <span class="o">+</span> <span class="s2">&quot;quantized/&quot;</span>
<span class="c1"># 判断是否需要创建该路径</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">quantized_output_dir</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">quantized_output_dir</span><span class="p">)</span>
    <span class="c1"># 使用save_pretrained保存模型</span>
    <span class="n">quantized_model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">quantized_output_dir</span><span class="p">)</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code># 在bert_finetuning_test/目录下

- quantized/
    - config.json
    - pytorch_model.bin
</code></pre></div>


<h3 id="_12">附录</h3>
<h4 id="run_gluepy">run_glue.py微调脚本代码</h4>
<p>请访问: <a href="http://git.itcast.cn/Stephen/AI-key-file/blob/master/run_glue.py">http://git.itcast.cn/Stephen/AI-key-file/blob/master/run_glue.py</a></p>
<hr />
<h2 id="72">7.2 模型剪枝技术</h2>
<h3 id="_13">学习目标</h3>
<ul>
<li>了解模型剪枝原理。</li>
<li>掌握使用prune对已有模型进行剪枝。</li>
</ul>
<hr />
<p><center><img alt="avatar" src="../img/prune.jpeg" /></center></p>
<hr />
<h3 id="_14">剪枝技术介绍与原理</h3>
<ul>
<li>当前在NLP任务上取得优异成绩的模型往往是拥有大量参数的模型，如BERT，GPT等，但实际上，生物的高度文明是使用神经网络有效的稀疏连接，这正是“剪枝技术”的发展起源。而所谓剪枝技术原理，也就是将现有模型中的某些参数设置为0，等效于这些神经元失活。而参数置0的方式是使用MASK蒙版（蒙版0位置得到0，1位置得到原来的值）。下面我们将详细介绍这种技术的实现方式。</li>
</ul>
<hr />
<h3 id="prune">使用prune对已有模型进行剪枝</h3>
<ul>
<li>我们将使用torch.nn.utils.prune中的方法进行网络稀疏化，即剪枝。</li>
<li>要求torch版本应该&gt;= 1.4.0。</li>
</ul>
<div class="codehilite"><pre><span></span><code>pip install torch&gt;<span class="o">=</span><span class="m">1</span>.4.0
</code></pre></div>


<ul>
<li>剪枝的讲解将分为以下步骤：<ul>
<li>第一步：导入工具包并获得模型</li>
<li>第二步：使用剪枝工具并了解其作用方式</li>
<li>第三步：持久化修剪后的模型</li>
<li>第四步：工程中常用的修剪方法</li>
</ul>
</li>
</ul>
<hr />
<h4 id="_15">第一步：导入工具包并获得模型</h4>
<ul>
<li>这里我们将使用LeNet作为剪枝的对象，当然你可以选择任何你已经训练好的某个网络和参数。</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="c1"># 导入必备的工具包</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.utils.prune</span> <span class="k">as</span> <span class="nn">prune</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>


<span class="c1"># 这里不再对LeNet网络做过多的介绍，因为我们的重点是剪枝技术</span>
<span class="c1"># 无论你使用哪一种网络进行剪枝，你必须熟知这个网络中的组成部分即哪些能够剪枝</span>
<span class="c1"># 比如在这里，conv1，conv2，fc1，fc2，fc3都是可以被剪枝的</span>
<span class="k">class</span> <span class="nc">LeNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LeNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>  <span class="c1"># 5x5 image dimension</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># 获得这个模型对象</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LeNet</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</code></pre></div>


<hr />
<h4 id="_16">第二步：使用剪枝工具并了解其作用方式</h4>
<ul>
<li>下面我们会使用剪枝工具并逐步查看参数的变化，先来看看没有剪枝之前的状态。</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="c1"># 获得conv1模块</span>
<span class="n">module</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">conv1</span>
<span class="c1"># 查看该模块的所有原生参数，一般由原生weight和原生bias组成</span>
<span class="c1"># 什么是原生参数呢？</span>
<span class="c1"># 这里是因为torch在设计存储网络参数时，允许为参数添加修改方式（如蒙版），</span>
<span class="c1"># 这些修改方式以buffer的形式存储，不会直接作用在named_parameters中的参数上，</span>
<span class="c1"># 因此把named_parameters中的参数叫做原生参数。</span>
<span class="c1"># 那如何获得作用了buffer之后的真实参数呢？</span>
<span class="c1"># 使用module.weight和module.bias即可</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()))</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>[(&#39;weight&#39;, Parameter containing:
tensor([[[[ 0.3161, -0.2212,  0.0417],
          [ 0.2488,  0.2415,  0.2071],
          [-0.2412, -0.2400, -0.2016]]],


        [[[ 0.0419,  0.3322, -0.2106],
          [ 0.1776, -0.1845, -0.3134],
          [-0.0708,  0.1921,  0.3095]]],


        [[[-0.2070,  0.0723,  0.2876],
          [ 0.2209,  0.2077,  0.2369],
          [ 0.2108,  0.0861, -0.2279]]],


        [[[-0.2799, -0.1527, -0.0388],
          [-0.2043,  0.1220,  0.1032],
          [-0.0755,  0.1281,  0.1077]]],


        [[[ 0.2035,  0.2245, -0.1129],
          [ 0.3257, -0.0385, -0.0115],
          [-0.3146, -0.2145, -0.1947]]],


        [[[-0.1426,  0.2370, -0.1089],
          [-0.2491,  0.1282,  0.1067],
          [ 0.2159, -0.1725,  0.0723]]]], device=&#39;cuda:0&#39;, requires_grad=True)), 

(&#39;bias&#39;, Parameter containing:
tensor([-0.1214, -0.0749, -0.2656, -0.1519, -0.1021,  0.1425], device=&#39;cuda:0&#39;,
       requires_grad=True))]
</code></pre></div>


<hr />
<div class="codehilite"><pre><span></span><code><span class="c1"># 查看参数修改方式即buffer</span>
<span class="c1"># 没有buffer的原生参数就是真实参数</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">()))</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>[]
</code></pre></div>


<hr />
<ul>
<li>使用prune.random_unstructured随机方式进行剪枝</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="c1"># 剪枝指定moudle即conv1中的weight参数，修剪（设置为0）30%</span>
<span class="n">prune</span><span class="o">.</span><span class="n">random_unstructured</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">amount</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</code></pre></div>


<div class="codehilite"><pre><span></span><code><span class="c1"># 查看修剪后的named_parameters参数</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()))</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>[(&#39;bias&#39;, Parameter containing:
tensor([-0.1214, -0.0749, -0.2656, -0.1519, -0.1021,  0.1425], device=&#39;cuda:0&#39;,
       requires_grad=True)), 

(&#39;weight_orig&#39;, Parameter containing:
tensor([[[[ 0.3161, -0.2212,  0.0417],
          [ 0.2488,  0.2415,  0.2071],
          [-0.2412, -0.2400, -0.2016]]],


        [[[ 0.0419,  0.3322, -0.2106],
          [ 0.1776, -0.1845, -0.3134],
          [-0.0708,  0.1921,  0.3095]]],


        [[[-0.2070,  0.0723,  0.2876],
          [ 0.2209,  0.2077,  0.2369],
          [ 0.2108,  0.0861, -0.2279]]],


        [[[-0.2799, -0.1527, -0.0388],
          [-0.2043,  0.1220,  0.1032],
          [-0.0755,  0.1281,  0.1077]]],


        [[[ 0.2035,  0.2245, -0.1129],
          [ 0.3257, -0.0385, -0.0115],
          [-0.3146, -0.2145, -0.1947]]],


        [[[-0.1426,  0.2370, -0.1089],
          [-0.2491,  0.1282,  0.1067],
          [ 0.2159, -0.1725,  0.0723]]]], device=&#39;cuda:0&#39;, requires_grad=True))]
</code></pre></div>


<ul>
<li>在结果中，我们发现原生参数并没有变化，而是名字由weight变成了weight_orig，进一步强调了它是原生参数。</li>
<li>为什么要进一步强调原生呢，是因为此时buffer中已经多了一些信息，原生参数和真实参数已经不再等价。</li>
</ul>
<hr />
<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">()))</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>[(&#39;weight_mask&#39;, tensor([[[[0., 1., 0.],
          [1., 0., 0.],
          [1., 1., 1.]]],


        [[[1., 0., 1.],
          [1., 1., 0.],
          [1., 0., 1.]]],


        [[[1., 0., 0.],
          [0., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 0., 0.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 0., 1.],
          [1., 1., 1.],
          [0., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 0.],
          [1., 1., 0.]]]], device=&#39;cuda:0&#39;))]
</code></pre></div>


<ul>
<li>buffer中已经存在了用于剪枝的蒙版</li>
</ul>
<hr />
<ul>
<li>使用module.weight查看真实使用参数：</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([[[[ 0.0000, -0.2212,  0.0000],
          [ 0.2488,  0.0000,  0.0000],
          [-0.2412, -0.2400, -0.2016]]],


        [[[ 0.0419,  0.0000, -0.2106],
          [ 0.1776, -0.1845, -0.0000],
          [-0.0708,  0.0000,  0.3095]]],


        [[[-0.2070,  0.0000,  0.0000],
          [ 0.0000,  0.2077,  0.2369],
          [ 0.2108,  0.0861, -0.2279]]],


        [[[-0.2799, -0.0000, -0.0000],
          [-0.2043,  0.1220,  0.1032],
          [-0.0755,  0.1281,  0.1077]]],


        [[[ 0.2035,  0.0000, -0.1129],
          [ 0.3257, -0.0385, -0.0115],
          [-0.0000, -0.2145, -0.1947]]],


        [[[-0.1426,  0.2370, -0.1089],
          [-0.2491,  0.1282,  0.0000],
          [ 0.2159, -0.1725,  0.0000]]]], device=&#39;cuda:0&#39;,
       grad_fn=&lt;MulBackward0&gt;)
</code></pre></div>


<hr />
<h4 id="_17">第三步：持久化修剪后的模型</h4>
<ul>
<li>假如你已经对剪枝后的模型进行了必要的验证，并觉得它可以被保存并在将来部署使用，那么你需要持久化模型。</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="c1"># 一般我们会首先将buffer中的蒙版永久作用在name_parameters中的参数上</span>
<span class="c1"># 这里的remove不是移除，而是永久化</span>
<span class="n">prune</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()))</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>[(&#39;bias_orig&#39;, Parameter containing:
tensor([-0.1214, -0.0749, -0.2656, -0.1519, -0.1021,  0.1425], device=&#39;cuda:0&#39;,
       requires_grad=True)), (&#39;weight&#39;, Parameter containing:
tensor([[[[ 0.0000, -0.2212,  0.0000],
          [ 0.2488,  0.0000,  0.0000],
          [-0.2412, -0.2400, -0.2016]]],


        [[[ 0.0000,  0.0000, -0.0000],
          [ 0.0000, -0.0000, -0.0000],
          [-0.0000,  0.0000,  0.0000]]],


        [[[-0.2070,  0.0000,  0.0000],
          [ 0.0000,  0.2077,  0.2369],
          [ 0.2108,  0.0861, -0.2279]]],


        [[[-0.0000, -0.0000, -0.0000],
          [-0.0000,  0.0000,  0.0000],
          [-0.0000,  0.0000,  0.0000]]],


        [[[ 0.2035,  0.0000, -0.1129],
          [ 0.3257, -0.0385, -0.0115],
          [-0.0000, -0.2145, -0.1947]]],


        [[[-0.0000,  0.0000, -0.0000],
          [-0.0000,  0.0000,  0.0000],
          [ 0.0000, -0.0000,  0.0000]]]], device=&#39;cuda:0&#39;, requires_grad=True))]
</code></pre></div>


<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">()))</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>[]
</code></pre></div>


<ul>
<li>此时buffer中已经没有任何修改，此时原生参数和真实参数等价。</li>
</ul>
<hr />
<ul>
<li>保存序列化模型：</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">PATH</span> <span class="o">=</span> <span class="s2">&quot;./model.pth&quot;</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>odict_keys([&#39;conv1.bias&#39;, &#39;conv1.weight&#39;, &#39;conv2.weight&#39;, &#39;conv2.bias&#39;, &#39;fc1.weight&#39;, &#39;fc1.bias&#39;, &#39;fc2.weight&#39;, &#39;fc2.bias&#39;, &#39;fc3.weight&#39;, &#39;fc3.bias&#39;])
</code></pre></div>


<ul>
<li>实际上假如你不进行prune.remove操作，直接保存state_dict()也是可以的，因为buffer也可以被保存在state_dict中，因为剪枝后往往需要继续重新训练，一般直到最后判断剪枝模型可用才使用remove永久化剪枝参数。</li>
</ul>
<hr />
<h4 id="_18">第四步：工程中常用的修剪方法</h4>
<ul>
<li>刚刚我们学习的都是“局部”的剪枝方式，而实际工程中我们往往直接针对模型进行整体剪枝。下面就是整体剪枝的方法：</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="c1"># 获得模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LeNet</span><span class="p">()</span>

<span class="c1"># 用元组指定需要剪枝的层和参数类型</span>
<span class="n">parameters_to_prune</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv1</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv2</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc1</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc2</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc3</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># 进行全局剪枝，参数分别是需要剪枝的层和参数类型，剪枝方法，剪枝比例</span>
<span class="c1"># 通过这样的操作我们就可以得到剪枝后的模型，这里的0.2是整体的20%，各个部分剪枝在20%左右</span>
<span class="c1"># 这里使用了L1剪枝</span>
<span class="n">prune</span><span class="o">.</span><span class="n">global_unstructured</span><span class="p">(</span>
    <span class="n">parameters_to_prune</span><span class="p">,</span>
    <span class="n">pruning_method</span><span class="o">=</span><span class="n">prune</span><span class="o">.</span><span class="n">L1Unstructured</span><span class="p">,</span>
    <span class="n">amount</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
<span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;#################&quot;</span><span class="p">)</span>

<span class="c1"># 永久化参数</span>
<span class="k">for</span> <span class="n">module</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">parameters_to_prune</span><span class="p">:</span>
    <span class="n">prune</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>dict_keys([&#39;conv1.bias&#39;, &#39;conv1.weight&#39;, &#39;conv2.weight&#39;, &#39;conv2.bias&#39;, &#39;fc1.weight&#39;, &#39;fc1.bias&#39;, &#39;fc2.weight&#39;, &#39;fc2.bias&#39;, &#39;fc3.weight&#39;, &#39;fc3.bias&#39;])
odict_keys([&#39;conv1.bias&#39;, &#39;conv1.weight_orig&#39;, &#39;conv1.weight_mask&#39;, &#39;conv2.bias&#39;, &#39;conv2.weight_orig&#39;, &#39;conv2.weight_mask&#39;, &#39;fc1.bias&#39;, &#39;fc1.weight_orig&#39;, &#39;fc1.weight_mask&#39;, &#39;fc2.bias&#39;, &#39;fc2.weight_orig&#39;, &#39;fc2.weight_mask&#39;, &#39;fc3.bias&#39;, &#39;fc3.weight_orig&#39;, &#39;fc3.weight_mask&#39;])

#################

odict_keys([&#39;conv1.bias&#39;, &#39;conv1.weight&#39;, &#39;conv2.weight&#39;, &#39;conv2.bias&#39;, &#39;fc1.weight&#39;, &#39;fc1.bias&#39;, &#39;fc2.weight&#39;, &#39;fc2.bias&#39;, &#39;fc3.weight&#39;, &#39;fc3.bias&#39;])
odict_keys([&#39;conv1.bias&#39;, &#39;conv1.weight&#39;, &#39;conv2.bias&#39;, &#39;conv2.weight&#39;, &#39;fc1.bias&#39;, &#39;fc1.weight&#39;, &#39;fc2.bias&#39;, &#39;fc2.weight&#39;, &#39;fc3.bias&#39;, &#39;fc3.weight&#39;])
</code></pre></div>


<hr />
<ul>
<li>检查一下全局剪枝的20%在各个层中剪枝占比:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Sparsity in conv1.weight: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="mf">100.</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>
        <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">())</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Sparsity in conv2.weight: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="mf">100.</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv2</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>
        <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv2</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">())</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Sparsity in fc1.weight: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="mf">100.</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>
        <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">())</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Sparsity in fc2.weight: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="mf">100.</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc2</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>
        <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc2</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">())</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Sparsity in fc3.weight: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="mf">100.</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc3</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>
        <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc3</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">())</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Global sparsity: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="mf">100.</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
            <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv2</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
            <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
            <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc2</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
            <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc3</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="o">/</span> <span class="nb">float</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
            <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">conv2</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
            <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
            <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">fc2</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
            <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">fc3</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
        <span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>Sparsity in conv1.weight: 7.41%
Sparsity in conv2.weight: 9.49%
Sparsity in fc1.weight: 22.00%
Sparsity in fc2.weight: 12.28%
Sparsity in fc3.weight: 9.76%
Global sparsity: 20.00%
</code></pre></div>


<hr />
<ul>
<li>常用的剪枝方式解释：<ul>
<li>RandomUnstructured：随机剪枝</li>
<li>L1Unstructured：按照L1范数（绝对值大小）剪枝，因为数值越小对结果的扰动也越小。</li>
</ul>
</li>
</ul>
<hr />
<h3 id="_19">小节总结</h3>
<ul>
<li>学习了模型剪枝原理</li>
<li>学习了使用prune对已有模型进行剪枝<ul>
<li>第一步：导入工具包并获得模型</li>
<li>第二步：使用剪枝工具并了解其作用方式</li>
<li>第三步：持久化修剪后的模型</li>
<li>第四步：工程中常用的修剪方法</li>
</ul>
</li>
</ul>
<hr />
<h2 id="73-onnx-runtime">7.3 使用ONNX-Runtime进行模型推断加速</h2>
<h3 id="_20">学习目标</h3>
<ul>
<li>了解ONNX及其ONNX-Runtime的主要作用。</li>
<li>掌握如何使用ONNX-Runtime进行模型推断加速。</li>
</ul>
<hr />
<p><center><img alt="avatar" src="../img/trt-info.png" /></center></p>
<hr />
<h3 id="onnxonnx-runtime">什么是ONNX和ONNX-Runtime</h3>
<ul>
<li>ONNX（Open Neural Network Exchange）开放式神经网络交换格式，与torch的pth，keras的h5，tensorflow的pb一样，它属于一种模型格式。</li>
<li>这种交换格式被设计的初衷：希望各种模型框架训练得到的模型能够通用。而现在它已经结合ONNX-Runtime成为一种加速模型推理的方法。</li>
<li>ONNX-Runtime就是指ONNX格式模型的运行环境，它由微软开源，该环境集成多种模型加速工具，如Nvidia的TensorRT等，用于快速模型推断。</li>
<li>能够原生支持ONNX模型转换的模型包括Pytorch，MXNET，Caffe2等框架（Tensorflow不可以）。</li>
</ul>
<hr />
<p><center><img alt="avatar" src="../img/trt-info2.png" /></center></p>
<ul>
<li>图中，蓝色的部分就是ONNX-Runtime，它能够自动利用已有设备上的各种加速工具，完成模型加速，无需人工参与，只需要我们将ONNX格式的模型作为输入即可。</li>
</ul>
<hr />
<h3 id="onnx-runtime">使用ONNX-Runtime进行模型推断加速的步骤</h3>
<ul>
<li>第一步：安装必备的工具包</li>
<li>第二步：将已有模型转换成ONNX格式</li>
<li>第三步：使用ONNX-Runtime进行模型预测</li>
<li>第四步：对比结果差异和推断时间</li>
</ul>
<hr />
<h4 id="_21">第一步：安装必备的工具包</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># onnx 1.7.0</span>
<span class="c1"># onnxruntime 1.4.0</span>
pip install onnx onnxruntime
</code></pre></div>


<hr />
<h4 id="onnx">第二步：将已有模型转换成ONNX格式</h4>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.onnx</span>
<span class="kn">import</span> <span class="nn">torchvision</span>

<span class="c1"># 这里以resnet18模型为例</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># 随机初始化一个指定shape的输入</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># 评估模式</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</code></pre></div>


<hr />
<div class="codehilite"><pre><span></span><code><span class="c1"># 定义onnx输入输出的名字（格式需要）</span>
<span class="n">input_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;input1&quot;</span><span class="p">]</span>
<span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;output1&quot;</span><span class="p">]</span>

<span class="n">onnx_model_name</span> <span class="o">=</span> <span class="s2">&quot;resnet18.onnx&quot;</span>

<span class="c1"># 使用torch.onnx导出resnet18.onnx</span>
<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">onnx_model_name</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">)</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果:<ul>
<li>在该脚本路径下得到resnet18.onnx格式的模型。</li>
</ul>
</li>
</ul>
</blockquote>
<hr />
<h4 id="onnx-runtime_1">第三步：使用ONNX-Runtime进行模型预测</h4>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">onnxruntime</span>

<span class="c1"># 当前onnxruntime的输入要求的为numpy形式</span>
<span class="k">def</span> <span class="nf">to_numpy</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;将tensor转化成numpy&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">else</span> <span class="n">tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="c1"># 使用模型创建onnxruntime的session</span>
<span class="n">ort_session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="n">onnx_model_name</span><span class="p">)</span>
<span class="c1"># 在session中运行，要求输入为dict形式，key为之前定义好的input名字，且input必须为numpy形式</span>
<span class="n">ort_outs</span> <span class="o">=</span> <span class="n">ort_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="n">ort_session</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">to_numpy</span><span class="p">(</span><span class="nb">input</span><span class="p">)})</span>
</code></pre></div>


<hr />
<h4 id="_22">第四步：对比结果差异和推断时间</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># pth模型的结果</span>
<span class="n">torch_out</span> <span class="o">=</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>

<span class="c1"># onnx模型的结果</span>
<span class="n">ort_out</span> <span class="o">=</span> <span class="n">ort_outs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">torch_out</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ort_out</span><span class="p">)</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>## 结果是一模一样的，没有任何差异

[[ 6.46063685e-01  2.58094740e+00  2.67934680e+00  2.84586716e+00
   4.45001364e+00  3.60939002e+00  3.51634717e+00  2.71348268e-01
  -1.15397012e+00 -7.00954318e-01 -7.89242506e-01  9.08504605e-01
   1.55155942e-01  1.08485329e+00  1.31591737e+00  5.42257011e-01]]

[[ 6.46063685e-01  2.58094740e+00  2.67934680e+00  2.84586716e+00
   4.45001364e+00  3.60939002e+00  3.51634717e+00  2.71348268e-01
  -1.15397012e+00 -7.00954318e-01 -7.89242506e-01  9.08504605e-01
   1.55155942e-01  1.08485329e+00  1.31591737e+00  5.42257011e-01]]
</code></pre></div>


<hr />
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">time</span>


<span class="c1"># 对比两者在CPU上的预测时间差异</span>
<span class="c1"># 二者在GPU上的表现相当，因为onnxruntime本身也是调用cuda</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">torch_out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">ort_outs</span> <span class="o">=</span> <span class="n">ort_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;input1&quot;</span><span class="p">:</span> <span class="n">to_numpy</span><span class="p">(</span><span class="nb">input</span><span class="p">)})</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code># pth模型推断时间为24.6ms
0.024605449676513672

# onnx仅需8.5ms，大约节省2-3倍时间
0.008518030548095703
</code></pre></div>


<hr />
<h3 id="onnx-runtime_2">ONNX-Runtime能够加速的原理</h3>
<ul>
<li>下面以其调用TensorRT(nvidia加速工具)加速BERT为例进行说明：</li>
</ul>
<p><center><img alt="" src="../img/op1.jpg" /></center></p>
<blockquote>
<ul>
<li>
<p>在BERT编码器中，将LayerNormalization层和残差连接进行融合以加速计算。</p>
</li>
<li>
<p>对gelu激活函数使用<code>简化gelu</code>计算方法加速计算。</p>
</li>
</ul>
</blockquote>
<hr />
<p><center><img alt="" src="../img/op2.jpg" /></center>
<center><img alt="" src="../img/op3.jpg" /></center></p>
<blockquote>
<ul>
<li>
<p>对所有的self-attention layer中的全连接层进行融合，以减少内存和正反向传播次数加速计算。</p>
</li>
<li>
<p>ONNX-Runtime内置针对主流模型（BERT，RESNET等）的并行计算模式，实现加速计算。</p>
</li>
</ul>
</blockquote>
<p><center><img alt="" src="../img/op4.jpg" /></center></p>
<hr />
<h3 id="_23">小节总结</h3>
<ul>
<li>学习了使用ONNX-Runtime进行模型推断加速的步骤：<ul>
<li>第一步：安装必备的工具包</li>
<li>第二步：将已有模型转换成ONNX格式</li>
<li>第三步：使用ONNX-Runtime进行模型预测</li>
<li>第四步：对比结果差异和推断时间</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>ONNX-Runtime能够加速的原因：<ul>
<li>1，作为加速工具的集成环境，能够自动调用加速工具cuda，TensorRT，nGragh等。</li>
<li>2，对特定模型的层和张量进行融合，以减少正反向传播次数。</li>
<li>3，对特定激活函数gelu进行简化计算。</li>
<li>4，针对主流模型（BERT，RESNET等）的并行计算模式，实现加速计算。</li>
</ul>
</li>
</ul>
<hr />
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            ©Copyright 2020, AITutorials.CN This website has been reviewed by the review agency. 京ICP备19006137号
          </div>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/vendor.c3dc8c49.min.js"></script>
      <script src="../assets/javascripts/bundle.f9edbbd5.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "\u590d\u5236", "clipboard.copied": "\u5df2\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c"}</script>
      
      <script>
        app = initialize({
          base: "..",
          features: [],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.8e2cddea.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
    
  </body>
</html>