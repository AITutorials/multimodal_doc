



<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="https://www.tisv.cn/7/">
      
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="ja">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="../img/AI.jpg">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-4.4.0">
    
    
      
        <title>7 - 基于多模态的视频标签化系统</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.0284f74d.css">
      
      
    
    
      <script src="../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
    
      
        
<script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "UA-36723568-3", "mkdocs.org")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
    <body dir="ltr">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#71" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://www.tisv.cn/" title="基于多模态的视频标签化系统" class="md-header-nav__button md-logo">
          
            <img src="../img/AI.jpg" width="24" height="24">
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              基于多模态的视频标签化系统
            </span>
            <span class="md-header-nav__topic">
              
                7
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/AITutorials/datasets" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    Github | Give Me A Star
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://www.tisv.cn/" title="基于多模态的视频标签化系统" class="md-nav__button md-logo">
      
        <img src="../img/AI.jpg" width="48" height="48">
      
    </a>
    基于多模态的视频标签化系统
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/AITutorials/datasets" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    Github | Give Me A Star
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../1/" title="第一章：工程师眼中的多模态发展" class="md-nav__link">
      第一章：工程师眼中的多模态发展
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../2/" title="第二章：多模态整体解决方案" class="md-nav__link">
      第二章：多模态整体解决方案
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../3/" title="图像数据增强工具Albumentations" class="md-nav__link">
      图像数据增强工具Albumentations
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../4/" title="使用flask部署模型服务" class="md-nav__link">
      使用flask部署模型服务
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#71" title="7.1 模型量化技术" class="md-nav__link">
    7.1 模型量化技术
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" title="学习目标" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" title="相关知识" class="md-nav__link">
    相关知识
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" title="数据集说明" class="md-nav__link">
    数据集说明
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#huggingfacebert" title="使用huggingface中的预训练BERT模型进行微调" class="md-nav__link">
    使用huggingface中的预训练BERT模型进行微调
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_4" title="第一步: 安装核心的工具包并导入" class="md-nav__link">
    第一步: 安装核心的工具包并导入
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" title="第二步: 下载数据集并使用脚本进行微调" class="md-nav__link">
    第二步: 下载数据集并使用脚本进行微调
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" title="第三步: 设定全局配置并加载微调模型" class="md-nav__link">
    第三步: 设定全局配置并加载微调模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" title="第四步: 编写用于模型使用的评估函数" class="md-nav__link">
    第四步: 编写用于模型使用的评估函数
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bert" title="使用动态量化技术对训练后的bert模型进行压缩" class="md-nav__link">
    使用动态量化技术对训练后的bert模型进行压缩
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_8" title="第一步: 将模型应用动态量化技术" class="md-nav__link">
    第一步: 将模型应用动态量化技术
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" title="第二步: 对比压缩后模型的大小" class="md-nav__link">
    第二步: 对比压缩后模型的大小
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" title="第三步: 对比压缩后的模型的推理准确性和耗时" class="md-nav__link">
    第三步: 对比压缩后的模型的推理准确性和耗时
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" title="第四步: 序列化模型以便之后使用" class="md-nav__link">
    第四步: 序列化模型以便之后使用
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" title="附录" class="md-nav__link">
    附录
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#run_gluepy" title="run_glue.py微调脚本代码" class="md-nav__link">
    run_glue.py微调脚本代码
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#72" title="7.2 模型剪枝技术" class="md-nav__link">
    7.2 模型剪枝技术
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_13" title="学习目标" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" title="剪枝技术介绍与原理" class="md-nav__link">
    剪枝技术介绍与原理
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prune" title="使用prune对已有模型进行剪枝" class="md-nav__link">
    使用prune对已有模型进行剪枝
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_15" title="第一步：导入工具包并获得模型" class="md-nav__link">
    第一步：导入工具包并获得模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_16" title="第二步：使用剪枝工具并了解其作用方式" class="md-nav__link">
    第二步：使用剪枝工具并了解其作用方式
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_17" title="第三步：持久化修剪后的模型" class="md-nav__link">
    第三步：持久化修剪后的模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_18" title="第四步：工程中常用的修剪方法" class="md-nav__link">
    第四步：工程中常用的修剪方法
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_19" title="小节总结" class="md-nav__link">
    小节总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#73-onnx-runtime" title="7.3 使用ONNX-Runtime进行模型推断加速" class="md-nav__link">
    7.3 使用ONNX-Runtime进行模型推断加速
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_20" title="学习目标" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#onnxonnx-runtime" title="什么是ONNX和ONNX-Runtime" class="md-nav__link">
    什么是ONNX和ONNX-Runtime
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#onnx-runtime" title="使用ONNX-Runtime进行模型推断加速的步骤" class="md-nav__link">
    使用ONNX-Runtime进行模型推断加速的步骤
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_21" title="第一步：安装必备的工具包" class="md-nav__link">
    第一步：安装必备的工具包
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#onnx" title="第二步：将已有模型转换成ONNX格式" class="md-nav__link">
    第二步：将已有模型转换成ONNX格式
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#onnx-runtime_1" title="第三步：使用ONNX-Runtime进行模型预测" class="md-nav__link">
    第三步：使用ONNX-Runtime进行模型预测
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_22" title="第四步：对比结果差异和推断时间" class="md-nav__link">
    第四步：对比结果差异和推断时间
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#onnx-runtime_2" title="ONNX-Runtime能够加速的原理" class="md-nav__link">
    ONNX-Runtime能够加速的原理
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_23" title="小节总结" class="md-nav__link">
    小节总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>7</h1>
                
                <h2 id="71">7.1 模型量化技术</h2>
<h3 id="_1">学习目标</h3>
<ul>
<li>了解模型压缩技术中的动态量化与静态量化的相关知识。</li>
<li>掌握使用huggingface中的预训练BERT模型进行微调。</li>
<li>掌握使用动态量化技术对训练后的bert模型进行压缩。</li>
</ul>
<hr />
<p><center><img alt="avatar" src="../img/bert1.png" /></center></p>
<hr />
<h3 id="_2">相关知识</h3>
<ul>
<li>模型压缩:<ul>
<li>模型压缩是一种针对大型模型(参数量巨大)在使用过程中进行优化的一种常用措施。它往往能够使模型体积缩小，简化计算，增快推断速度，满足模型在特定场合(如: 移动端)的需求。目前，模型压缩可以从多方面考虑，如剪枝方法(简化模型架构)，参数量化方法(简化模型参数)，知识蒸馏等。本案例将着重讲解模型参数量化方法。</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>模型参数量化:<ul>
<li>在机器学习（深度学习）领域，模型量化一般是指将模型参数由类型FP32转换为INT8的过程，转换之后的模型大小被压缩为原来的1/4，所需内存和带宽减小4倍，同时，计算量减小约为2-4倍。模型又可分为动态量化和静态量化。</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>模型动态量化：<ul>
<li>操作最简单也是压缩效果最好的量化方式，量化过程发生在模型训练后，针对模型权重采取量化，之后会在模型预测过程中，再决定是否针对激活值采取量化，因此称作动态量化（在预测时可能发生量化）。这是我们本案例将会使用的量化方式。</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>模型静态量化：<ul>
<li>考虑到动态量化这种“一刀切”的量化方式有时会带来模型预测效果的大幅度下降，因此引入静态量化，它同样发生在模型训练后，为了判断哪些权重或激活值应该被量化，哪些应该保留或小幅度量化，在预测过程开始前，在模型中节点插入“观测者”（衡量节点使用情况的一些计算方法），他们将在一些实验数据中评估节点使用情况，来决定是否将其权重或激活值进行量化，因为在预测过程中，这些节点是否被量化已经确定，因此称作静态量化。</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>（扩展知识）量化意识训练：<ul>
<li>这是一种操作相对复杂的模型量化方法，在训练过程中使用它，原理与静态量化类似，都需要像模型中插入“观测者”，同时它还需要插入量化计算操作，使得模型训练过程中除了进行原有的浮点型计算，还要进行量化计算，但模型参数的更新还是使用浮点型，而量化计算的作用就是让模型“意识”到这个过程，通过“观测者”评估每次量化结果与训练过程中参数更新程度，为之后模型如何进行量化还能保证准确率提供衡量指标。（类似于，人在接受训练时，意识到自己接下来可能除了训练内容外，还会接受其他“操作”（量化），因此也会准备一些如果进行量化仍能达成目标的措施）</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>BERT模型:<ul>
<li>这里使用bert-base-uncased，它的编码器具有12个隐层, 输出768维张量, 12个自注意力头, 共110M参数量, 在小写的英文文本上进行训练而得到。</li>
</ul>
</li>
</ul>
<hr />
<h3 id="_3">数据集说明</h3>
<ul>
<li>
<p>GLUE数据集合的介绍：</p>
<ul>
<li>GLUE由纽约大学，华盛顿大学，Google联合推出，涵盖不同的NLP任务类型，持续至2020年1月，其中包括11个子任务数据集，成为NLP研究发展的标准。我们这里使用其实MRPC数据集。</li>
</ul>
</li>
<li>
<p>数据下载地址: 标准数据集一般使用下载脚本进行下载，会在之后的代码中演示。</p>
</li>
<li>
<p>MRPC数据集的任务类型：</p>
<ul>
<li>句子对二分类任务<ul>
<li>训练集上正样本占68%，负样本占32%</li>
</ul>
</li>
<li>评估指标这里使用：F1</li>
<li>评估指标计算方式：F1=2∗(precision∗recall)/(precision+recall)</li>
</ul>
</li>
<li>
<p>数据集预览:</p>
</li>
</ul>
<blockquote>
<ul>
<li>MRPC数据集文件样式：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="o">-</span> <span class="n">MRPC</span><span class="o">/</span>
        <span class="o">-</span> <span class="n">dev</span><span class="p">.</span><span class="n">tsv</span>
        <span class="o">-</span> <span class="n">test</span><span class="p">.</span><span class="n">tsv</span>
        <span class="o">-</span> <span class="n">train</span><span class="p">.</span><span class="n">tsv</span>
    <span class="o">-</span> <span class="n">dev_ids</span><span class="p">.</span><span class="n">tsv</span>
    <span class="o">-</span> <span class="n">msr_paraphrase_test</span><span class="p">.</span><span class="n">txt</span>
    <span class="o">-</span> <span class="n">msr_paraphrase_train</span><span class="p">.</span><span class="n">txt</span>
</code></pre></div>


<blockquote>
<ul>
<li>文件样式说明：<ul>
<li>在使用中常用到的文件是train.tsv，dev.tsv，test.tsv，分别代表训练集，验证集和测试集。其中train.tsv与dev.tsv数据样式相同，都是带有标签的数据，其中test.tsv是不带有标签的数据。</li>
</ul>
</li>
</ul>
</blockquote>
<ul>
<li>train.tsv数据样式：</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="nv">Quality</span> <span class="sc">#1</span> <span class="nv">ID</span>   <span class="sc">#2</span> <span class="nv">ID</span>   <span class="sc">#1</span> <span class="nv">String</span>   <span class="sc">#2</span> <span class="nv">String</span>
<span class="mi">1</span>   <span class="mi">702876</span>  <span class="mi">702977</span>  <span class="nv">Amrozi</span> <span class="nv">accused</span> <span class="nv">his</span> <span class="nv">brother</span> , <span class="nv">whom</span> <span class="nv">he</span> <span class="nv">called</span> <span class="s2">&quot;</span><span class="s"> the witness </span><span class="s2">&quot;</span> , <span class="nv">of</span> <span class="nv">deliberately</span> <span class="nv">distorting</span> <span class="nv">his</span> <span class="nv">evidence</span> . <span class="nv">Referring</span> <span class="nv">to</span> <span class="nv">him</span> <span class="nv">as</span> <span class="nv">only</span> <span class="s2">&quot;</span><span class="s"> the witness </span><span class="s2">&quot;</span> , <span class="nv">Amrozi</span> <span class="nv">accused</span> <span class="nv">his</span> <span class="nv">brother</span> <span class="nv">of</span> <span class="nv">deliberately</span> <span class="nv">distorting</span> <span class="nv">his</span> <span class="nv">evidence</span> .
<span class="mi">0</span>   <span class="mi">2108705</span> <span class="mi">2108831</span> <span class="nv">Yucaipa</span> <span class="nv">owned</span> <span class="nv">Dominick</span> <span class="s1">&#39;</span><span class="s">s before selling the chain to Safeway in 1998 for $ 2.5 billion .   Yucaipa bought Dominick </span><span class="s1">&#39;</span><span class="nv">s</span> <span class="nv">in</span> <span class="mi">1995</span> <span class="k">for</span> $ <span class="mi">693</span> <span class="nv">million</span> <span class="nv">and</span> <span class="nv">sold</span> <span class="nv">it</span> <span class="nv">to</span> <span class="nv">Safeway</span> <span class="k">for</span> $ <span class="mi">1</span>.<span class="mi">8</span> <span class="nv">billion</span> <span class="nv">in</span> <span class="mi">1998</span> .
<span class="mi">1</span>   <span class="mi">1330381</span> <span class="mi">1330521</span> <span class="nv">They</span> <span class="nv">had</span> <span class="nv">published</span> <span class="nv">an</span> <span class="nv">advertisement</span> <span class="nv">on</span> <span class="nv">the</span> <span class="nv">Internet</span> <span class="nv">on</span> <span class="nv">June</span> <span class="mi">10</span> , <span class="nv">offering</span> <span class="nv">the</span> <span class="nv">cargo</span> <span class="k">for</span> <span class="nv">sale</span> , <span class="nv">he</span> <span class="nv">added</span> .   <span class="nv">On</span> <span class="nv">June</span> <span class="mi">10</span> , <span class="nv">the</span> <span class="nv">ship</span> <span class="s1">&#39;</span><span class="s">s owners had published an advertisement on the Internet , offering the explosives for sale .</span>
<span class="mi">0</span>   <span class="mi">3344667</span> <span class="mi">3344648</span> <span class="nv">Around</span> <span class="mi">0335</span> <span class="nv">GMT</span> , <span class="nv">Tab</span> <span class="nv">shares</span> <span class="nv">were</span> <span class="nv">up</span> <span class="mi">19</span> <span class="nv">cents</span> , <span class="nv">or</span> <span class="mi">4</span>.<span class="mi">4</span> <span class="o">%</span> , <span class="nv">at</span> <span class="nv">A</span> $ <span class="mi">4</span>.<span class="mi">56</span> , <span class="nv">having</span> <span class="nv">earlier</span> <span class="nv">set</span> <span class="nv">a</span> <span class="nv">record</span> <span class="nv">high</span> <span class="nv">of</span> <span class="nv">A</span> $ <span class="mi">4</span>.<span class="mi">57</span> . <span class="nv">Tab</span> <span class="nv">shares</span> <span class="nv">jumped</span> <span class="mi">20</span> <span class="nv">cents</span> , <span class="nv">or</span> <span class="mi">4</span>.<span class="mi">6</span> <span class="o">%</span> , <span class="nv">to</span> <span class="nv">set</span> <span class="nv">a</span> <span class="nv">record</span> <span class="nv">closing</span> <span class="nv">high</span> <span class="nv">at</span> <span class="nv">A</span> $ <span class="mi">4</span>.<span class="mi">57</span> .
<span class="mi">1</span>   <span class="mi">1236820</span> <span class="mi">1236712</span> <span class="nv">The</span> <span class="nv">stock</span> <span class="nv">rose</span> $ <span class="mi">2</span>.<span class="mi">11</span> , <span class="nv">or</span> <span class="nv">about</span> <span class="mi">11</span> <span class="nv">percent</span> , <span class="nv">to</span> <span class="nv">close</span> <span class="nv">Friday</span> <span class="nv">at</span> $ <span class="mi">21</span>.<span class="mi">51</span> <span class="nv">on</span> <span class="nv">the</span> <span class="nv">New</span> <span class="nv">York</span> <span class="nv">Stock</span> <span class="nv">Exchange</span> .   <span class="nv">PG</span> <span class="o">&amp;</span> <span class="nv">E</span> <span class="nv">Corp</span>. <span class="nv">shares</span> <span class="nv">jumped</span> $ <span class="mi">1</span>.<span class="mi">63</span> <span class="nv">or</span> <span class="mi">8</span> <span class="nv">percent</span> <span class="nv">to</span> $ <span class="mi">21</span>.<span class="mi">03</span> <span class="nv">on</span> <span class="nv">the</span> <span class="nv">New</span> <span class="nv">York</span> <span class="nv">Stock</span> <span class="nv">Exchange</span> <span class="nv">on</span> <span class="nv">Friday</span> .
<span class="mi">1</span>   <span class="mi">738533</span>  <span class="mi">737951</span>  <span class="nv">Revenue</span> <span class="nv">in</span> <span class="nv">the</span> <span class="nv">first</span> <span class="nv">quarter</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">year</span> <span class="nv">dropped</span> <span class="mi">15</span> <span class="nv">percent</span> <span class="nv">from</span> <span class="nv">the</span> <span class="nv">same</span> <span class="nv">period</span> <span class="nv">a</span> <span class="nv">year</span> <span class="nv">earlier</span> .   <span class="nv">With</span> <span class="nv">the</span> <span class="nv">scandal</span> <span class="nv">hanging</span> <span class="nv">over</span> <span class="nv">Stewart</span> <span class="s1">&#39;</span><span class="s">s company , revenue the first quarter of the year dropped 15 percent from the same period a year earlier .</span>
<span class="mi">0</span>   <span class="mi">264589</span>  <span class="mi">264502</span>  <span class="nv">The</span> <span class="nv">Nasdaq</span> <span class="nv">had</span> <span class="nv">a</span> <span class="nv">weekly</span> <span class="nv">gain</span> <span class="nv">of</span> <span class="mi">17</span>.<span class="mi">27</span> , <span class="nv">or</span> <span class="mi">1</span>.<span class="mi">2</span> <span class="nv">percent</span> , <span class="nv">closing</span> <span class="nv">at</span> <span class="mi">1</span>,<span class="mi">520</span>.<span class="mi">15</span> <span class="nv">on</span> <span class="nv">Friday</span> .    <span class="nv">The</span> <span class="nv">tech</span><span class="o">-</span><span class="nv">laced</span> <span class="nv">Nasdaq</span> <span class="nv">Composite</span> .<span class="nv">IXIC</span> <span class="nv">rallied</span> <span class="mi">30</span>.<span class="mi">46</span> <span class="nv">points</span> , <span class="nv">or</span> <span class="mi">2</span>.<span class="mi">04</span> <span class="nv">percent</span> , <span class="nv">to</span> <span class="mi">1</span>,<span class="mi">520</span>.<span class="mi">15</span> .
<span class="mi">1</span>   <span class="mi">579975</span>  <span class="mi">579810</span>  <span class="nv">The</span> <span class="nv">DVD</span><span class="o">-</span><span class="nv">CCA</span> <span class="k">then</span> <span class="nv">appealed</span> <span class="nv">to</span> <span class="nv">the</span> <span class="nv">state</span> <span class="nv">Supreme</span> <span class="nv">Court</span> .  <span class="nv">The</span> <span class="nv">DVD</span> <span class="nv">CCA</span> <span class="nv">appealed</span> <span class="nv">that</span> <span class="nv">decision</span> <span class="nv">to</span> <span class="nv">the</span> <span class="nv">U</span>.<span class="nv">S</span>. <span class="nv">Supreme</span> <span class="nv">Court</span> .
...
</code></pre></div>


<blockquote>
<ul>
<li>train.tsv数据样式说明：<ul>
<li>train.tsv中的数据内容共分为5列，第一列数据，0或1，代表每对句子是否具有相同的含义，0代表含义不相同，1代表含义相同。第二列和第三列分别代表每对句子的id，第四列和第五列分别具有相同/不同含义的句子对。</li>
</ul>
</li>
</ul>
</blockquote>
<hr />
<ul>
<li>test.tsv数据样式：</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="nv">index</span>   <span class="sc">#1</span> <span class="nv">ID</span>   <span class="sc">#2</span> <span class="nv">ID</span>   <span class="sc">#1</span> <span class="nv">String</span>   <span class="sc">#2</span> <span class="nv">String</span>
<span class="mi">0</span>   <span class="mi">1089874</span> <span class="mi">1089925</span> <span class="nv">PCCW</span> <span class="s1">&#39;</span><span class="s">s chief operating officer , Mike Butcher , and Alex Arena , the chief financial officer , will report directly to Mr So . Current Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So .</span>
<span class="mi">1</span>   <span class="mi">3019446</span> <span class="mi">3019327</span> <span class="nv">The</span> <span class="nv">world</span> <span class="s1">&#39;</span><span class="s">s two largest automakers said their U.S. sales declined more than predicted last month as a late summer sales frenzy caused more of an industry backlash than expected . Domestic sales at both GM and No. 2 Ford Motor Co. declined more than predicted as a late summer sales frenzy prompted a larger-than-expected industry backlash .</span>
<span class="mi">2</span>   <span class="mi">1945605</span> <span class="mi">1945824</span> <span class="nv">According</span> <span class="nv">to</span> <span class="nv">the</span> <span class="nv">federal</span> <span class="nv">Centers</span> <span class="k">for</span> <span class="nv">Disease</span> <span class="nv">Control</span> <span class="nv">and</span> <span class="nv">Prevention</span> <span class="ss">(</span> <span class="nv">news</span> <span class="o">-</span> <span class="nv">web</span> <span class="nv">sites</span> <span class="ss">)</span> , <span class="nv">there</span> <span class="nv">were</span> <span class="mi">19</span> <span class="nv">reported</span> <span class="nv">cases</span> <span class="nv">of</span> <span class="nv">measles</span> <span class="nv">in</span> <span class="nv">the</span> <span class="nv">United</span> <span class="nv">States</span> <span class="nv">in</span> <span class="mi">2002</span> .   <span class="nv">The</span> <span class="nv">Centers</span> <span class="k">for</span> <span class="nv">Disease</span> <span class="nv">Control</span> <span class="nv">and</span> <span class="nv">Prevention</span> <span class="nv">said</span> <span class="nv">there</span> <span class="nv">were</span> <span class="mi">19</span> <span class="nv">reported</span> <span class="nv">cases</span> <span class="nv">of</span> <span class="nv">measles</span> <span class="nv">in</span> <span class="nv">the</span> <span class="nv">United</span> <span class="nv">States</span> <span class="nv">in</span> <span class="mi">2002</span> .
<span class="mi">3</span>   <span class="mi">1430402</span> <span class="mi">1430329</span> <span class="nv">A</span> <span class="nv">tropical</span> <span class="nv">storm</span> <span class="nv">rapidly</span> <span class="nv">developed</span> <span class="nv">in</span> <span class="nv">the</span> <span class="nv">Gulf</span> <span class="nv">of</span> <span class="nv">Mexico</span> <span class="nv">Sunday</span> <span class="nv">and</span> <span class="nv">was</span> <span class="nv">expected</span> <span class="nv">to</span> <span class="nv">hit</span> <span class="nv">somewhere</span> <span class="nv">along</span> <span class="nv">the</span> <span class="nv">Texas</span> <span class="nv">or</span> <span class="nv">Louisiana</span> <span class="nv">coasts</span> <span class="nv">by</span> <span class="nv">Monday</span> <span class="nv">night</span> . <span class="nv">A</span> <span class="nv">tropical</span> <span class="nv">storm</span> <span class="nv">rapidly</span> <span class="nv">developed</span> <span class="nv">in</span> <span class="nv">the</span> <span class="nv">Gulf</span> <span class="nv">of</span> <span class="nv">Mexico</span> <span class="nv">on</span> <span class="nv">Sunday</span> <span class="nv">and</span> <span class="nv">could</span> <span class="nv">have</span> <span class="nv">hurricane</span><span class="o">-</span><span class="nv">force</span> <span class="nv">winds</span> <span class="nv">when</span> <span class="nv">it</span> <span class="nv">hits</span> <span class="nv">land</span> <span class="nv">somewhere</span> <span class="nv">along</span> <span class="nv">the</span> <span class="nv">Louisiana</span> <span class="nv">coast</span> <span class="nv">Monday</span> <span class="nv">night</span> .
<span class="mi">4</span>   <span class="mi">3354381</span> <span class="mi">3354396</span> <span class="nv">The</span> <span class="nv">company</span> <span class="nv">didn</span> <span class="s1">&#39;</span><span class="s">t detail the costs of the replacement and repairs .   But company officials expect the costs of the replacement work to run into the millions of dollars .</span>
<span class="mi">5</span>   <span class="mi">1390995</span> <span class="mi">1391183</span> <span class="nv">The</span> <span class="nv">settling</span> <span class="nv">companies</span> <span class="nv">would</span> <span class="nv">also</span> <span class="nv">assign</span> <span class="nv">their</span> <span class="nv">possible</span> <span class="nv">claims</span> <span class="nv">against</span> <span class="nv">the</span> <span class="nv">underwriters</span> <span class="nv">to</span> <span class="nv">the</span> <span class="nv">investor</span> <span class="nv">plaintiffs</span> , <span class="nv">he</span> <span class="nv">added</span> . <span class="nv">Under</span> <span class="nv">the</span> <span class="nv">agreement</span> , <span class="nv">the</span> <span class="nv">settling</span> <span class="nv">companies</span> <span class="nv">will</span> <span class="nv">also</span> <span class="nv">assign</span> <span class="nv">their</span> <span class="nv">potential</span> <span class="nv">claims</span> <span class="nv">against</span> <span class="nv">the</span> <span class="nv">underwriters</span> <span class="nv">to</span> <span class="nv">the</span> <span class="nv">investors</span> , <span class="nv">he</span> <span class="nv">added</span> .
<span class="mi">6</span>   <span class="mi">2201401</span> <span class="mi">2201285</span> <span class="nv">Air</span> <span class="nv">Commodore</span> <span class="nv">Quaife</span> <span class="nv">said</span> <span class="nv">the</span> <span class="nv">Hornets</span> <span class="nv">remained</span> <span class="nv">on</span> <span class="nv">three</span><span class="o">-</span><span class="nv">minute</span> <span class="nv">alert</span> <span class="nv">throughout</span> <span class="nv">the</span> <span class="nv">operation</span> . <span class="nv">Air</span> <span class="nv">Commodore</span> <span class="nv">John</span> <span class="nv">Quaife</span> <span class="nv">said</span> <span class="nv">the</span> <span class="nv">security</span> <span class="nv">operation</span> <span class="nv">was</span> <span class="nv">unprecedented</span> .
<span class="mi">7</span>   <span class="mi">2453843</span> <span class="mi">2453998</span> <span class="nv">A</span> <span class="nv">Washington</span> <span class="nv">County</span> <span class="nv">man</span> <span class="nv">may</span> <span class="nv">have</span> <span class="nv">the</span> <span class="nv">countys</span> <span class="nv">first</span> <span class="nv">human</span> <span class="nv">case</span> <span class="nv">of</span> <span class="nv">West</span> <span class="nv">Nile</span> <span class="nv">virus</span> , <span class="nv">the</span> <span class="nv">health</span> <span class="nv">department</span> <span class="nv">said</span> <span class="nv">Friday</span> .  <span class="nv">The</span> <span class="nv">countys</span> <span class="nv">first</span> <span class="nv">and</span> <span class="nv">only</span> <span class="nv">human</span> <span class="nv">case</span> <span class="nv">of</span> <span class="nv">West</span> <span class="nv">Nile</span> <span class="nv">this</span> <span class="nv">year</span> <span class="nv">was</span> <span class="nv">confirmed</span> <span class="nv">by</span> <span class="nv">health</span> <span class="nv">officials</span> <span class="nv">on</span> <span class="nv">Sept</span> . <span class="mi">8</span> .
...
</code></pre></div>


<blockquote>
<ul>
<li>test.tsv数据样式说明：<ul>
<li>test.tsv中的数据内容共分为5列，第一列数据代表每条文本数据的索引；其余列的含义与train.tsv中相同。</li>
</ul>
</li>
</ul>
</blockquote>
<hr />
<h3 id="huggingfacebert">使用huggingface中的预训练BERT模型进行微调</h3>
<ul>
<li>第一步: 安装必要的工具包并导入</li>
<li>第二步: 下载数据集并使用脚本进行微调</li>
<li>第三步: 设定全局配置并加载微调模型</li>
<li>第四步: 编写用于模型使用的评估函数</li>
</ul>
<hr />
<h4 id="_4">第一步: 安装核心的工具包并导入</h4>
<ul>
<li>安装核心工具包:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="o">#</span> <span class="err">这是由</span><span class="n">huggingface提供的预训练模型使用工具包</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">transformers</span><span class="o">==</span><span class="mi">2</span><span class="p">.</span><span class="mi">3</span><span class="p">.</span><span class="mi">0</span>
</code></pre></div>


<ul>
<li>工具包导入</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span><span class="p">,</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># 用于设定全局配置的命名空间</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>

<span class="c1"># 从torch.utils.data中导入常用的模型处理工具，会在代码使用中进行详细介绍</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="p">(</span><span class="n">DataLoader</span><span class="p">,</span> <span class="n">RandomSampler</span><span class="p">,</span> <span class="n">SequentialSampler</span><span class="p">,</span>
                              <span class="n">TensorDataset</span><span class="p">)</span>

<span class="c1"># 模型进度可视化工具，在评估过程中，帮助打印进度条</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1"># 从transformers中导入BERT模型的相关工具</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span><span class="n">BertConfig</span><span class="p">,</span> <span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">BertTokenizer</span><span class="p">,)</span>

<span class="c1"># 从transformers中导入GLUE数据集的评估指标计算方法</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">glue_compute_metrics</span> <span class="k">as</span> <span class="n">compute_metrics</span>

<span class="c1"># 从transformers中导入GLUE数据集的输出模式(回归/分类)</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">glue_output_modes</span> <span class="k">as</span> <span class="n">output_modes</span>

<span class="c1"># 从transformers中导入GLUE数据集的预处理器processors</span>
<span class="c1"># processors是将持久化文件加载到内存的过程，即输入一般为文件路径，输出是训练数据和对应标签的某种数据结构，如列表表示。</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">glue_processors</span> <span class="k">as</span> <span class="n">processors</span>

<span class="c1"># 从transformers中导入GLUE数据集的特征处理器convert_examples_to_features</span>
<span class="c1"># convert_examples_to_features是将processor的输出处理成模型需要的输入，NLP定中的一般流程为数值映射，指定长度的截断补齐等</span>
<span class="c1"># 在BERT模型上处理句子对时，还需要在句子前插入[CLS]开始标记，在两个句子中间和第二个句子末端插入[SEP]分割/结束标记</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">glue_convert_examples_to_features</span> <span class="k">as</span> <span class="n">convert_examples_to_features</span>


<span class="c1"># 设定与日志打印有关的配置</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">format</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1"> - </span><span class="si">%(levelname)s</span><span class="s1"> - </span><span class="si">%(name)s</span><span class="s1"> -   </span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span>
                    <span class="n">datefmt</span> <span class="o">=</span> <span class="s1">&#39;%m/</span><span class="si">%d</span><span class="s1">/%Y %H:%M:%S&#39;</span><span class="p">,</span>
                    <span class="n">level</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">)</span>

<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;transformers.modeling_utils&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span>
   <span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">)</span>  <span class="c1"># Reduce logging</span>


<span class="k">print</span><span class="p">(</span><span class="s2">&quot;torch version:&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>

<span class="c1"># 设置torch允许启动的线程数, 因为之后会对比压缩模型的耗时，因此防止该变量产生影响</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_num_threads</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__config__</span><span class="o">.</span><span class="n">parallel_info</span><span class="p">())</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>torch version: 1.3.1

ATen/Parallel:
    at::get_num_threads() : 1
    at::get_num_interop_threads() : 8
OpenMP 201511 (a.k.a. OpenMP 4.5)
    omp_get_max_threads() : 1
Intel(R) Math Kernel Library Version 2019.0.4 Product Build 20190411 for Intel(R) 64 architecture applications
    mkl_get_max_threads() : 1
Intel(R) MKL-DNN v0.20.5 (Git Hash 0125f28c61c1f822fd48570b4c1066f96fcb9b2e)
std::thread::hardware_concurrency() : 16
Environment variables:
    OMP_NUM_THREADS : [not set]
    MKL_NUM_THREADS : [not set]
ATen parallel backend: OpenMP
</code></pre></div>


<hr />
<h4 id="_5">第二步: 下载数据集并使用脚本进行微调</h4>
<ul>
<li>下载GLUE中的MRPC数据集:</li>
</ul>
<div class="codehilite"><pre><span></span><code>python download_glue_data.py --data_dir<span class="o">=</span><span class="s1">&#39;./glue_data&#39;</span> --tasks<span class="o">=</span><span class="s1">&#39;MRPC&#39;</span>
</code></pre></div>


<hr />
<ul>
<li>使用<a href="http://47.92.175.143:8008/4/#1">run_glue.py[具体代码内容见附录]</a>进行模型微调:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="c1"># 注意: 这是一段使用shell运行的脚本, 运行过程中需要请求AWS的S3进行预训练模型下载</span>

<span class="c1"># 定义GLUE_DIR: 微调数据所在路径, 这里我们使用glue_data中的数据作为微调数据</span>
<span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>./glue_data
<span class="c1"># 定义OUT_DIR: 模型的保存路径, 我们将模型保存在当前目录的bert_finetuning_test文件中</span>
<span class="nb">export</span> <span class="nv">OUT_DIR</span><span class="o">=</span>./bert_finetuning_test/

python ./run_glue.py <span class="se">\</span>
    --model_type bert <span class="se">\</span>
    --model_name_or_path bert-base-uncased <span class="se">\</span>
    --task_name MRPC <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --do_lower_case <span class="se">\</span>
    --data_dir <span class="nv">$GLUE_DIR</span>/MRPC <span class="se">\</span>
    --max_seq_length <span class="m">128</span> <span class="se">\</span>
    --per_gpu_eval_batch_size<span class="o">=</span><span class="m">8</span>   <span class="se">\</span>
    --per_gpu_train_batch_size<span class="o">=</span><span class="m">8</span>   <span class="se">\</span>
    --learning_rate 2e-5 <span class="se">\</span>
    --num_train_epochs <span class="m">1</span>.0 <span class="se">\</span>
    --output_dir <span class="nv">$OUT_DIR</span>

<span class="c1"># 使用python运行微调脚本</span>
<span class="c1"># --model_type: 选择需要微调的模型类型, 这里可以选择BERT, XLNET, XLM, roBERTa, distilBERT, ALBERT</span>
<span class="c1"># --model_name_or_path: 选择具体的模型或者变体, 这里是在英文语料上微调, 因此选择bert-base-uncased</span>
<span class="c1"># --task_name: 它将代表对应的任务类型, 如MRPC代表句子对二分类任务</span>
<span class="c1"># --do_train: 使用微调脚本进行训练</span>
<span class="c1"># --do_eval: 使用微调脚本进行验证</span>
<span class="c1"># --data_dir: 训练集及其验证集所在路径, 将自动寻找该路径下的train.tsv和dev.tsv作为训练集和验证集</span>
<span class="c1"># --max_seq_length: 输入句子的最大长度, 超过则截断, 不足则补齐</span>
<span class="c1"># --learning_rate: 学习率</span>
<span class="c1"># --num_train_epochs: 训练轮数</span>
<span class="c1"># --output_dir $OUT_DIR: 训练后的模型保存路径</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>...
03/18/2020 00:55:17 - INFO - __main__ -   Loading features from cached file ./glue_data/MRPC/cached_train_bert-base-uncased_128_mrpc
03/18/2020 00:55:17 - INFO - __main__ -   ***** Running training *****
03/18/2020 00:55:17 - INFO - __main__ -     Num examples = 3668
03/18/2020 00:55:17 - INFO - __main__ -     Num Epochs = 1
03/18/2020 00:55:17 - INFO - __main__ -     Instantaneous batch size per GPU = 8
03/18/2020 00:55:17 - INFO - __main__ -     Total train batch size (w. parallel, distributed &amp; accumulation) = 8
03/18/2020 00:55:17 - INFO - __main__ -     Gradient Accumulation steps = 1
03/18/2020 00:55:17 - INFO - __main__ -     Total optimization steps = 459
Epoch:   0%|                 | 0/1 [00:00&lt;?, ?it/s]
Iteration:   2%|   | 8/459 [00:13&lt;12:42,  1.69s/it]
</code></pre></div>


<blockquote>
<ul>
<li>运行成功后会在当前目录下生成 ./bert_finetuning_test文件夹，内部文件如下:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">added_tokens</span><span class="p">.</span><span class="n">json</span>  <span class="k">checkpoint</span><span class="o">-</span><span class="mi">200</span>  <span class="k">checkpoint</span><span class="o">-</span><span class="mi">350</span>  <span class="n">eval_results</span><span class="p">.</span><span class="n">txt</span>         <span class="n">tokenizer_config</span><span class="p">.</span><span class="n">json</span>
<span class="k">checkpoint</span><span class="o">-</span><span class="mi">100</span>     <span class="k">checkpoint</span><span class="o">-</span><span class="mi">250</span>  <span class="k">checkpoint</span><span class="o">-</span><span class="mi">50</span>   <span class="n">pytorch_model</span><span class="p">.</span><span class="n">bin</span>        <span class="n">training_args</span><span class="p">.</span><span class="n">bin</span>
<span class="k">checkpoint</span><span class="o">-</span><span class="mi">150</span>     <span class="k">checkpoint</span><span class="o">-</span><span class="mi">300</span>  <span class="n">config</span><span class="p">.</span><span class="n">json</span>     <span class="n">special_tokens_map</span><span class="p">.</span><span class="n">json</span>  <span class="n">vocab</span><span class="p">.</span><span class="n">txt</span>
</code></pre></div>


<hr />
<h4 id="_6">第三步: 设定全局配置并加载微调模型</h4>
<ul>
<li>设定全局配置:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="c1"># 这些配置将在调用微调模型时进行使用</span>

<span class="c1"># 实例化一个配置的命名空间</span>
<span class="n">configs</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">()</span>

<span class="c1"># 模型的输出文件路径</span>
<span class="n">configs</span><span class="o">.</span><span class="n">output_dir</span> <span class="o">=</span> <span class="s2">&quot;./bert_finetuning_test/&quot;</span>

<span class="c1"># 验证数据集所在路径(与训练集相同)</span>
<span class="n">configs</span><span class="o">.</span><span class="n">data_dir</span> <span class="o">=</span> <span class="s2">&quot;./glue_data/MRPC&quot;</span>

<span class="c1"># 预训练模型的名字</span>
<span class="n">configs</span><span class="o">.</span><span class="n">model_name_or_path</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span>

<span class="c1"># 文本的最大对齐长度</span>
<span class="n">configs</span><span class="o">.</span><span class="n">max_seq_length</span> <span class="o">=</span> <span class="mi">128</span>

<span class="c1"># GLUE中的任务名(需要小写)</span>
<span class="n">configs</span><span class="o">.</span><span class="n">task_name</span> <span class="o">=</span> <span class="s2">&quot;MRPC&quot;</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

<span class="c1"># 根据任务名从GLUE数据集处理工具包中取出对应的预处理工具</span>
<span class="n">configs</span><span class="o">.</span><span class="n">processor</span> <span class="o">=</span> <span class="n">processors</span><span class="p">[</span><span class="n">configs</span><span class="o">.</span><span class="n">task_name</span><span class="p">]()</span>

<span class="c1"># 得到对应模型输出模式(MRPC为分类)</span>
<span class="n">configs</span><span class="o">.</span><span class="n">output_mode</span> <span class="o">=</span> <span class="n">output_modes</span><span class="p">[</span><span class="n">configs</span><span class="o">.</span><span class="n">task_name</span><span class="p">]</span>

<span class="c1"># 得到该任务的对应的标签种类列表</span>
<span class="n">configs</span><span class="o">.</span><span class="n">label_list</span> <span class="o">=</span> <span class="n">configs</span><span class="o">.</span><span class="n">processor</span><span class="o">.</span><span class="n">get_labels</span><span class="p">()</span>

<span class="c1"># 定义模型类型</span>
<span class="n">configs</span><span class="o">.</span><span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;bert&quot;</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

<span class="c1"># 是否全部使用小写文本</span>
<span class="n">configs</span><span class="o">.</span><span class="n">do_lower_case</span> <span class="o">=</span> <span class="bp">True</span>

<span class="c1"># 使用的设备</span>
<span class="n">configs</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span>
<span class="c1"># 每次验证的批次大小</span>
<span class="n">configs</span><span class="o">.</span><span class="n">per_eval_batch_size</span> <span class="o">=</span> <span class="mi">8</span>

<span class="c1"># gpu的数量</span>
<span class="n">configs</span><span class="o">.</span><span class="n">n_gpu</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># 是否需要重写数据缓存</span>
<span class="n">configs</span><span class="o">.</span><span class="n">overwrite_cache</span> <span class="o">=</span> <span class="bp">False</span>
</code></pre></div>


<hr />
<ul>
<li>加载微调模型:</li>
</ul>
<div class="codehilite"><pre><span></span><code># 因为在模型使用中，会使用一些随机方法，为了使每次运行的结果可以复现
# 需要设定确定的随机种子，保证每次随机化的数字在范围内浮动
<span class="nv">def</span> <span class="nv">set_seed</span><span class="ss">(</span><span class="nv">seed</span><span class="ss">)</span>:
    <span class="k">random</span>.<span class="nv">seed</span><span class="ss">(</span><span class="nv">seed</span><span class="ss">)</span>
    <span class="nv">np</span>.<span class="k">random</span>.<span class="nv">seed</span><span class="ss">(</span><span class="nv">seed</span><span class="ss">)</span>
    <span class="nv">torch</span>.<span class="nv">manual_seed</span><span class="ss">(</span><span class="nv">seed</span><span class="ss">)</span>
<span class="nv">set_seed</span><span class="ss">(</span><span class="mi">42</span><span class="ss">)</span>


## 加载微调模型

# 加载<span class="nv">BERT</span>预训练模型的数值映射器
<span class="nv">tokenizer</span> <span class="o">=</span> <span class="nv">BertTokenizer</span>.<span class="nv">from_pretrained</span><span class="ss">(</span>
    <span class="nv">configs</span>.<span class="nv">output_dir</span>, <span class="nv">do_lower_case</span><span class="o">=</span><span class="nv">configs</span>.<span class="nv">do_lower_case</span><span class="ss">)</span>

# 加载带有文本分类头的<span class="nv">BERT</span>模型
<span class="nv">model</span> <span class="o">=</span> <span class="nv">BertForSequenceClassification</span>.<span class="nv">from_pretrained</span><span class="ss">(</span><span class="nv">configs</span>.<span class="nv">output_dir</span><span class="ss">)</span>

# 将模型传到制定设备上
<span class="nv">model</span>.<span class="nv">to</span><span class="ss">(</span><span class="nv">configs</span>.<span class="nv">device</span><span class="ss">)</span>
</code></pre></div>


<hr />
<h4 id="_7">第四步: 编写用于模型使用的评估函数</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    模型评估函数</span>
<span class="sd">    :param args: 模型的全局配置对象，里面包含模型的各种配置信息</span>
<span class="sd">    :param model: 使用的模型</span>
<span class="sd">    :param tokenizer: 文本数据的数值映射器</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># 因为之后会多次用到任务名和输出路径</span>
    <span class="c1"># 所以将其从参数中取出</span>
    <span class="n">eval_task</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">task_name</span>
    <span class="n">eval_output_dir</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">output_dir</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># 调用load_and_cache_examples加载原始或者已经缓存的数据</span>
        <span class="c1"># 得到一个验证数据集的迭代器对象</span>
        <span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">load_and_cache_examples</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">eval_task</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>

        <span class="c1"># 判断模型输出路径是否存在</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">eval_output_dir</span><span class="p">):</span>
            <span class="c1"># 不存在的话，创建该路径</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">eval_output_dir</span><span class="p">)</span>

        <span class="c1"># 使用SequentialSampler封装验证数据集的迭代器对象</span>
        <span class="c1"># SequentialSampler是采样器对象，一般在Dataloader数据加载器中使用，</span>
        <span class="c1"># 因为数据加载器是以迭代的方式产生数据，因此每个批次数据可以指定采样规则，</span>
        <span class="c1"># SequentialSampler是顺序采样器，不改变原有数据集的顺序，依次取出数据。</span>
        <span class="n">eval_sampler</span> <span class="o">=</span> <span class="n">SequentialSampler</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">)</span>
        <span class="c1"># 使用Dataloader数据加载器，参数分别是数据集的迭代器对象，采集器对象，批次大小</span>
        <span class="n">eval_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">eval_sampler</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">per_eval_batch_size</span><span class="p">)</span>

        <span class="c1"># 开始评估</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;***** Running evaluation *****&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;  Num examples = </span><span class="si">%d</span><span class="s2">&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">))</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;  Batch size = </span><span class="si">%d</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">per_eval_batch_size</span><span class="p">)</span>
        <span class="c1"># 初始化验证损失</span>
        <span class="n">eval_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="c1"># 初始化验证步数</span>
        <span class="n">nb_eval_steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># 初始化预测的概率分布</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="c1"># 初始化输出真实标签值</span>
        <span class="n">out_label_ids</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="c1"># 循环数据批次，使用tqdm封装数据加载器，可以在评估时显示进度条</span>
        <span class="c1"># desc是进度条前面的描述信息</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">eval_dataloader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Evaluating&quot;</span><span class="p">):</span>
            <span class="c1"># 评估过程中模型开启评估模式，不进行反向传播</span>
            <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
            <span class="c1"># 从batch中取出数据的所有相关信息存于元组中</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>
            <span class="c1"># 不进行梯度计算</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="c1"># 将batch携带的数据信息表示称字典形式</span>
                <span class="c1"># 这些数据信息和load_and_cache_examples函数返回的数据对象中信息相同</span>
                <span class="c1"># 词汇的映射数值, 词汇的类型数值(0或1, 代表第一句和第二句话)</span>
                <span class="c1"># 注意力掩码张量，以及对应的标签</span>
                <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;input_ids&#39;</span><span class="p">:</span>      <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                          <span class="s1">&#39;attention_mask&#39;</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                          <span class="s1">&#39;token_type_ids&#39;</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                          <span class="s1">&#39;labels&#39;</span><span class="p">:</span>         <span class="n">batch</span><span class="p">[</span><span class="mi">3</span><span class="p">]}</span>
                <span class="c1"># 将该字典作为参数输入到模型中获得输出</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
                <span class="c1"># 获得损失和预测分布</span>
                <span class="n">tmp_eval_loss</span><span class="p">,</span> <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span>
                <span class="c1"># 将损失累加求均值</span>
                <span class="n">eval_loss</span> <span class="o">+=</span> <span class="n">tmp_eval_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="c1"># 验证步数累加</span>
            <span class="n">nb_eval_steps</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># 如果是第一批次的数据</span>
            <span class="k">if</span> <span class="n">preds</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="c1"># 结果分布就是模型的输出分布</span>
                <span class="n">preds</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="c1"># 输出真实标签值为输入对应的labels</span>
                <span class="n">out_label_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># 结果分布就是每一次模型输出分布组成的数组</span>
                <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="c1"># 输出真实标签值为每一次输入对应的labels组成的数组</span>
                <span class="n">out_label_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out_label_ids</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># 计算每一轮的平均损失</span>
        <span class="n">eval_loss</span> <span class="o">=</span> <span class="n">eval_loss</span> <span class="o">/</span> <span class="n">nb_eval_steps</span>
        <span class="c1"># 取结果分布中最大的值对应的索引</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># 使用compute_metrics计算对应的评估指标</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">compute_metrics</span><span class="p">(</span><span class="n">eval_task</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">out_label_ids</span><span class="p">)</span>
        <span class="c1"># 在日志中打印每一轮的评估结果</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;***** Eval results {} *****&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
         <span class="k">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>


<span class="k">def</span> <span class="nf">load_and_cache_examples</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">task</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    加载或使用缓存数据</span>
<span class="sd">    :param args: 全局配置参数</span>
<span class="sd">    :param task: 任务名</span>
<span class="sd">    :param tokenizer: 数值映射器</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># 根据任务名(MRPC)获得对应数据预处理器</span>
    <span class="n">processor</span> <span class="o">=</span> <span class="n">processors</span><span class="p">[</span><span class="n">task</span><span class="p">]()</span>
    <span class="c1"># 获得输出模式</span>
    <span class="n">output_mode</span> <span class="o">=</span> <span class="n">output_modes</span><span class="p">[</span><span class="n">task</span><span class="p">]</span>
    <span class="c1"># 定义缓存数据文件的名字</span>
    <span class="n">cached_features_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;cached_{}_{}_{}_{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="s1">&#39;dev&#39;</span><span class="p">,</span>
        <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">model_name_or_path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)))</span><span class="o">.</span><span class="n">pop</span><span class="p">(),</span>
        <span class="nb">str</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_seq_length</span><span class="p">),</span>
        <span class="nb">str</span><span class="p">(</span><span class="n">task</span><span class="p">)))</span>
    <span class="c1"># 判断缓存文件是否存在，以及全局配置中是否需要重写数据</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">cached_features_file</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">overwrite_cache</span><span class="p">:</span>
        <span class="c1"># 使用torch.load(解序列化，一般用于加载模型，在这里用于加载训练数据)加载缓存文件</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">cached_features_file</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># 如果没有缓存文件，则需要使用processor从原始数据路径中加载数据</span>
        <span class="n">examples</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">get_dev_examples</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_dir</span><span class="p">)</span>
        <span class="c1"># 获取对应的标签</span>
        <span class="n">label_list</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">get_labels</span><span class="p">()</span>
        <span class="c1"># 再使用convert_examples_to_features生成模型需要的输入形式</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">convert_examples_to_features</span><span class="p">(</span><span class="n">examples</span><span class="p">,</span>
                                                <span class="n">tokenizer</span><span class="p">,</span>
                                                <span class="n">label_list</span><span class="o">=</span><span class="n">label_list</span><span class="p">,</span>
                                                <span class="n">max_length</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">max_seq_length</span><span class="p">,</span>
                                                <span class="n">output_mode</span><span class="o">=</span><span class="n">output_mode</span><span class="p">,</span>
                                                <span class="n">pad_token</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">([</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span><span class="p">])[</span><span class="mi">0</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Saving features into cached file </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">cached_features_file</span><span class="p">)</span>
        <span class="c1"># 将其保存至缓存文件路径中</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">cached_features_file</span><span class="p">)</span>


    <span class="c1"># 为了有效利用内存，之后将使用数据加载器，我们需要在这里将张量数据转换成数据迭代器对象TensorDatase</span>
    <span class="c1"># TensorDataset：用于自定义训练数据结构的迭代封装器，它可以封装任何与训练数据映射值相关的数据</span>
    <span class="c1">#（如：训练数据对应的标签，训练数据使用的掩码张量，token的类型id等），</span>
    <span class="c1"># 它们必须能转换成张量，将同训练数据映射值一起在训练过程中迭代使用。</span>

    <span class="c1"># 以下是分别把input_ids，attention_mask，token_type_ids，label封装在TensorDataset之中</span>
    <span class="n">all_input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">f</span><span class="o">.</span><span class="n">input_ids</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">features</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="n">all_attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">f</span><span class="o">.</span><span class="n">attention_mask</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">features</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="n">all_token_type_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">f</span><span class="o">.</span><span class="n">token_type_ids</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">features</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="n">all_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">f</span><span class="o">.</span><span class="n">label</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">features</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">all_input_ids</span><span class="p">,</span> <span class="n">all_attention_mask</span><span class="p">,</span> <span class="n">all_token_type_ids</span><span class="p">,</span> <span class="n">all_labels</span><span class="p">)</span>
    <span class="c1"># 返回数据迭代器对象</span>
    <span class="k">return</span> <span class="n">dataset</span>
</code></pre></div>


<ul>
<li>我们将在下面的模型量化中调用该评估函数。</li>
</ul>
<hr />
<h3 id="bert">使用动态量化技术对训练后的bert模型进行压缩</h3>
<ul>
<li>第一步: 将模型应用动态量化技术</li>
<li>第二步: 对比压缩后模型的大小</li>
<li>第三步: 对比压缩后的模型的推理准确性和耗时</li>
<li>第四步: 序列化模型以便之后使用</li>
</ul>
<hr />
<h4 id="_8">第一步: 将模型应用动态量化技术</h4>
<ul>
<li>应用动态量化技术:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="c1"># 使用torch.quantization.quantize_dynamic获得动态量化的模型</span>
<span class="c1"># 量化的网络层为所有的nn.Linear的权重，使其成为int8</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">quantize_dynamic</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="p">{</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">},</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span>
<span class="p">)</span>

<span class="c1"># 打印动态量化后的BERT模型</span>
<span class="k">print</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">)</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>## 模型中的所有Linear层变成了DynamicQuantizedLinear层

BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (key): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (value): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, scale=1.0, zero_point=0)
          )
          (output): BertOutput(
            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, scale=1.0, zero_point=0)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (key): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (value): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, scale=1.0, zero_point=0)
          )
          (output): BertOutput(
            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, scale=1.0, zero_point=0)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
...
...

        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (key): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (value): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, scale=1.0, zero_point=0)
          )
          (output): BertOutput(
            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, scale=1.0, zero_point=0)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): DynamicQuantizedLinear(in_features=768, out_features=768, scale=1.0, zero_point=0)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): DynamicQuantizedLinear(in_features=768, out_features=2, scale=1.0, zero_point=0)
)
</code></pre></div>


<hr />
<h4 id="_9">第二步: 对比压缩后模型的大小</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">print_size_of_model</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;打印模型大小&quot;&quot;&quot;</span>
    <span class="c1"># 保存模型中的参数部分到持久化文件</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">&quot;temp.p&quot;</span><span class="p">)</span>
    <span class="c1"># 打印持久化文件的大小</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Size (MB):&#39;</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">getsize</span><span class="p">(</span><span class="s2">&quot;temp.p&quot;</span><span class="p">)</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span>
    <span class="c1"># 移除该文件</span>
    <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s1">&#39;temp.p&#39;</span><span class="p">)</span>

<span class="c1"># 分别打印model和quantized_model</span>
<span class="n">print_size_of_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">print_size_of_model</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">)</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>## 模型参数文件大小缩减了257MB

Size (MB): 437.982584
Size (MB): 181.430351
</code></pre></div>


<h4 id="_10">第三步: 对比压缩后的模型的推理准确性和耗时</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">time_model_evaluation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">configs</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;获得模型评估结果和运行时间&quot;&quot;&quot;</span>
    <span class="c1"># 获得评估前时间</span>
    <span class="n">eval_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="c1"># 进行模型评估</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">configs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
    <span class="c1"># 获得评估后时间</span>
    <span class="n">eval_end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="c1"># 获得评估耗时</span>
    <span class="n">eval_duration_time</span> <span class="o">=</span> <span class="n">eval_end_time</span> <span class="o">-</span> <span class="n">eval_start_time</span>
    <span class="c1"># 打印模型评估结果</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Evaluate result:&quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
    <span class="c1"># 打印耗时</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Evaluate total time (seconds): {0:.1f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eval_duration_time</span><span class="p">))</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>Evaluating: 100%|██| 51/51 [01:36&lt;00:00,  1.89s/it]
Evaluate result: {&#39;acc&#39;: 0.8161764705882353, &#39;f1&#39;: 0.8739495798319329, &#39;acc_and_f1&#39;: 0.8450630252100841}
Evaluate total time (seconds): 96.4

Evaluating: 100%|███████████████████████████████| 51/51 [00:43&lt;00:00,  1.19it/s]
Evaluate result: {&#39;acc&#39;: 0.7965686274509803, &#39;f1&#39;: 0.8663446054750403, &#39;acc_and_f1&#39;: 0.8314566164630104}
Evaluate total time (seconds): 43.0
</code></pre></div>


<ul>
<li>结论:<ul>
<li>对模型进行动态量化后，参数文件大小明显减少。</li>
<li>动态量化后的模型在验证集上评估指标几乎不变，但是耗时却只用了原来的一半左右。</li>
</ul>
</li>
</ul>
<hr />
<h4 id="_11">第四步: 序列化模型以便之后使用</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># 量化模型的保存路径</span>
<span class="n">quantized_output_dir</span> <span class="o">=</span> <span class="n">configs</span><span class="o">.</span><span class="n">output_dir</span> <span class="o">+</span> <span class="s2">&quot;quantized/&quot;</span>
<span class="c1"># 判断是否需要创建该路径</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">quantized_output_dir</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">quantized_output_dir</span><span class="p">)</span>
    <span class="c1"># 使用save_pretrained保存模型</span>
    <span class="n">quantized_model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">quantized_output_dir</span><span class="p">)</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code># 在bert_finetuning_test/目录下

- quantized/
    - config.json
    - pytorch_model.bin
</code></pre></div>


<h3 id="_12">附录</h3>
<h4 id="run_gluepy">run_glue.py微调脚本代码</h4>
<p>请访问: <a href="http://git.itcast.cn/Stephen/AI-key-file/blob/master/run_glue.py">http://git.itcast.cn/Stephen/AI-key-file/blob/master/run_glue.py</a></p>
<hr />
<h2 id="72">7.2 模型剪枝技术</h2>
<h3 id="_13">学习目标</h3>
<ul>
<li>了解模型剪枝原理。</li>
<li>掌握使用prune对已有模型进行剪枝。</li>
</ul>
<hr />
<p><center><img alt="avatar" src="../img/prune.jpeg" /></center></p>
<hr />
<h3 id="_14">剪枝技术介绍与原理</h3>
<ul>
<li>当前在NLP任务上取得优异成绩的模型往往是拥有大量参数的模型，如BERT，GPT等，但实际上，生物的高度文明是使用神经网络有效的稀疏连接，这正是“剪枝技术”的发展起源。而所谓剪枝技术原理，也就是将现有模型中的某些参数设置为0，等效于这些神经元失活。而参数置0的方式是使用MASK蒙版（蒙版0位置得到0，1位置得到原来的值）。下面我们将详细介绍这种技术的实现方式。</li>
</ul>
<hr />
<h3 id="prune">使用prune对已有模型进行剪枝</h3>
<ul>
<li>我们将使用torch.nn.utils.prune中的方法进行网络稀疏化，即剪枝。</li>
<li>要求torch版本应该&gt;= 1.4.0。</li>
</ul>
<div class="codehilite"><pre><span></span><code>pip install torch&gt;<span class="o">=</span><span class="m">1</span>.4.0
</code></pre></div>


<ul>
<li>剪枝的讲解将分为以下步骤：<ul>
<li>第一步：导入工具包并获得模型</li>
<li>第二步：使用剪枝工具并了解其作用方式</li>
<li>第三步：持久化修剪后的模型</li>
<li>第四步：工程中常用的修剪方法</li>
</ul>
</li>
</ul>
<hr />
<h4 id="_15">第一步：导入工具包并获得模型</h4>
<ul>
<li>这里我们将使用LeNet作为剪枝的对象，当然你可以选择任何你已经训练好的某个网络和参数。</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="c1"># 导入必备的工具包</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.utils.prune</span> <span class="kn">as</span> <span class="nn">prune</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>


<span class="c1"># 这里不再对LeNet网络做过多的介绍，因为我们的重点是剪枝技术</span>
<span class="c1"># 无论你使用哪一种网络进行剪枝，你必须熟知这个网络中的组成部分即哪些能够剪枝</span>
<span class="c1"># 比如在这里，conv1，conv2，fc1，fc2，fc3都是可以被剪枝的</span>
<span class="k">class</span> <span class="nc">LeNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LeNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>  <span class="c1"># 5x5 image dimension</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># 获得这个模型对象</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LeNet</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</code></pre></div>


<hr />
<h4 id="_16">第二步：使用剪枝工具并了解其作用方式</h4>
<ul>
<li>下面我们会使用剪枝工具并逐步查看参数的变化，先来看看没有剪枝之前的状态。</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="c1"># 获得conv1模块</span>
<span class="n">module</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">conv1</span>
<span class="c1"># 查看该模块的所有原生参数，一般由原生weight和原生bias组成</span>
<span class="c1"># 什么是原生参数呢？</span>
<span class="c1"># 这里是因为torch在设计存储网络参数时，允许为参数添加修改方式（如蒙版），</span>
<span class="c1"># 这些修改方式以buffer的形式存储，不会直接作用在named_parameters中的参数上，</span>
<span class="c1"># 因此把named_parameters中的参数叫做原生参数。</span>
<span class="c1"># 那如何获得作用了buffer之后的真实参数呢？</span>
<span class="c1"># 使用module.weight和module.bias即可</span>
<span class="k">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()))</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>[(&#39;weight&#39;, Parameter containing:
tensor([[[[ 0.3161, -0.2212,  0.0417],
          [ 0.2488,  0.2415,  0.2071],
          [-0.2412, -0.2400, -0.2016]]],


        [[[ 0.0419,  0.3322, -0.2106],
          [ 0.1776, -0.1845, -0.3134],
          [-0.0708,  0.1921,  0.3095]]],


        [[[-0.2070,  0.0723,  0.2876],
          [ 0.2209,  0.2077,  0.2369],
          [ 0.2108,  0.0861, -0.2279]]],


        [[[-0.2799, -0.1527, -0.0388],
          [-0.2043,  0.1220,  0.1032],
          [-0.0755,  0.1281,  0.1077]]],


        [[[ 0.2035,  0.2245, -0.1129],
          [ 0.3257, -0.0385, -0.0115],
          [-0.3146, -0.2145, -0.1947]]],


        [[[-0.1426,  0.2370, -0.1089],
          [-0.2491,  0.1282,  0.1067],
          [ 0.2159, -0.1725,  0.0723]]]], device=&#39;cuda:0&#39;, requires_grad=True)), 

(&#39;bias&#39;, Parameter containing:
tensor([-0.1214, -0.0749, -0.2656, -0.1519, -0.1021,  0.1425], device=&#39;cuda:0&#39;,
       requires_grad=True))]
</code></pre></div>


<hr />
<div class="codehilite"><pre><span></span><code><span class="c1"># 查看参数修改方式即buffer</span>
<span class="c1"># 没有buffer的原生参数就是真实参数</span>
<span class="k">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">()))</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>[]
</code></pre></div>


<hr />
<ul>
<li>使用prune.random_unstructured随机方式进行剪枝</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="c1"># 剪枝指定moudle即conv1中的weight参数，修剪（设置为0）30%</span>
<span class="n">prune</span><span class="o">.</span><span class="n">random_unstructured</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">amount</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</code></pre></div>


<div class="codehilite"><pre><span></span><code><span class="c1"># 查看修剪后的named_parameters参数</span>
<span class="k">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()))</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>[(&#39;bias&#39;, Parameter containing:
tensor([-0.1214, -0.0749, -0.2656, -0.1519, -0.1021,  0.1425], device=&#39;cuda:0&#39;,
       requires_grad=True)), 

(&#39;weight_orig&#39;, Parameter containing:
tensor([[[[ 0.3161, -0.2212,  0.0417],
          [ 0.2488,  0.2415,  0.2071],
          [-0.2412, -0.2400, -0.2016]]],


        [[[ 0.0419,  0.3322, -0.2106],
          [ 0.1776, -0.1845, -0.3134],
          [-0.0708,  0.1921,  0.3095]]],


        [[[-0.2070,  0.0723,  0.2876],
          [ 0.2209,  0.2077,  0.2369],
          [ 0.2108,  0.0861, -0.2279]]],


        [[[-0.2799, -0.1527, -0.0388],
          [-0.2043,  0.1220,  0.1032],
          [-0.0755,  0.1281,  0.1077]]],


        [[[ 0.2035,  0.2245, -0.1129],
          [ 0.3257, -0.0385, -0.0115],
          [-0.3146, -0.2145, -0.1947]]],


        [[[-0.1426,  0.2370, -0.1089],
          [-0.2491,  0.1282,  0.1067],
          [ 0.2159, -0.1725,  0.0723]]]], device=&#39;cuda:0&#39;, requires_grad=True))]
</code></pre></div>


<ul>
<li>在结果中，我们发现原生参数并没有变化，而是名字由weight变成了weight_orig，进一步强调了它是原生参数。</li>
<li>为什么要进一步强调原生呢，是因为此时buffer中已经多了一些信息，原生参数和真实参数已经不再等价。</li>
</ul>
<hr />
<div class="codehilite"><pre><span></span><code><span class="k">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">()))</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>[(&#39;weight_mask&#39;, tensor([[[[0., 1., 0.],
          [1., 0., 0.],
          [1., 1., 1.]]],


        [[[1., 0., 1.],
          [1., 1., 0.],
          [1., 0., 1.]]],


        [[[1., 0., 0.],
          [0., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 0., 0.],
          [1., 1., 1.],
          [1., 1., 1.]]],


        [[[1., 0., 1.],
          [1., 1., 1.],
          [0., 1., 1.]]],


        [[[1., 1., 1.],
          [1., 1., 0.],
          [1., 1., 0.]]]], device=&#39;cuda:0&#39;))]
</code></pre></div>


<ul>
<li>buffer中已经存在了用于剪枝的蒙版</li>
</ul>
<hr />
<ul>
<li>使用module.weight查看真实使用参数：</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="k">print</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([[[[ 0.0000, -0.2212,  0.0000],
          [ 0.2488,  0.0000,  0.0000],
          [-0.2412, -0.2400, -0.2016]]],


        [[[ 0.0419,  0.0000, -0.2106],
          [ 0.1776, -0.1845, -0.0000],
          [-0.0708,  0.0000,  0.3095]]],


        [[[-0.2070,  0.0000,  0.0000],
          [ 0.0000,  0.2077,  0.2369],
          [ 0.2108,  0.0861, -0.2279]]],


        [[[-0.2799, -0.0000, -0.0000],
          [-0.2043,  0.1220,  0.1032],
          [-0.0755,  0.1281,  0.1077]]],


        [[[ 0.2035,  0.0000, -0.1129],
          [ 0.3257, -0.0385, -0.0115],
          [-0.0000, -0.2145, -0.1947]]],


        [[[-0.1426,  0.2370, -0.1089],
          [-0.2491,  0.1282,  0.0000],
          [ 0.2159, -0.1725,  0.0000]]]], device=&#39;cuda:0&#39;,
       grad_fn=&lt;MulBackward0&gt;)
</code></pre></div>


<hr />
<h4 id="_17">第三步：持久化修剪后的模型</h4>
<ul>
<li>假如你已经对剪枝后的模型进行了必要的验证，并觉得它可以被保存并在将来部署使用，那么你需要持久化模型。</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="c1"># 一般我们会首先将buffer中的蒙版永久作用在name_parameters中的参数上</span>
<span class="c1"># 这里的remove不是移除，而是永久化</span>
<span class="n">prune</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()))</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>[(&#39;bias_orig&#39;, Parameter containing:
tensor([-0.1214, -0.0749, -0.2656, -0.1519, -0.1021,  0.1425], device=&#39;cuda:0&#39;,
       requires_grad=True)), (&#39;weight&#39;, Parameter containing:
tensor([[[[ 0.0000, -0.2212,  0.0000],
          [ 0.2488,  0.0000,  0.0000],
          [-0.2412, -0.2400, -0.2016]]],


        [[[ 0.0000,  0.0000, -0.0000],
          [ 0.0000, -0.0000, -0.0000],
          [-0.0000,  0.0000,  0.0000]]],


        [[[-0.2070,  0.0000,  0.0000],
          [ 0.0000,  0.2077,  0.2369],
          [ 0.2108,  0.0861, -0.2279]]],


        [[[-0.0000, -0.0000, -0.0000],
          [-0.0000,  0.0000,  0.0000],
          [-0.0000,  0.0000,  0.0000]]],


        [[[ 0.2035,  0.0000, -0.1129],
          [ 0.3257, -0.0385, -0.0115],
          [-0.0000, -0.2145, -0.1947]]],


        [[[-0.0000,  0.0000, -0.0000],
          [-0.0000,  0.0000,  0.0000],
          [ 0.0000, -0.0000,  0.0000]]]], device=&#39;cuda:0&#39;, requires_grad=True))]
</code></pre></div>


<div class="codehilite"><pre><span></span><code><span class="k">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">()))</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>[]
</code></pre></div>


<ul>
<li>此时buffer中已经没有任何修改，此时原生参数和真实参数等价。</li>
</ul>
<hr />
<ul>
<li>保存序列化模型：</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">PATH</span> <span class="o">=</span> <span class="s2">&quot;./model.pth&quot;</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>odict_keys([&#39;conv1.bias&#39;, &#39;conv1.weight&#39;, &#39;conv2.weight&#39;, &#39;conv2.bias&#39;, &#39;fc1.weight&#39;, &#39;fc1.bias&#39;, &#39;fc2.weight&#39;, &#39;fc2.bias&#39;, &#39;fc3.weight&#39;, &#39;fc3.bias&#39;])
</code></pre></div>


<ul>
<li>实际上假如你不进行prune.remove操作，直接保存state_dict()也是可以的，因为buffer也可以被保存在state_dict中，因为剪枝后往往需要继续重新训练，一般直到最后判断剪枝模型可用才使用remove永久化剪枝参数。</li>
</ul>
<hr />
<h4 id="_18">第四步：工程中常用的修剪方法</h4>
<ul>
<li>刚刚我们学习的都是“局部”的剪枝方式，而实际工程中我们往往直接针对模型进行整体剪枝。下面就是整体剪枝的方法：</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="c1"># 获得模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LeNet</span><span class="p">()</span>

<span class="c1"># 用元组指定需要剪枝的层和参数类型</span>
<span class="n">parameters_to_prune</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv1</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv2</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc1</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc2</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc3</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># 进行全局剪枝，参数分别是需要剪枝的层和参数类型，剪枝方法，剪枝比例</span>
<span class="c1"># 通过这样的操作我们就可以得到剪枝后的模型，这里的0.2是整体的20%，各个部分剪枝在20%左右</span>
<span class="c1"># 这里使用了L1剪枝</span>
<span class="n">prune</span><span class="o">.</span><span class="n">global_unstructured</span><span class="p">(</span>
    <span class="n">parameters_to_prune</span><span class="p">,</span>
    <span class="n">pruning_method</span><span class="o">=</span><span class="n">prune</span><span class="o">.</span><span class="n">L1Unstructured</span><span class="p">,</span>
    <span class="n">amount</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
<span class="p">)</span>


<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;#################&quot;</span><span class="p">)</span>

<span class="c1"># 永久化参数</span>
<span class="k">for</span> <span class="n">module</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">parameters_to_prune</span><span class="p">:</span>
    <span class="n">prune</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>dict_keys([&#39;conv1.bias&#39;, &#39;conv1.weight&#39;, &#39;conv2.weight&#39;, &#39;conv2.bias&#39;, &#39;fc1.weight&#39;, &#39;fc1.bias&#39;, &#39;fc2.weight&#39;, &#39;fc2.bias&#39;, &#39;fc3.weight&#39;, &#39;fc3.bias&#39;])
odict_keys([&#39;conv1.bias&#39;, &#39;conv1.weight_orig&#39;, &#39;conv1.weight_mask&#39;, &#39;conv2.bias&#39;, &#39;conv2.weight_orig&#39;, &#39;conv2.weight_mask&#39;, &#39;fc1.bias&#39;, &#39;fc1.weight_orig&#39;, &#39;fc1.weight_mask&#39;, &#39;fc2.bias&#39;, &#39;fc2.weight_orig&#39;, &#39;fc2.weight_mask&#39;, &#39;fc3.bias&#39;, &#39;fc3.weight_orig&#39;, &#39;fc3.weight_mask&#39;])

#################

odict_keys([&#39;conv1.bias&#39;, &#39;conv1.weight&#39;, &#39;conv2.weight&#39;, &#39;conv2.bias&#39;, &#39;fc1.weight&#39;, &#39;fc1.bias&#39;, &#39;fc2.weight&#39;, &#39;fc2.bias&#39;, &#39;fc3.weight&#39;, &#39;fc3.bias&#39;])
odict_keys([&#39;conv1.bias&#39;, &#39;conv1.weight&#39;, &#39;conv2.bias&#39;, &#39;conv2.weight&#39;, &#39;fc1.bias&#39;, &#39;fc1.weight&#39;, &#39;fc2.bias&#39;, &#39;fc2.weight&#39;, &#39;fc3.bias&#39;, &#39;fc3.weight&#39;])
</code></pre></div>


<hr />
<ul>
<li>检查一下全局剪枝的20%在各个层中剪枝占比:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="k">print</span><span class="p">(</span>
    <span class="s2">&quot;Sparsity in conv1.weight: {:.2f}%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="mf">100.</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>
        <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">())</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="k">print</span><span class="p">(</span>
    <span class="s2">&quot;Sparsity in conv2.weight: {:.2f}%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="mf">100.</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv2</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>
        <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv2</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">())</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="k">print</span><span class="p">(</span>
    <span class="s2">&quot;Sparsity in fc1.weight: {:.2f}%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="mf">100.</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>
        <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">())</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="k">print</span><span class="p">(</span>
    <span class="s2">&quot;Sparsity in fc2.weight: {:.2f}%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="mf">100.</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc2</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>
        <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc2</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">())</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="k">print</span><span class="p">(</span>
    <span class="s2">&quot;Sparsity in fc3.weight: {:.2f}%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="mf">100.</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc3</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>
        <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc3</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">())</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="k">print</span><span class="p">(</span>
    <span class="s2">&quot;Global sparsity: {:.2f}%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="mf">100.</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
            <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">conv2</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
            <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
            <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc2</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
            <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc3</span><span class="o">.</span><span class="n">weight</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="o">/</span> <span class="nb">float</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
            <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">conv2</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
            <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
            <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">fc2</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
            <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">fc3</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
        <span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果：</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>Sparsity in conv1.weight: 7.41%
Sparsity in conv2.weight: 9.49%
Sparsity in fc1.weight: 22.00%
Sparsity in fc2.weight: 12.28%
Sparsity in fc3.weight: 9.76%
Global sparsity: 20.00%
</code></pre></div>


<hr />
<ul>
<li>常用的剪枝方式解释：<ul>
<li>RandomUnstructured：随机剪枝</li>
<li>L1Unstructured：按照L1范数（绝对值大小）剪枝，因为数值越小对结果的扰动也越小。</li>
</ul>
</li>
</ul>
<hr />
<h3 id="_19">小节总结</h3>
<ul>
<li>学习了模型剪枝原理</li>
<li>学习了使用prune对已有模型进行剪枝<ul>
<li>第一步：导入工具包并获得模型</li>
<li>第二步：使用剪枝工具并了解其作用方式</li>
<li>第三步：持久化修剪后的模型</li>
<li>第四步：工程中常用的修剪方法</li>
</ul>
</li>
</ul>
<hr />
<h2 id="73-onnx-runtime">7.3 使用ONNX-Runtime进行模型推断加速</h2>
<h3 id="_20">学习目标</h3>
<ul>
<li>了解ONNX及其ONNX-Runtime的主要作用。</li>
<li>掌握如何使用ONNX-Runtime进行模型推断加速。</li>
</ul>
<hr />
<p><center><img alt="avatar" src="../img/trt-info.png" /></center></p>
<hr />
<h3 id="onnxonnx-runtime">什么是ONNX和ONNX-Runtime</h3>
<ul>
<li>ONNX（Open Neural Network Exchange）开放式神经网络交换格式，与torch的pth，keras的h5，tensorflow的pb一样，它属于一种模型格式。</li>
<li>这种交换格式被设计的初衷：希望各种模型框架训练得到的模型能够通用。而现在它已经结合ONNX-Runtime成为一种加速模型推理的方法。</li>
<li>ONNX-Runtime就是指ONNX格式模型的运行环境，它由微软开源，该环境集成多种模型加速工具，如Nvidia的TensorRT等，用于快速模型推断。</li>
<li>能够原生支持ONNX模型转换的模型包括Pytorch，MXNET，Caffe2等框架（Tensorflow不可以）。</li>
</ul>
<hr />
<p><center><img alt="avatar" src="../img/trt-info2.png" /></center></p>
<ul>
<li>图中，蓝色的部分就是ONNX-Runtime，它能够自动利用已有设备上的各种加速工具，完成模型加速，无需人工参与，只需要我们将ONNX格式的模型作为输入即可。</li>
</ul>
<hr />
<h3 id="onnx-runtime">使用ONNX-Runtime进行模型推断加速的步骤</h3>
<ul>
<li>第一步：安装必备的工具包</li>
<li>第二步：将已有模型转换成ONNX格式</li>
<li>第三步：使用ONNX-Runtime进行模型预测</li>
<li>第四步：对比结果差异和推断时间</li>
</ul>
<hr />
<h4 id="_21">第一步：安装必备的工具包</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># onnx 1.7.0</span>
<span class="c1"># onnxruntime 1.4.0</span>
pip install onnx onnxruntime
</code></pre></div>


<hr />
<h4 id="onnx">第二步：将已有模型转换成ONNX格式</h4>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.onnx</span>
<span class="kn">import</span> <span class="nn">torchvision</span>

<span class="c1"># 这里以resnet18模型为例</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># 随机初始化一个指定shape的输入</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># 评估模式</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</code></pre></div>


<hr />
<div class="codehilite"><pre><span></span><code><span class="c1"># 定义onnx输入输出的名字（格式需要）</span>
<span class="n">input_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;input1&quot;</span><span class="p">]</span>
<span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;output1&quot;</span><span class="p">]</span>

<span class="n">onnx_model_name</span> <span class="o">=</span> <span class="s2">&quot;resnet18.onnx&quot;</span>

<span class="c1"># 使用torch.onnx导出resnet18.onnx</span>
<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">onnx_model_name</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">)</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果:<ul>
<li>在该脚本路径下得到resnet18.onnx格式的模型。</li>
</ul>
</li>
</ul>
</blockquote>
<hr />
<h4 id="onnx-runtime_1">第三步：使用ONNX-Runtime进行模型预测</h4>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">onnxruntime</span>

<span class="c1"># 当前onnxruntime的输入要求的为numpy形式</span>
<span class="k">def</span> <span class="nf">to_numpy</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;将tensor转化成numpy&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">else</span> <span class="n">tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="c1"># 使用模型创建onnxruntime的session</span>
<span class="n">ort_session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="n">onnx_model_name</span><span class="p">)</span>
<span class="c1"># 在session中运行，要求输入为dict形式，key为之前定义好的input名字，且input必须为numpy形式</span>
<span class="n">ort_outs</span> <span class="o">=</span> <span class="n">ort_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="p">{</span><span class="n">ort_session</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">to_numpy</span><span class="p">(</span><span class="nb">input</span><span class="p">)})</span>
</code></pre></div>


<hr />
<h4 id="_22">第四步：对比结果差异和推断时间</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># pth模型的结果</span>
<span class="n">torch_out</span> <span class="o">=</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>

<span class="c1"># onnx模型的结果</span>
<span class="n">ort_out</span> <span class="o">=</span> <span class="n">ort_outs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="n">torch_out</span><span class="p">)</span>
<span class="k">print</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">ort_out</span><span class="p">)</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>## 结果是一模一样的，没有任何差异

[[ 6.46063685e-01  2.58094740e+00  2.67934680e+00  2.84586716e+00
   4.45001364e+00  3.60939002e+00  3.51634717e+00  2.71348268e-01
  -1.15397012e+00 -7.00954318e-01 -7.89242506e-01  9.08504605e-01
   1.55155942e-01  1.08485329e+00  1.31591737e+00  5.42257011e-01]]

[[ 6.46063685e-01  2.58094740e+00  2.67934680e+00  2.84586716e+00
   4.45001364e+00  3.60939002e+00  3.51634717e+00  2.71348268e-01
  -1.15397012e+00 -7.00954318e-01 -7.89242506e-01  9.08504605e-01
   1.55155942e-01  1.08485329e+00  1.31591737e+00  5.42257011e-01]]
</code></pre></div>


<hr />
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">time</span>


<span class="c1"># 对比两者在CPU上的预测时间差异</span>
<span class="c1"># 二者在GPU上的表现相当，因为onnxruntime本身也是调用cuda</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">torch_out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">ort_outs</span> <span class="o">=</span> <span class="n">ort_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;input1&quot;</span><span class="p">:</span> <span class="n">to_numpy</span><span class="p">(</span><span class="nb">input</span><span class="p">)})</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code># pth模型推断时间为24.6ms
0.024605449676513672

# onnx仅需8.5ms，大约节省2-3倍时间
0.008518030548095703
</code></pre></div>


<hr />
<h3 id="onnx-runtime_2">ONNX-Runtime能够加速的原理</h3>
<ul>
<li>下面以其调用TensorRT(nvidia加速工具)加速BERT为例进行说明：</li>
</ul>
<p><center><img alt="" src="../img/op1.jpg" /></center></p>
<blockquote>
<ul>
<li>
<p>在BERT编码器中，将LayerNormalization层和残差连接进行融合以加速计算。</p>
</li>
<li>
<p>对gelu激活函数使用<code>简化gelu</code>计算方法加速计算。</p>
</li>
</ul>
</blockquote>
<hr />
<p><center><img alt="" src="../img/op2.jpg" /></center>
<center><img alt="" src="../img/op3.jpg" /></center></p>
<blockquote>
<ul>
<li>
<p>对所有的self-attention layer中的全连接层进行融合，以减少内存和正反向传播次数加速计算。</p>
</li>
<li>
<p>ONNX-Runtime内置针对主流模型（BERT，RESNET等）的并行计算模式，实现加速计算。</p>
</li>
</ul>
</blockquote>
<p><center><img alt="" src="../img/op4.jpg" /></center></p>
<hr />
<h3 id="_23">小节总结</h3>
<ul>
<li>学习了使用ONNX-Runtime进行模型推断加速的步骤：<ul>
<li>第一步：安装必备的工具包</li>
<li>第二步：将已有模型转换成ONNX格式</li>
<li>第三步：使用ONNX-Runtime进行模型预测</li>
<li>第四步：对比结果差异和推断时间</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>ONNX-Runtime能够加速的原因：<ul>
<li>1，作为加速工具的集成环境，能够自动调用加速工具cuda，TensorRT，nGragh等。</li>
<li>2，对特定模型的层和张量进行融合，以减少正反向传播次数。</li>
<li>3，对特定激活函数gelu进行简化计算。</li>
<li>4，针对主流模型（BERT，RESNET等）的并行计算模式，实现加速计算。</li>
</ul>
</li>
</ul>
<hr />
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            ©Copyright 2020, AITutorials.CN This website has been reviewed by the review agency. 京ICP备19006137号
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
  <div class="md-footer-social">
    <link rel="stylesheet" href="../assets/fonts/font-awesome.css">
    
      <a href="https://www.linkedin.com/in/%E7%A7%91%E6%8A%80%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8-%E5%8C%97%E4%BA%AC%E6%A9%98%E6%98%9F-6bb7081a1/" class="md-footer-social__link fa fa-linkedin"></a>
    
      <a href="https://weibo.com/u/3469990762?is_all=1" class="md-footer-social__link fa fa-weibo"></a>
    
      <a href="http://bitbucket.org/AITutorials" class="md-footer-social__link fa fa-bitbucket"></a>
    
      <a href="https://github.com/AITutorials/datasets/issues" class="md-footer-social__link fa fa-gitlab"></a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.245445c6.js"></script>
      
        
        
          
          <script src="../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
                <script src="../assets/javascripts/lunr/tinyseg.js"></script>
              
              
                <script src="../assets/javascripts/lunr/lunr.ja.js"></script>
              
            
          
          
        
      
      <script>app.initialize({version:"1.1.2",url:{base:".."}})</script>
      
    
  </body>
</html>